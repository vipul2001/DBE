{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.9200,  1.3039,  0.4200,  0.3109,  0.4690,  0.6029,  2.3439,  2.6662,\n",
       "          1.2795,  1.7548,  1.1437,  1.5545,  1.3618,  1.1509,  1.8838,  2.3340,\n",
       "         -0.2855,  0.3181,  1.4549,  2.7728,  2.9489,  1.1961,  0.1313,  1.0537,\n",
       "          0.9653,  3.3742, -0.5977,  1.7905,  2.6157,  1.6762,  2.4348,  5.1139,\n",
       "          1.7743,  3.9008,  3.3089,  4.0199,  4.3795,  2.1529,  4.3314,  3.1195,\n",
       "          2.7146,  2.6723,  1.3058,  1.9635,  3.3496,  5.4887,  3.7517,  2.5722,\n",
       "          3.1012,  3.7096,  2.9797,  3.1102,  4.1768,  4.8650,  3.2323,  4.2794,\n",
       "          4.4610,  4.5559,  4.6264,  5.0121,  3.6080,  3.7246,  3.2909,  4.3206,\n",
       "          3.3818,  4.6746,  3.3896,  3.6219,  2.0272,  4.3527,  4.8225,  2.4896,\n",
       "          4.0724,  1.2796,  4.1529,  1.5669,  4.5154,  5.2255,  4.5732,  5.2348,\n",
       "          6.6031,  7.9478,  7.6087,  5.5336,  8.9007,  6.9986,  9.0629,  7.2478,\n",
       "          7.5873,  7.2370,  8.0176,  7.4196,  6.8805,  7.0515,  8.7453,  6.7770,\n",
       "          5.7419,  8.3283,  8.5734,  7.8886,  8.8710,  7.9695,  7.3486,  7.9480,\n",
       "          8.1781,  8.9018,  6.1641,  6.8513,  6.0547,  4.2587,  5.6902,  5.6418,\n",
       "          4.4415,  5.7136,  5.4761,  4.1892,  5.7500,  8.8783,  8.4516,  7.9876,\n",
       "          7.2845,  8.0348,  7.4532,  6.9267,  6.7684,  7.1594,  7.9175]),\n",
       " tensor([ 1.1617,  3.1285,  1.1425,  2.9066, -0.2473,  1.6789,  5.5695,  3.4744,\n",
       "          4.1910,  5.7034,  7.2866,  6.0454,  5.9729,  5.2277,  4.9422,  5.2739,\n",
       "          7.7596,  5.3931,  6.5589,  4.9515,  4.8260,  7.5903,  4.6301,  4.9998,\n",
       "          4.7837,  5.2400,  4.3563,  5.4042,  6.7766,  5.9919,  6.7366,  6.3247,\n",
       "          5.3094,  5.5809,  5.7391,  5.3876,  6.7532,  2.5693,  4.3243,  5.2744,\n",
       "          4.7213,  4.8286,  4.2824,  8.4289,  6.4762,  4.9566,  7.0629,  6.3336,\n",
       "          3.4222,  7.7188,  4.2045,  5.9411,  7.8853,  5.3301,  6.4085,  4.7312,\n",
       "          6.4880,  4.1685,  5.0425,  4.4296,  5.8602,  6.2335,  5.9575,  4.1700,\n",
       "          6.3917,  5.8017,  5.6740,  4.7879,  4.6047,  4.9150,  5.4607,  4.8387,\n",
       "          6.1880,  5.6695,  4.4208,  6.1062,  2.4042,  4.8964,  4.1079,  1.2125,\n",
       "          2.2285,  1.4793,  1.9718,  0.5377,  1.1949,  0.7586,  1.7473,  3.0448,\n",
       "          4.2528,  2.9199,  2.8478,  0.8343,  1.5167,  2.9404,  0.5555,  0.9021,\n",
       "          0.7838,  1.2076,  2.0105,  1.9321,  1.3645,  4.0641,  1.1206,  1.9572,\n",
       "          1.0456,  1.4531,  1.6152,  0.7485, -0.9509,  5.7061,  4.6662,  6.0373,\n",
       "          6.5818,  5.2994,  6.5591,  7.7134,  5.4960,  2.4234,  2.8208,  3.1866,\n",
       "          2.8703,  3.3462,  2.5599,  3.8810,  3.7327,  5.3723,  7.0598]),\n",
       " 127)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2533fdfd7c0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x253482bdf70>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        num_hiddens=4\n",
    "        self.num_states=4\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION()\n",
    "        self.A2=ATTENTION()\n",
    "        self.A3=ATTENTION()\n",
    "        self.A4=ATTENTION()\n",
    "        self.A5=ATTENTION()\n",
    "        self.A6=ATTENTION()\n",
    "        self.A7=ATTENTION()\n",
    "        self.A8=ATTENTION()\n",
    "        self.A9=ATTENTION()\n",
    "        self.A10=ATTENTION()\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "        #print(cumsum_mu)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))+1e-20\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))+1e-20\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        #print(pred)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()*1.0\n",
    "        pred1_base1=pred1.clone()*1.0\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))+1e-20\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))+1e-20\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "\n",
    "        #print(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base)) +1e-20\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base)) +1e-20\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone().float()\n",
    "        pred1_base1=pred1.clone().float()\n",
    "\n",
    "        pred=pred.float()\n",
    "        pred1=pred1.float()\n",
    "\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base)) +1e-20\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base)) +1e-20\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()*1.0\n",
    "        pred1_base1=pred1.clone()*1.0\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()*1.0\n",
    "        pred1=pred1_1.clone()*1.0\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbansal5\\AppData\\Local\\Temp\\2\\ipykernel_63552\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\vbansal5\\AppData\\Local\\Temp\\2\\ipykernel_63552\\1628520156.py:222: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\vbansal5\\AppData\\Local\\Temp\\2\\ipykernel_63552\\1628520156.py:223: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2311.953125\n",
      "tensor([112.5364,  58.0870, -10.1550, -98.0286], device='cuda:0')\n",
      "tensor([ 1.7584,  4.2038,  7.1290, 10.6372], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vbansal5\\AppData\\Local\\Temp\\2\\ipykernel_63552\\1628520156.py:341: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
      "C:\\Users\\vbansal5\\AppData\\Local\\Temp\\2\\ipykernel_63552\\1628520156.py:342: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2289.3037109375\n",
      "tensor([ 114.0327,   52.2193,   -3.5243, -100.3518], device='cuda:0')\n",
      "tensor([ 1.6537,  4.0132,  6.9221, 10.5248], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2299.276611328125\n",
      "tensor([ 127.4733,   55.2382,   -8.0503, -101.0882], device='cuda:0')\n",
      "tensor([ 1.6852,  4.0942,  7.0001, 10.5446], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2289.83154296875\n",
      "tensor([ 111.2227,   43.8835,   -3.6997, -103.1399], device='cuda:0')\n",
      "tensor([ 1.6587,  3.9989,  6.8617, 10.3856], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2298.1064453125\n",
      "tensor([ 126.5486,   55.6245,   -9.2602, -100.2732], device='cuda:0')\n",
      "tensor([ 1.6891,  4.1055,  7.0175, 10.5777], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2310.541015625\n",
      "tensor([126.5285,  59.5680,  -6.5863, -95.6041], device='cuda:0')\n",
      "tensor([ 1.6994,  4.1305,  7.0909, 10.6187], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2290.6201171875\n",
      "tensor([  98.4968,   47.0933,   -3.2691, -104.8801], device='cuda:0')\n",
      "tensor([ 1.6302,  3.9913,  6.8895, 10.4592], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2306.48681640625\n",
      "tensor([130.4101,  59.6471, -10.2175, -98.9420], device='cuda:0')\n",
      "tensor([ 1.7261,  4.1731,  7.1175, 10.6581], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2287.5771484375\n",
      "tensor([ 100.0886,   46.4681,   -1.7066, -103.0817], device='cuda:0')\n",
      "tensor([ 1.6243,  3.9695,  6.8577, 10.4848], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "501\n",
      "501\n",
      "tensor(50100, device='cuda:0')\n",
      "tensor(50100, device='cuda:0')\n",
      "-2291.7255859375\n",
      "tensor([ 114.0746,   49.4949,   -4.9193, -104.7999], device='cuda:0')\n",
      "tensor([ 1.6605,  4.0112,  6.9012, 10.4825], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 5000\n",
    "n_trials = 10\n",
    "learning_rate = 0.001\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _,_ in pred1]\n",
    "    observations_o1 = [o for _, o, _,_ in pred1]\n",
    "    true_states = [t.numpy()[:,0:1] for _, _, _,t in pred1]\n",
    "    true_states1 = [t.numpy()[:,1:2] for _, _, _,t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "    # m_1[:,:-1000]=1\n",
    "    # m_2[:,:-1000]=1\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        # l1=torch.nan_to_num(l1)\n",
    "        # l2=torch.nan_to_num(l2)\n",
    "        # print(l1)\n",
    "        # print(l2)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "    m1=m1.cpu()\n",
    "    m2=m2.cpu()\n",
    "    for i in range(0,batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.758354 ,  4.203844 ,  7.1289864, 10.637243 ], dtype=float32),\n",
       " array([ 1.6536808,  4.013229 ,  6.922077 , 10.524752 ], dtype=float32),\n",
       " array([ 1.6851518,  4.0942345,  7.0001097, 10.544578 ], dtype=float32),\n",
       " array([ 1.6587483,  3.9989462,  6.8616867, 10.385643 ], dtype=float32),\n",
       " array([ 1.6891127,  4.1054945,  7.01746  , 10.57773  ], dtype=float32),\n",
       " array([ 1.6993563,  4.1304855,  7.090892 , 10.618658 ], dtype=float32),\n",
       " array([ 1.6301651,  3.9912722,  6.889524 , 10.459214 ], dtype=float32),\n",
       " array([ 1.7261261,  4.173128 ,  7.117499 , 10.658137 ], dtype=float32),\n",
       " array([ 1.6242956,  3.969521 ,  6.8577266, 10.484761 ], dtype=float32),\n",
       " array([ 1.6604908,  4.011245 ,  6.901157 , 10.482488 ], dtype=float32)]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc1): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2415.848388671875"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2343.41748046875, -2351.34619140625]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 2347.3818359375, Std Loss: 3.96435546875\n",
      "Mean Training Time: 148.04913997650146s, Std Training Time: 0.08489441871643066s\n",
      "Final mu values (across trials): [[ 2.4670436  5.881177   9.886644  12.507506 ]\n",
      " [ 2.4676795  6.084865   8.647102  13.144716 ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 2])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Filtering MSE Results:\n",
      "Average MSE for l1: 0.8533, Std Dev: 0.0205\n",
      "Average MSE for l2: 0.8550, Std Dev: 0.0210\n",
      "Trial 1: MSE l1 = 0.8259, MSE l2 = 0.8252\n",
      "Trial 2: MSE l1 = 0.8509, MSE l2 = 0.8677\n",
      "Trial 3: MSE l1 = 0.8492, MSE l2 = 0.8547\n",
      "Trial 4: MSE l1 = 0.8926, MSE l2 = 0.8929\n",
      "Trial 5: MSE l1 = 0.8483, MSE l2 = 0.8370\n",
      "Trial 6: MSE l1 = 0.8370, MSE l2 = 0.8384\n",
      "Trial 7: MSE l1 = 0.8734, MSE l2 = 0.8726\n",
      "Trial 8: MSE l1 = 0.8244, MSE l2 = 0.8279\n",
      "Trial 9: MSE l1 = 0.8608, MSE l2 = 0.8693\n",
      "Trial 10: MSE l1 = 0.8706, MSE l2 = 0.8640\n"
     ]
    }
   ],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8258682634730539,\n",
       " 0.8509181636726547,\n",
       " 0.8492215568862276,\n",
       " 0.8926147704590819,\n",
       " 0.8482834331337326,\n",
       " 0.8370459081836328,\n",
       " 0.873373253493014,\n",
       " 0.8244311377245509,\n",
       " 0.8608383233532935,\n",
       " 0.8706387225548902]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8252095808383234,\n",
       " 0.8677245508982036,\n",
       " 0.8546706586826347,\n",
       " 0.8928942115768463,\n",
       " 0.8369660678642714,\n",
       " 0.8384431137724551,\n",
       " 0.8725948103792415,\n",
       " 0.8279041916167664,\n",
       " 0.8692814371257485,\n",
       " 0.8639520958083833]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 2880\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
