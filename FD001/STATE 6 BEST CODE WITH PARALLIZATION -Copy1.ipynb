{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.5647, 0.8397, 0.0669, 2.1669, 1.5503, 1.1846, 1.8434, 1.1654, 1.5909,\n",
       "         2.2010, 2.8509, 0.9913, 2.6032, 5.0497, 4.2302, 1.8903, 6.3816, 3.8510,\n",
       "         5.7694, 6.2977, 7.5529, 5.0953, 4.9833, 5.0690, 3.1362, 3.5129, 5.1975,\n",
       "         5.9836, 3.7792, 5.7210, 4.8450, 6.8723, 5.9905, 7.6032, 6.7872, 7.7624,\n",
       "         6.8356]),\n",
       " tensor([3.5932, 1.2244, 1.8918, 0.9722, 0.3196, 1.1175, 2.6474, 5.6609, 6.2670,\n",
       "         5.0563, 8.1186, 8.5619, 5.2282, 5.4092, 6.2162, 5.7065, 4.6347, 3.4664,\n",
       "         3.2322, 3.1118, 3.7951, 4.6609, 3.9956, 2.7582, 9.7730, 8.6012, 4.8317,\n",
       "         3.4614, 5.4371, 4.0120, 6.0033, 4.9802, 3.4954, 5.4311, 3.7773, 4.4464,\n",
       "         5.8434]),\n",
       " 37)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x248ebc2c950>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x248ebc7af00>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=6):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13748\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13748\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13748\\2278689434.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13748\\2278689434.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-357.59893798828125\n",
      "tensor([-19.6906,  -5.5586,   0.9148,   0.8377,   2.3877,   0.1908],\n",
      "       device='cuda:0')\n",
      "tensor([1.3017, 1.8686, 2.2857, 2.4033, 2.9597, 3.6944], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-389.3458557128906\n",
      "tensor([9.4345e-01, 4.4926e-01, 3.2845e+00, 5.7857e-01, 2.5190e-02, 2.2956e-03],\n",
      "       device='cuda:0')\n",
      "tensor([1.4829, 2.3156, 3.2613, 6.1442, 6.2919, 6.4378], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-351.8290100097656\n",
      "tensor([ 1.2508e+02,  4.2926e+01,  1.6029e+01,  7.1665e+00, -2.9577e-01,\n",
      "        -1.4660e-02], device='cuda:0')\n",
      "tensor([1.2989, 1.8749, 2.4011, 2.9423, 3.6631, 3.6726], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-362.5729675292969\n",
      "tensor([-14.1222,  -9.6784,  -7.3431,  -5.2695,  -1.8859,   0.2985],\n",
      "       device='cuda:0')\n",
      "tensor([0.7257, 1.2235, 1.7429, 2.2464, 2.7625, 3.5593], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-352.27777099609375\n",
      "tensor([-4.1412e+02, -1.4463e+02, -7.5975e+01, -4.0720e+01, -1.7104e+01,\n",
      "        -1.0921e-02], device='cuda:0')\n",
      "tensor([1.2879, 1.8463, 2.3730, 2.9099, 3.6608, 5.9205], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-366.54339599609375\n",
      "tensor([45.4785, 21.5618, 14.0611,  3.1334, 10.7365,  0.8904], device='cuda:0')\n",
      "tensor([1.3320, 1.9556, 2.3730, 2.5403, 3.4152, 5.8476], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-345.39324951171875\n",
      "tensor([20.8711,  8.9017, 17.6792,  5.8171,  2.2188, -0.6042], device='cuda:0')\n",
      "tensor([1.2635, 1.7685, 2.2341, 2.6880, 3.2504, 3.9030], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-356.54461669921875\n",
      "tensor([7.2406e+01, 3.2930e+01, 4.4123e+01, 3.3067e+01, 1.3868e+01, 6.2047e-02],\n",
      "       device='cuda:0')\n",
      "tensor([1.2970, 1.8663, 2.4024, 2.9586, 3.6956, 5.8342], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-351.2784729003906\n",
      "tensor([-46.0569, -14.3474,  -6.5887,  -2.6746,  -1.2858,  -1.4625],\n",
      "       device='cuda:0')\n",
      "tensor([1.2382, 1.7059, 2.1599, 2.6094, 3.1468, 3.8170], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-355.5516662597656\n",
      "tensor([-8.0573, -6.3686, -4.0840, -1.1361, -0.0465,  0.6229], device='cuda:0')\n",
      "tensor([1.3210, 1.9232, 2.4742, 3.0302, 3.0547, 3.7592], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.3016655, 1.8686476, 2.2856843, 2.4033062, 2.9597023, 3.6943629],\n",
       "       dtype=float32),\n",
       " array([1.4828991, 2.3155923, 3.2612925, 6.1441917, 6.291933 , 6.4378486],\n",
       "       dtype=float32),\n",
       " array([1.298872 , 1.8749082, 2.4010503, 2.9423473, 3.663125 , 3.6725688],\n",
       "       dtype=float32),\n",
       " array([0.7257329, 1.2235354, 1.742897 , 2.2464113, 2.762529 , 3.559314 ],\n",
       "       dtype=float32),\n",
       " array([1.2878622, 1.8462598, 2.3730311, 2.9098978, 3.6608002, 5.920483 ],\n",
       "       dtype=float32),\n",
       " array([1.3319612, 1.9556037, 2.3729804, 2.5402937, 3.4152324, 5.8475685],\n",
       "       dtype=float32),\n",
       " array([1.2635287, 1.7685461, 2.2340734, 2.6879551, 3.250405 , 3.903027 ],\n",
       "       dtype=float32),\n",
       " array([1.2969987, 1.8662603, 2.4024436, 2.9586058, 3.6955948, 5.8342123],\n",
       "       dtype=float32),\n",
       " array([1.2381667, 1.7059152, 2.1598632, 2.6093533, 3.1467924, 3.8169816],\n",
       "       dtype=float32),\n",
       " array([1.3210093, 1.9231625, 2.474173 , 3.0302467, 3.0546515, 3.7592103],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=7, out_features=6, bias=False)\n",
       "  (fc1): Linear(in_features=7, out_features=6, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=6, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=6, bias=True)\n",
       "  (fc4): Linear(in_features=6, out_features=6, bias=True)\n",
       "  (fc5): Linear(in_features=6, out_features=6, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAu0UlEQVR4nO3df3RU9Z3/8df8SCY/SAYSTIaRiGHFAgYrBmsFFFoQ2oq26674A6mu1C9WQVOwIsXdWrcSZbdIW1tcratWSnHdytZu6ZaglpaChQ2g/EYrQpDECISZBJKZZObz/SNyNzP8HEhyb8jzcc6ck7n3nZn3/YTDvM7n3vsZlzHGCAAAwEHcdjcAAACQjIACAAAch4ACAAAch4ACAAAch4ACAAAch4ACAAAch4ACAAAch4ACAAAcx2t3A2ciHo9r3759ysnJkcvlsrsdAABwGowxqq+vVzAYlNt98jmSLhlQ9u3bp6KiIrvbAAAAZ6Cqqkp9+/Y9aU2XDCg5OTmSWg8wNzfX5m4AAMDpCIfDKioqsj7HT6ZLBpSjp3Vyc3MJKAAAdDGnc3kGF8kCAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADH6ZJfFthRWg4c0P5n/k3ujAwVzJxhdzsAAHRbzKC0EQuFVffyy6p75RW7WwEAoFsjoLR19NufjbG1DQAAujsCShsu16cJhYACAICtCChtEVAAAHAEAkpb7tbhMAQUAABsRUBpixkUAAAcgYCSgIACAIATEFDacHEXDwAAjkBAaYtTPAAAOAIBpS0CCgAAjkBAaevTgEI8AQDAXgSUtphBAQDAEQgoCQgoAAA4AQGlLe7iAQDAEQgobVjfxROP29sIAADdHAGlLTfDAQCAE/CJ3Ja1UhvfxwMAgJ0IKG21CShchwIAgH0IKCdCQAEAwDYElDZczKAAAOAIBJS2CCgAADgCAaUtAgoAAI5AQGmr7V08NrYBAEB3R0BpixkUAAAcgYCSoE1AYTVZAABsQ0Bpo+0ECjMoAADYh4DSVtul7gkoAADYhoDSVsJS9zb2AQBAN0dAaSvxHI9tbQAA0N0RUNriLh4AAByBgNJG2/kTAgoAAPYhoLTFDAoAAI5AQGmLgAIAgCMQUNpKuIuHgAIAgF0IKG0xgwIAgCMQUNoioAAA4AgElDZcBBQAAByBgJLsaEghoAAAYBsCSrJPAwoXyQIAYB8CSjJrBsXeNgAA6M5SCigtLS165JFHVFxcrMzMTPXv31+PPfaY4vG4VWOM0aOPPqpgMKjMzEyNHj1aW7ZsSXidSCSi6dOnq3fv3srOztYNN9ygvXv3ts8RnS3rOhQSCgAAdkkpoDz55JN65pln9PTTT2vbtm2aN2+e/uVf/kU//vGPrZp58+Zp/vz5evrpp7Vu3ToFAgFde+21qq+vt2rKysq0dOlSLVmyRKtWrVJDQ4MmTJigWCzWfkd2prgGBQAA27lMChdbTJgwQYWFhXr++eetbX/3d3+nrKwsvfzyyzLGKBgMqqysTLNmzZLUOltSWFioJ598UlOnTlUoFNJ5552nl19+WTfffLMkad++fSoqKtKyZcs0fvz4U/YRDofl9/sVCoWUm5ub6jGf1PYhl8o0N+uit95UWp8+7fraAAB0Z6l8fqc0gzJy5Ei98cYb2rlzpyTpnXfe0apVq/SVr3xFkrRr1y7V1NRo3Lhx1u/4fD6NGjVKq1evliRVVlaqubk5oSYYDKqkpMSqSRaJRBQOhxMeHeboDEqb01YAAKBzeVMpnjVrlkKhkAYOHCiPx6NYLKbHH39ct956qySppqZGklRYWJjwe4WFhdq9e7dVk56erl69eh1Tc/T3k5WXl+t73/teKq2eOesuns55OwAAcKyUZlBeeeUVLVq0SIsXL9b69ev10ksv6V//9V/10ksvJdQlLHim1gtnk7clO1nN7NmzFQqFrEdVVVUqbaeGi2QBALBdSjMo3/72t/Xwww/rlltukSQNGTJEu3fvVnl5ue644w4FAgFJrbMkfdpcv1FbW2vNqgQCAUWjUdXV1SXMotTW1mr48OHHfV+fzyefz5fakZ0pLpIFAMB2Kc2gHDlyRG534q94PB7rNuPi4mIFAgFVVFRY+6PRqFauXGmFj9LSUqWlpSXUVFdXa/PmzScMKJ2KgAIAgO1SmkG5/vrr9fjjj+uCCy7QJZdcog0bNmj+/Pm66667JLWe2ikrK9PcuXM1YMAADRgwQHPnzlVWVpZuu+02SZLf79eUKVM0c+ZM5efnKy8vTw8++KCGDBmisWPHtv8RpsjlcrWe3CGgAABgm5QCyo9//GP94z/+o+69917V1tYqGAxq6tSp+qd/+ier5qGHHlJjY6Puvfde1dXV6corr9Ty5cuVk5Nj1Tz11FPyer2aOHGiGhsbNWbMGL344ovyeDztd2RnihkUAABsl9I6KE7Rkeug7Ljic4rX16v/75bJV1zcrq8NAEB31mHroHQLfBcPAAC2I6Ak4zZjAABsR0BJYq3EwkqyAADYhoCSjItkAQCwHQElmbXUPQEFAAC7EFCScZEsAAC2I6Ak4yJZAABsR0BJxjUoAADYjoCSxEVAAQDAdgSUZAQUAABsR0BJxl08AADYjoCSjLt4AACwHQElmRVQWEkWAAC7EFCSWXcZM4UCAIBdCChJXOIiWQAA7EZAScZdPAAA2I6Akoy7eAAAsB0BJRl38QAAYDsCSjK+iwcAANsRUJKw1D0AAPYjoCQjoAAAYDsCSrKjF8nGWagNAAC7EFCScZEsAAC2I6Ak4xQPAAC2I6AkO3oTD1MoAADYhoCShLt4AACwHwHlGAQUAADsRkBJxlL3AADYjoCSjLt4AACwHQElGdegAABgOwJKMjffxQMAgN0IKElcRy+SZSVZAABsQ0BJxkWyAADYjoCSjGtQAACwHQElicvjaf2BUzwAANiGgJLM65UkmZaYzY0AANB9EVCSWDMosRZ7GwEAoBsjoCRxeVsDiokxgwIAgF0IKMk8nOIBAMBuBJQknOIBAMB+BJRkR0/xMIMCAIBtCChJXEdP8TCDAgCAbQgoSf7vFA8zKAAA2IWAkoxTPAAA2I6AkoRTPAAA2I+AkuToOiic4gEAwD4ElGQeTvEAAGA3AkoSTvEAAGA/AkoS6xQPMygAANiGgJLMmkEhoAAAYBcCShKWugcAwH4ElCSuNL4sEAAAuxFQkh29i4cZFAAAbENASWLdxdNCQAEAwC4ElCSu9HRJkolGbe4EAIDui4CSxOX7NKBECCgAANiFgJLE7fNJkkwkYnMnAAB0XwSUJC5fhiTJRAkoAADYhYCS5OgpnngTAQUAALsQUJJwigcAAPsRUJK4jgYUTvEAAGAbAkqSowEl8t77NncCAED3lXJA+eijj3T77bcrPz9fWVlZuuyyy1RZWWntN8bo0UcfVTAYVGZmpkaPHq0tW7YkvEYkEtH06dPVu3dvZWdn64YbbtDevXvP/mjawdFTPJJ0eO1aGzsBAKD7Simg1NXVacSIEUpLS9Pvfvc7bd26VT/4wQ/Us2dPq2bevHmaP3++nn76aa1bt06BQEDXXnut6uvrrZqysjItXbpUS5Ys0apVq9TQ0KAJEyYo5oBvEHa1CSiN6zfY2AkAAN2XN5XiJ598UkVFRXrhhResbRdeeKH1szFGCxYs0Jw5c3TjjTdKkl566SUVFhZq8eLFmjp1qkKhkJ5//nm9/PLLGjt2rCRp0aJFKioq0ooVKzR+/Ph2OKwz1zagyM0ZMAAA7JDSJ/Drr7+uYcOG6aabblJBQYGGDh2q5557ztq/a9cu1dTUaNy4cdY2n8+nUaNGafXq1ZKkyspKNTc3J9QEg0GVlJRYNckikYjC4XDCo6McXeq+9UmHvQ0AADiJlALKBx98oIULF2rAgAH6/e9/r3vuuUf333+/fv7zn0uSampqJEmFhYUJv1dYWGjtq6mpUXp6unr16nXCmmTl5eXy+/3Wo6ioKJW2U+LOyPi/Jw445QQAQHeUUkCJx+O6/PLLNXfuXA0dOlRTp07V3XffrYULFybUuVyJUw/GmGO2JTtZzezZsxUKhaxHVVVVKm2npO0Miok2d9j7AACAE0spoPTp00eDBw9O2DZo0CDt2bNHkhQIBCTpmJmQ2tpaa1YlEAgoGo2qrq7uhDXJfD6fcnNzEx4dpW1Aadq2rcPeBwAAnFhKAWXEiBHasWNHwradO3eqX79+kqTi4mIFAgFVVFRY+6PRqFauXKnhw4dLkkpLS5WWlpZQU11drc2bN1s1dnK1uTC24a23bOwEAIDuK6W7eL71rW9p+PDhmjt3riZOnKi1a9fq2Wef1bPPPiup9dROWVmZ5s6dqwEDBmjAgAGaO3eusrKydNttt0mS/H6/pkyZopkzZyo/P195eXl68MEHNWTIEOuuHgAA0L2lFFCuuOIKLV26VLNnz9Zjjz2m4uJiLViwQJMmTbJqHnroITU2Nuree+9VXV2drrzySi1fvlw5OTlWzVNPPSWv16uJEyeqsbFRY8aM0YsvviiPx9N+RwYAALoslzHG2N1EqsLhsPx+v0KhUIdcj7Jt4CDr50HbuQ4FAID2kMrnNyuRHYcnP1+S1POWm23uBACA7omAchzxI0ckSYeWvGJzJwAAdE8ElOMwjY12twAAQLdGQDmO7BEj7G4BAIBujYByHLnXXWd3CwAAdGsElOOoX77c7hYAAOjWCCjH0fOmv7d+NvG4jZ0AANA9EVCOI/uqq6yfmzvwiwkBAMDxEVCOw52VJZfPJ0lq2rnT5m4AAOh+CCgnkDN+nCSp4a0/2NsIAADdEAHlBFxpaZKk0Guv2dwJAADdDwHlBPLuuMP6Obpnj42dAADQ/RBQTiDj4ovlu/hiSVJ9xQqbuwEAoHshoJyE/8a/lSQdWbfO5k4AAOheCCgnkTF4sCQpwp08AAB0KgLKSWR85jOSpOZ9+xSrr7e5GwAAug8Cykl4/H55+/SRJEXee8/mbgAA6D4IKKeQFghIklr277e5EwAAug8Cyil4/H5JUiwUsrkTAAC6DwLKKRwNKHECCgAAnYaAcgqensygAADQ2Qgop+A+eorn0CF7GwEAoBshoJyCJ/fTgFLfYHMnAAB0HwSUU/Dk9JAkxcNhmzsBAKD7IKCcgjsnV5JYqA0AgE5EQDkFawaFgAIAQKchoJyCO5cZFAAAOhsB5RQ8PZhBAQCgsxFQTuHoDIqJRhWPRGzuBgCA7oGAcgru7GzJ5ZLELAoAAJ2FgHIKLrdb7k9P88TCBBQAADoDAeU0uK07eVgLBQCAzkBAOQ0eay0UVpMFAKAzEFBOAzMoAAB0LgLKafCwmiwAAJ2KgHIa3KwmCwBApyKgnAZmUAAA6FwElNNgzaBwmzEAAJ2CgHIavHl5kqSW/ftt7gQAgO6BgHIa0vr2lSQ1f/SRzZ0AANA9EFBOQ3pxsSQp8t57ih85YnM3AACc+wgopyH9wguVFgzKRKPacXmpYg2HdfAXv9C2gYPUtHOn3e0BAHDOIaCcBpfLpR6jR1nPdw4bpo//+fuSpF03fNWutgAAOGcRUE5T4cMPn3Dfof/8z07sBACAcx8B5TS50tM1aPs25d999zH7an/4Q8WbmmzoCgCAcxMBJUUFM2do0PZtGrhtqwa++468wT6KfbJfDX9YaXdrAACcMwgoZ8jlcsmVnq7sK66QJEV377a5IwAAzh0ElLPkLSiUJLUcYBE3AADaCwHlLHl6+iVJsUOH7G0EAIBzCAHlLHl69pQkxUIhexsBAOAcQkA5Sx4/MygAALQ3AspZsmZQCCgAALQbAspZOjqDEg+Fbe4EAIBzBwHlLLlzPz3FEw7LxOM2dwMAwLmBgHKWPP7c1h/iccUPH7a3GQAAzhEElLPkzsiQy+eTJMU4zQMAQLsgoLQD606e0CF7GwEA4BxBQGkHR0/zxMPMoAAA0B4IKO3A4+8pSWo5eNDeRgAAOEcQUNqBN9hHktTwxps2dwIAwLmBgNIO3J9eJBtetszmTgAAODcQUNpBr1tvtX7eUTpMsQZuNwYA4GycVUApLy+Xy+VSWVmZtc0Yo0cffVTBYFCZmZkaPXq0tmzZkvB7kUhE06dPV+/evZWdna0bbrhBe/fuPZtWbJUxeLD1c/zwYe0cNkyRv/7Vxo4AAOjazjigrFu3Ts8++6wuvfTShO3z5s3T/Pnz9fTTT2vdunUKBAK69tprVV9fb9WUlZVp6dKlWrJkiVatWqWGhgZNmDBBsVjszI/EZhf/7/8mPN9z1xSbOgEAoOs7o4DS0NCgSZMm6bnnnlOvXr2s7cYYLViwQHPmzNGNN96okpISvfTSSzpy5IgWL14sSQqFQnr++ef1gx/8QGPHjtXQoUO1aNEibdq0SStWrGifo7KBp0e2Bm3fpvMXLJAktXz8sVr277e3KQAAuqgzCij33XefrrvuOo0dOzZh+65du1RTU6Nx48ZZ23w+n0aNGqXVq1dLkiorK9Xc3JxQEwwGVVJSYtUki0QiCofDCQ+nyv3SeKWdf74k6eCLL9rbDAAAXVTKAWXJkiVav369ysvLj9lXU1MjSSosLEzYXlhYaO2rqalRenp6wsxLck2y8vJy+f1+61FUVJRq250q1tAgSTrws+dt7gQAgK4ppYBSVVWlBx54QIsWLVJGRsYJ61wuV8JzY8wx25KdrGb27NkKhULWo6qqKpW2O12PkSPtbgEAgC4tpYBSWVmp2tpalZaWyuv1yuv1auXKlfrRj34kr9drzZwkz4TU1tZa+wKBgKLRqOrq6k5Yk8zn8yk3Nzfh4WS9v3mP9XP0ww/tawQAgC4qpYAyZswYbdq0SRs3brQew4YN06RJk7Rx40b1799fgUBAFRUV1u9Eo1GtXLlSw4cPlySVlpYqLS0toaa6ulqbN2+2aro630UXWT83bt5ykkoAAHA8KQWUnJwclZSUJDyys7OVn5+vkpISa02UuXPnaunSpdq8ebPuvPNOZWVl6bbbbpMk+f1+TZkyRTNnztQbb7yhDRs26Pbbb9eQIUOOuei2K8u/+xuSpH0PPqhtAwfpoxkzFD984gXcjDGKtbkV+1zSXFurcEWFTDxudysAgC7C294v+NBDD6mxsVH33nuv6urqdOWVV2r58uXKycmxap566il5vV5NnDhRjY2NGjNmjF588UV5PJ72bsc2eXfdpfo331L00wXbwst+p/Cy3ymjpEQyRk1bTjyz0m/xYnl6+tW8r1pV32gNOgO3bZWJRvXJgh8qvfhC5X75y9p5xeda923dIpf7xFnTGCOp9dqglk8+kSs9XR6/v70O9ZR23z5ZzXv2KPcrX9b58+d32vsCALoulzn66dWFhMNh+f1+hUIhR1+P0lJXp/dGXi05cAG6zGGlSusTVPg3v7G2Fc5+WC2HDunAwmckSb6BA6VYi1y+DOXffbfS+12gtL5FitUdVOjXr8t/w/VyZ2Yq3tSk2n/9gQq/M1t1ixYpY9Ag5XzpS/q4/AmlF1+ojx/75/97j398RHmTJnX68XZXsYbDih3Yr/R+/exuBQBS+vwmoHQCE4+rZf9+Hf7jH+Xu0UMttbX6eG650oqK1OzwO5I6wvk/+qFy26yDg47z3ugvqKWmRv1/87p8AwbY3Q6Abo6Acg4w8bjiR47I5XbLxGJq2rJVLR/XKK2oSO7sbEV27lTLgQPyf/WravjDSkU/+Ksa/vxnRbZuO63Xd/l8MpFIBx/FyQ1Ys1repPVw0L62DRwkqfWaqIKZM23uBkB3R0BBuzDGyESjcnk8aq75WC3V+9RysE6mpVkZAweq/o03deBnP1OvmydKXq/UElPT9u1qevddxUIhpffvr+gHH0iSCh6epbzbblPtgh/q4L//+zHv1WfuXOWM+aJcGRlyeb2S233KtXM6U9O2bWrZv189rr76lLUtBw/K4/fL5YBrqo4GFEkatP30wisAdBQCChztk5/8RPt//HSHvX6vyZN1+E9/kv9v/1bZw69S0/btMo2NanznXWWPGKEe11ytWDisltpaZX3uc1I8rnhDg+pefVV5kyfLnbQIoYnHtX3wJdbzi1aulCenh9xZWce8d9PWrdp1499Jki5et1aeNheHt4f44cNyZ2cfd58xRvu+/ZBcXq/6lM/VeyNGKnbwoLW/70+eVs6YMWf1/tGqKv312tbTcwO3be2wEGmMkWKx1rAK4JxBQIHjHVm/Xrtv42LZE8kZP171v/+9JOnCV/9DH9400drXa/JkBeZ8R5IUj0ZV/Z05iuzcqfyp/0/7Zj4oSSp+7VdWUGqr5003yVtYqIMvv6yCGTNU/8YKqaVFWVdcofDvlyuyfbvyvzFF2VdfI1//Yr139TXqMWqU4pGI8qdMUdXddye8Xv7UqTrwb/+mnPHjlX7hhYqFDsnty1BGySXKGDxYTVu2aN9Ds9R34U/VuPEdpRf1lf+rX5UkudLSZJqbFXr9N6qvqFDO2DFqrq1V76lTteOyoTLNzZISZ37arjjdsn+/Ppw0SedNv1/NVXuUWVoqT48e2lv2LTXv2aO/Wf57pV9wQXv9SVrfPx6XXC5Hze4BXQkBBV2KicUU3bVL4d/9j45UVurI22/b3dI55byZMxTZvkPh3/7W7la6pOC8JxU7dEgZQ4Zo962t6zkVPDxLnpxcVc+Zo543/b0C3/ueZIxioZBcbreiez+St+A8udPT5fL5FG9qkjs7Wy63O2FWKPL++wovW6a8f/iH4862xRoaWn8vKRDFGxtbT4eeYVAysZgjTkGi+yGgoNux/hkbo8h778m0tKi5qkour1eR9/+qyAd/lTe/t5r37VPLJ5+ocf36lN8j/56pOvDMv7Vz5x1vwOo/y5uXp8Nr1ij0298q9J+/srslOMiF//GKmrZsUc33HlPWVZ9XwYwZ+vCmiXL36KGi556VNz9fzdU1atq8WfHDh+W76G8UXl6h8+6fLm9+vlwZGWres0fxw4fV+O67yiwtlYlEVL/iDeXdPklpwaBa9u9vnX2KxeQtLJTL7Vas4bBMc9S6UL65ulqevDy5fT5F3ntPTTt3KmfMGMUPH5Y3P7+1prZW8XA4YbXuVNh16vB0vo+uuyCgAA5kjJGamyWvt/XurE//w3alpUmSYocOKd7UJG9hoRo3bJSJRpV52Wd18MUX5bv4Ynl69ZLvoosU/etftW/OI9YigFLrXTp1//Gq4qGQ+i1erOpHHlH0gw+U/817VPDAAyftyeVyqeXAAXlycuRKT1e0qkpHKisVff995V5/vTw5OYp8sEtNm97VkXX/q973flOZl12W8J+8icUUb2iQicUkY3R41So1V1eracsW9Zw4sfVi63SfWvbvV93ixWratCmhj4zBg5VxyWA17dippnffbT2mb94jE43q4PPHXlR9Khf8/CXte/hhteyrTvl3cW5wZ2UpfuSI9Tzn2mtV3+YrVpL1vOkmHXr11RPu933mM4rs2CGlpUktLfINGKDe06cpvW9fyRi1fPKJPHl5MpGImrZtV+alQ1T/hz9Y60pJUt6ddypjSInSAgH5Lr5Y1d+Zo4xLBitr2DCl9++vWCikeEODfJ/5jEw0Knd2tqIffijveefJ5XLJlZVlBZ3YoUOKHzmitGBQsXBYsYMH5fb7Wy/Qd7vVuGWL9n7zXvX7xSKlFxWddKzikYhcaWkJC34aY6R4vN1n2ggoAJAiY4xkTMJ/0vGmJkV27JCJx+XOzlbLxx8r45JLFP7v30omroxLLlEsXK/q2bOV8dlLlfGZgWrcvKn1gzEWl8vnU9O2bcoaOlSH//xnG48O3Zm3oEAmHpcnN1funB5SLC4Tiymy7f+u70rrd4F8FxbLqDUStOyrVs748Tpv2n3t2gsBBQAg6dSnF8ynd7G5c3KklhZFq/YqLVAoud2KNzYq+uGHrdfOpKUpHomoZf9+udxuHfj3F9Rj5AgdXvO20vr0kfe83oru3tN6bUx6mjIvu0y1T85Tjy9+QS01Hyu6Z4+yRwxXdNeHaly/vnUtppaWk6607cnPV/oFF6hxw4aOGJqz5s7NlTsjQ/FIRPFQyO522p07O1sXvbFCnp492+01CSgAANig7Xefneg0iTGmdckAn0/xSFQuj1vxw4et0Na8d6/S+/VTtKpK7qwsudPTFW9qkuJxxcJheQsLpVhMzdU1konLxONKKyiQp1cvRauq5HK5FKuvV+bQy9VSU614U0SKxxQ/0qj4kcOt/Xi8itUdVHTXh/L0zpc3L6/1C2s/vUbHGKPsz3/+lKeHUkVAAQAAjpPK5/eJvwIXAADAJgQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOAQUAADgOCkFlPLycl1xxRXKyclRQUGBvva1r2nHjh0JNcYYPfroowoGg8rMzNTo0aO1ZcuWhJpIJKLp06erd+/eys7O1g033KC9e/ee/dEAAIBzQkoBZeXKlbrvvvv09ttvq6KiQi0tLRo3bpwOHz5s1cybN0/z58/X008/rXXr1ikQCOjaa69VfX29VVNWVqalS5dqyZIlWrVqlRoaGjRhwgTFYrH2OzIAANBluYwx5kx/+ZNPPlFBQYFWrlypa665RsYYBYNBlZWVadasWZJaZ0sKCwv15JNPaurUqQqFQjrvvPP08ssv6+abb5Yk7du3T0VFRVq2bJnGjx9/yvcNh8Py+/0KhULKzc090/YBAEAnSuXz+6yuQQmFQpKkvLw8SdKuXbtUU1OjcePGWTU+n0+jRo3S6tWrJUmVlZVqbm5OqAkGgyopKbFqkkUiEYXD4YQHAAA4d51xQDHGaMaMGRo5cqRKSkokSTU1NZKkwsLChNrCwkJrX01NjdLT09WrV68T1iQrLy+X3++3HkVFRWfaNgAA6ALOOKBMmzZN7777rn75y18es8/lciU8N8Ycsy3ZyWpmz56tUChkPaqqqs60bQAA0AWcUUCZPn26Xn/9db311lvq27evtT0QCEjSMTMhtbW11qxKIBBQNBpVXV3dCWuS+Xw+5ebmJjwAAMC5K6WAYozRtGnT9Nprr+nNN99UcXFxwv7i4mIFAgFVVFRY26LRqFauXKnhw4dLkkpLS5WWlpZQU11drc2bN1s1AACge/OmUnzfffdp8eLF+vWvf62cnBxrpsTv9yszM1Mul0tlZWWaO3euBgwYoAEDBmju3LnKysrSbbfdZtVOmTJFM2fOVH5+vvLy8vTggw9qyJAhGjt2bPsfIQAA6HJSCigLFy6UJI0ePTph+wsvvKA777xTkvTQQw+psbFR9957r+rq6nTllVdq+fLlysnJseqfeuopeb1eTZw4UY2NjRozZoxefPFFeTyeszsaAABwTjirdVDswjooAAB0PZ22DgoAAEBHIKAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAAAADHIaAcR3OsWS3xFrvbAACg27I1oPz0pz9VcXGxMjIyVFpaqj/96U92tiNJ2rJ/iy5fdLlG/HyYjhwO290OAADdkm0B5ZVXXlFZWZnmzJmjDRs26Oqrr9aXv/xl7dmzx66WJEm3/PYWSdIRV0w/+1mJrb0AANBd2RZQ5s+frylTpugb3/iGBg0apAULFqioqEgLFy60qyV9GPow4flzPf32NAIAQDdnS0CJRqOqrKzUuHHjEraPGzdOq1evtqMlSdL77ySeYuqfdYFNnQAA0L157XjT/fv3KxaLqbCwMGF7YWGhampqjqmPRCKKRCLW83C4Y64NGXzRCGlX68/F0Rb9+o7fdsj7AACAk7P1IlmXy5Xw3BhzzDZJKi8vl9/vtx5FRUUd0k8w2F/fL5iiOVlf1et3b+uQ9wAAAKdmywxK79695fF4jpktqa2tPWZWRZJmz56tGTNmWM/D4XCHhZSvfrmsQ14XAACcPltmUNLT01VaWqqKioqE7RUVFRo+fPgx9T6fT7m5uQkPAABw7rJlBkWSZsyYocmTJ2vYsGG66qqr9Oyzz2rPnj2655577GoJAAA4hG0B5eabb9aBAwf02GOPqbq6WiUlJVq2bJn69etnV0sAAMAhXMYYY3cTqQqHw/L7/QqFQpzuAQCgi0jl85vv4gEAAI5DQAEAAI5DQAEAAI5DQAEAAI5DQAEAAI5DQAEAAI5DQAEAAI5DQAEAAI5DQAEAAI5j21L3Z+Po4rfhcNjmTgAAwOk6+rl9OovYd8mAUl9fL0kqKiqyuRMAAJCq+vp6+f3+k9Z0ye/iicfj2rdvn3JycuRyudr1tcPhsIqKilRVVcX3/HQgxrlzMM6dg3HuPIx15+iocTbGqL6+XsFgUG73ya8y6ZIzKG63W3379u3Q98jNzeUffydgnDsH49w5GOfOw1h3jo4Y51PNnBzFRbIAAMBxCCgAAMBxCChJfD6fvvvd78rn89ndyjmNce4cjHPnYJw7D2PdOZwwzl3yIlkAAHBuYwYFAAA4DgEFAAA4DgEFAAA4DgEFAAA4DgGljZ/+9KcqLi5WRkaGSktL9ac//cnulhyrvLxcV1xxhXJyclRQUKCvfe1r2rFjR0KNMUaPPvqogsGgMjMzNXr0aG3ZsiWhJhKJaPr06erdu7eys7N1ww03aO/evQk1dXV1mjx5svx+v/x+vyZPnqxDhw519CE6Unl5uVwul8rKyqxtjHP7+eijj3T77bcrPz9fWVlZuuyyy1RZWWntZ6zPXktLix555BEVFxcrMzNT/fv312OPPaZ4PG7VMM6p++Mf/6jrr79ewWBQLpdL//Vf/5WwvzPHdM+ePbr++uuVnZ2t3r176/7771c0Gk39oAyMMcYsWbLEpKWlmeeee85s3brVPPDAAyY7O9vs3r3b7tYcafz48eaFF14wmzdvNhs3bjTXXXedueCCC0xDQ4NV88QTT5icnBzzq1/9ymzatMncfPPNpk+fPiYcDls199xzjzn//PNNRUWFWb9+vfnCF75gPvvZz5qWlhar5ktf+pIpKSkxq1evNqtXrzYlJSVmwoQJnXq8TrB27Vpz4YUXmksvvdQ88MAD1nbGuX0cPHjQ9OvXz9x5553mL3/5i9m1a5dZsWKFef/9960axvrsff/73zf5+fnmv//7v82uXbvMq6++anr06GEWLFhg1TDOqVu2bJmZM2eO+dWvfmUkmaVLlybs76wxbWlpMSUlJeYLX/iCWb9+vamoqDDBYNBMmzYt5WMioHzqc5/7nLnnnnsStg0cONA8/PDDNnXUtdTW1hpJZuXKlcYYY+LxuAkEAuaJJ56wapqamozf7zfPPPOMMcaYQ4cOmbS0NLNkyRKr5qOPPjJut9v8z//8jzHGmK1btxpJ5u2337Zq1qxZYySZ7du3d8ahOUJ9fb0ZMGCAqaioMKNGjbICCuPcfmbNmmVGjhx5wv2Mdfu47rrrzF133ZWw7cYbbzS33367MYZxbg/JAaUzx3TZsmXG7Xabjz76yKr55S9/aXw+nwmFQikdB6d4JEWjUVVWVmrcuHEJ28eNG6fVq1fb1FXXEgqFJEl5eXmSpF27dqmmpiZhTH0+n0aNGmWNaWVlpZqbmxNqgsGgSkpKrJo1a9bI7/fryiuvtGo+//nPy+/3d6u/zX333afrrrtOY8eOTdjOOLef119/XcOGDdNNN92kgoICDR06VM8995y1n7FuHyNHjtQbb7yhnTt3SpLeeecdrVq1Sl/5ylckMc4doTPHdM2aNSopKVEwGLRqxo8fr0gkknC69HR0yS8LbG/79+9XLBZTYWFhwvbCwkLV1NTY1FXXYYzRjBkzNHLkSJWUlEiSNW7HG9Pdu3dbNenp6erVq9cxNUd/v6amRgUFBce8Z0FBQbf52yxZskTr16/XunXrjtnHOLefDz74QAsXLtSMGTP0ne98R2vXrtX9998vn8+nr3/964x1O5k1a5ZCoZAGDhwoj8ejWCymxx9/XLfeeqsk/k13hM4c05qammPep1evXkpPT0953AkobbhcroTnxphjtuFY06ZN07vvvqtVq1Yds+9MxjS55nj13eVvU1VVpQceeEDLly9XRkbGCesY57MXj8c1bNgwzZ07V5I0dOhQbdmyRQsXLtTXv/51q46xPjuvvPKKFi1apMWLF+uSSy7Rxo0bVVZWpmAwqDvuuMOqY5zbX2eNaXuNO6d4JPXu3Vsej+eYdFdbW3tMEkSi6dOn6/XXX9dbb72lvn37WtsDgYAknXRMA4GAotGo6urqTlrz8ccfH/O+n3zySbf421RWVqq2tlalpaXyer3yer1auXKlfvSjH8nr9VpjwDifvT59+mjw4MEJ2wYNGqQ9e/ZI4t90e/n2t7+thx9+WLfccouGDBmiyZMn61vf+pbKy8slMc4doTPHNBAIHPM+dXV1am5uTnncCSiS0tPTVVpaqoqKioTtFRUVGj58uE1dOZsxRtOmTdNrr72mN998U8XFxQn7i4uLFQgEEsY0Go1q5cqV1piWlpYqLS0toaa6ulqbN2+2aq666iqFQiGtXbvWqvnLX/6iUCjULf42Y8aM0aZNm7Rx40brMWzYME2aNEkbN25U//79Ged2MmLEiGNuld+5c6f69esniX/T7eXIkSNyuxM/ejwej3WbMePc/jpzTK+66ipt3rxZ1dXVVs3y5cvl8/lUWlqaWuMpXVJ7Djt6m/Hzzz9vtm7dasrKykx2drb58MMP7W7Nkb75zW8av99v/vCHP5jq6mrrceTIEavmiSeeMH6/37z22mtm06ZN5tZbbz3ubW19+/Y1K1asMOvXrzdf/OIXj3tb26WXXmrWrFlj1qxZY4YMGXLO3ip4OtrexWMM49xe1q5da7xer3n88cfNe++9Z37xi1+YrKwss2jRIquGsT57d9xxhzn//POt24xfe+0107t3b/PQQw9ZNYxz6urr682GDRvMhg0bjCQzf/58s2HDBmupjM4a06O3GY8ZM8asX7/erFixwvTt25fbjM/WT37yE9OvXz+Tnp5uLr/8cuuWWRxL0nEfL7zwglUTj8fNd7/7XRMIBIzP5zPXXHON2bRpU8LrNDY2mmnTppm8vDyTmZlpJkyYYPbs2ZNQc+DAATNp0iSTk5NjcnJyzKRJk0xdXV0nHKUzJQcUxrn9/OY3vzElJSXG5/OZgQMHmmeffTZhP2N99sLhsHnggQfMBRdcYDIyMkz//v3NnDlzTCQSsWoY59S99dZbx/0/+Y477jDGdO6Y7t6921x33XUmMzPT5OXlmWnTppmmpqaUj8lljDGpzbkAAAB0LK5BAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjkNAAQAAjvP/AcF5Bf7vu/G6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "355.51617431640625"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-357.59893798828125,\n",
       " -389.3458557128906,\n",
       " -351.8290100097656,\n",
       " -362.5729675292969,\n",
       " -352.27777099609375,\n",
       " -366.54339599609375,\n",
       " -345.39324951171875,\n",
       " -356.54461669921875,\n",
       " -351.2784729003906,\n",
       " -355.5516662597656]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 358.8935943603516, Std Loss: 11.626205936526324\n",
      "Mean Training Time: 3053.6800161600113s, Std Training Time: 682.8931717421044s\n",
      "Final mu values (across trials): [[1.3016655 1.8686476 2.2856843 2.4033062 2.9597023 3.6943629]\n",
      " [1.4828991 2.3155923 3.2612925 6.1441917 6.291933  6.4378486]\n",
      " [1.298872  1.8749082 2.4010503 2.9423473 3.663125  3.6725688]\n",
      " [0.7257329 1.2235354 1.742897  2.2464113 2.762529  3.559314 ]\n",
      " [1.2878622 1.8462598 2.3730311 2.9098978 3.6608002 5.920483 ]\n",
      " [1.3319612 1.9556037 2.3729804 2.5402937 3.4152324 5.8475685]\n",
      " [1.2635287 1.7685461 2.2340734 2.6879551 3.250405  3.903027 ]\n",
      " [1.2969987 1.8662603 2.4024436 2.9586058 3.6955948 5.8342123]\n",
      " [1.2381667 1.7059152 2.1598632 2.6093533 3.1467924 3.8169816]\n",
      " [1.3210093 1.9231625 2.474173  3.0302467 3.0546515 3.7592103]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
