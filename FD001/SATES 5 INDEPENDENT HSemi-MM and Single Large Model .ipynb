{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)  # Set to any fixed integer\n",
    "\n",
    "\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # If using multiple GPUs\n",
    "import numpy as np\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_matrix=torch.tensor([[0.9, 0.1, 0.0,0.0],\n",
    "        [0.00, 0.9, 0.1,0],\n",
    "        [0.0, 0.00, 0.9,.1],\n",
    "                      [0.0,0.0,0.0,1]], device=device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9000, 0.1000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.9000, 0.1000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.9000, 0.1000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[0.8571, 0.1429, 0.0000, 0.0000],\n",
       "         [0.0000, 0.8571, 0.1429, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8571, 0.1429],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[0.8000, 0.2000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.8000, 0.2000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.8000, 0.2000],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000]],\n",
       "\n",
       "        [[0.6667, 0.3333, 0.0000, 0.0000],\n",
       "         [0.0000, 0.6667, 0.3333, 0.0000],\n",
       "         [0.0000, 0.0000, 0.6667, 0.3333],\n",
       "         [0.0000, 0.0000, 0.0000, 1.0000]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9000, 0.1000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9000, 0.1000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9000, 0.1000],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,2.5,3.5,5.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.1,1.2,1.3,1.4]).to(device)\n",
    "\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.5\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import torch\n",
    "# torch.manual_seed(42)\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29088\\1739442728.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\autograd\\tracer.py:48: RuntimeWarning: divide by zero encountered in log\n",
      "  return f_raw(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0dce3f2d81408da4bb26afb4d45d47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36715449e8bc4cbc9b47637eb35b14b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM for 'o': Mean Likelihood = -82.32641371535017, Std = 22.852191583749295, Filtration Error = 0\n",
      "HMM for 'o1': Mean Likelihood = -329.139200044297, Std = 70.13032440343007, Filtration Error = 0\n"
     ]
    }
   ],
   "source": [
    "from hmmlearn import hmm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "# Load data\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Extract observations and underlying states\n",
    "observations_o = [o.numpy().reshape(-1, 1) for o, _, _ in pred1]\n",
    "observations_o1 = [o.numpy().reshape(-1, 1) for _, o, _ in pred1]\n",
    "true_states = 0\n",
    "true_states1 = 0\n",
    "\n",
    "import numpy as np\n",
    "import ssm\n",
    "\n",
    "def train_hmm_and_evaluate(observations, n_components=5, obs_type=\"gaussian\", n_iter=100):\n",
    "    \"\"\"\n",
    "    Train a Hidden Semi-Markov Model (HSMM) on the provided observations.\n",
    "\n",
    "    Parameters:\n",
    "    - observations: List of sequences of observations (each sequence should be a 2D numpy array).\n",
    "    - n_components: The number of states in the HSMM.\n",
    "    - obs_type: Type of observation model (\"gaussian\", \"bernoulli\", etc.).\n",
    "    - n_iter: Number of EM iterations for training.\n",
    "\n",
    "    Returns:\n",
    "    - model: Trained HSMM model.\n",
    "    - mean_likelihood: Mean log-likelihood of the observation sequences.\n",
    "    - std_likelihood: Standard deviation of log-likelihoods.\n",
    "    - filtered_states: List of most likely states for each observation sequence.\n",
    "    \"\"\"\n",
    "    # Flatten the data and create lengths array\n",
    "    lengths = [len(seq) for seq in observations]\n",
    "    X = np.concatenate(observations, axis=0)\n",
    "\n",
    "    # Initialize HSMM\n",
    "    hsmm = ssm.HSMM(K=n_components, D=X.shape[1], observations=obs_type,transition_kwargs={'r_min' : 1, 'r_max' : 3})\n",
    "\n",
    "    # Fit the HSMM using the EM algorithm\n",
    "    hsmm.fit(X, method=\"em\")\n",
    "\n",
    "    # Calculate log-likelihoods for each sequence\n",
    "    log_likelihoods = [hsmm.log_likelihood(seq) for seq in observations]\n",
    "    mean_likelihood = np.mean(log_likelihoods)\n",
    "    std_likelihood = np.std(log_likelihoods)\n",
    "\n",
    "    # Get filtered states for each sequence\n",
    "    filtered_states = [hsmm.most_likely_states(seq) for seq in observations]\n",
    "\n",
    "    return hsmm, mean_likelihood, std_likelihood, filtered_states\n",
    "\n",
    "\n",
    "# Train HMMs for o and o1 observations\n",
    "model_o, mean_likelihood_o, std_likelihood_o, filtered_states_o = train_hmm_and_evaluate(observations_o)\n",
    "model_o1, mean_likelihood_o1, std_likelihood_o1, filtered_states_o1 = train_hmm_and_evaluate(observations_o1)\n",
    "\n",
    "# Calculate state filtration errors\n",
    "def calculate_filtration_error(filtered_states, true_states):\n",
    "    total_errors = 0\n",
    "    total_states = 0\n",
    "    for filtered, true in zip(filtered_states, true_states):\n",
    "        # print(((filtered == true)).shape)\n",
    "        total_errors +=  (np.diag(filtered == true)).sum()\n",
    "        total_states += len(true)\n",
    "        # print(total_states)\n",
    "    return total_errors / total_states\n",
    "\n",
    "error_o = 0\n",
    "error_o1 = 0\n",
    "\n",
    "# Print results\n",
    "print(f\"HMM for 'o': Mean Likelihood = {mean_likelihood_o}, Std = {std_likelihood_o}, Filtration Error = {error_o}\")\n",
    "print(f\"HMM for 'o1': Mean Likelihood = {mean_likelihood_o1}, Std = {std_likelihood_o1}, Filtration Error = {error_o1}\")\n",
    "# print(model_o.means_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rearrange_states(filtered_states, hmm_means):\n",
    "    \"\"\"\n",
    "    Rearranges states based on the order of HMM means.\n",
    "    \n",
    "    Args:\n",
    "        filtered_states (list of np.ndarray): List of filtered states.\n",
    "        hmm_means (np.ndarray): The means of the Gaussian HMM components.\n",
    "        \n",
    "    Returns:\n",
    "        list of np.ndarray: Rearranged filtered states.\n",
    "    \"\"\"\n",
    "    # Get the new state order by sorting the means\n",
    "    sorted_indices = np.argsort(hmm_means.flatten())\n",
    "    state_mapping = {old: new for new, old in enumerate(sorted_indices)}\n",
    "\n",
    "    # Rearrange states in the filtered results\n",
    "    rearranged_states = []\n",
    "    for states in filtered_states:\n",
    "        rearranged_states.append(np.vectorize(state_mapping.get)(states))\n",
    "    \n",
    "    return rearranged_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d4c0e192004459916a6476146d001b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e50135f39414a8e9ad303f90803910a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d79638a3574a25a1000946221e7aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1947fb9f4f41738699fa135ce6ac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4713c00fa43b4d359144b96405732d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43889ae74f34f1ca869fa630a761794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9afb8fa0b2be4985aa79126cf676ed53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c2ee417f874b09b19b614489721c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f39c1f458e4491587c0e04d2da93324",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "014953bbd4f241048b1ebdfd6efc5dae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017183774d1c49069556779a643eca5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53b76e3111804868aa90a9539e35df77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de46f9c3ab5046fcbf27ac1f663f2566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfc798893274931b607df9dca2a1d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07193a1f3b6c44b7a5bc18657a001e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a58f8bda8834b4e9280bdb6d875e5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399261331c5a431eaef597b19eeb34e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fde0ec66cb4f95925d207b828cf714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b6222641a0a448eba97d0caf655ee1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191,)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb79f54ce1e424b870edc087e7ee350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM for 'o':\n",
      "Mean Likelihood: -82.36587412574731, Std: 0.023319734497885058\n",
      "Mean Filtration Error: 0.0, Std: 0.0\n",
      "Mean Computation Time: 18.112204909324646s, Std: 1.5876760018229465s\n",
      "\n",
      "HMM for 'o1':\n",
      "Mean Likelihood: -329.1349627531049, Std: 0.023373054380530285\n",
      "Mean Filtration Error: 0.0, Std: 0.0\n",
      "Mean Computation Time: 33.550010895729066s, Std: 2.082631115095724s\n"
     ]
    }
   ],
   "source": [
    "# Repeat training multiple times and collect metrics\n",
    "import time\n",
    "n_repeats = 10\n",
    "results_o = []\n",
    "results_o1 = []\n",
    "time_o = []\n",
    "time_o1 = []\n",
    "\n",
    "for _ in range(n_repeats):\n",
    "    start_time = time.time()\n",
    "    model_o, mean_likelihood_o, std_likelihood_o, filtered_states_o = train_hmm_and_evaluate(observations_o)\n",
    "    hmm_means = model_o.observations.mus\n",
    "    filtered_states_o = rearrange_states(filtered_states_o, hmm_means)\n",
    "    print(filtered_states_o[0].shape)\n",
    " \n",
    "    elapsed_time = time.time() - start_time\n",
    "    error_o = 0\n",
    "    results_o.append((mean_likelihood_o, std_likelihood_o, error_o))\n",
    "    time_o.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    model_o1, mean_likelihood_o1, std_likelihood_o1, filtered_states_o1 = train_hmm_and_evaluate(observations_o1)\n",
    "    hmm_means = model_o1.observations.mus\n",
    "    filtered_states_o1 = rearrange_states(filtered_states_o1, hmm_means)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    error_o1 =0\n",
    "    results_o1.append((mean_likelihood_o1, std_likelihood_o1, error_o1))\n",
    "    time_o1.append(elapsed_time)\n",
    "\n",
    "# Calculate mean and standard deviation for metrics\n",
    "metrics_o = np.array(results_o)\n",
    "metrics_o1 = np.array(results_o1)\n",
    "\n",
    "mean_metrics_o = metrics_o.mean(axis=0)\n",
    "std_metrics_o = metrics_o.std(axis=0)\n",
    "\n",
    "mean_metrics_o1 = metrics_o1.mean(axis=0)\n",
    "std_metrics_o1 = metrics_o1.std(axis=0)\n",
    "\n",
    "# Calculate mean and standard deviation for computation time\n",
    "mean_time_o = np.mean(time_o)\n",
    "std_time_o = np.std(time_o)\n",
    "\n",
    "mean_time_o1 = np.mean(time_o1)\n",
    "std_time_o1 = np.std(time_o1)\n",
    "\n",
    "# Print results\n",
    "print(\"HMM for 'o':\")\n",
    "print(f\"Mean Likelihood: {mean_metrics_o[0]}, Std: {std_metrics_o[0]}\")\n",
    "print(f\"Mean Filtration Error: {mean_metrics_o[2]}, Std: {std_metrics_o[2]}\")\n",
    "print(f\"Mean Computation Time: {mean_time_o}s, Std: {std_time_o}s\\n\")\n",
    "\n",
    "print(\"HMM for 'o1':\")\n",
    "print(f\"Mean Likelihood: {mean_metrics_o1[0]}, Std: {std_metrics_o1[0]}\")\n",
    "print(f\"Mean Filtration Error: {mean_metrics_o1[2]}, Std: {std_metrics_o1[2]}\")\n",
    "print(f\"Mean Computation Time: {mean_time_o1}s, Std: {std_time_o1}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HMM for 'TOTAL':\n",
      "Mean Likelihood: -411.5008368788523, Std: 0.04565758811455304\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "print(\"HMM for 'TOTAL':\")\n",
    "print(f\"Mean Likelihood: {(metrics_o1+metrics_o).mean(axis=0)[0]}, Std: {(metrics_o1+metrics_o).std(axis=0)[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-82.41532376668155, 22.85441726652963, 0),\n",
       " (-82.33679517924662, 22.833862839098682, 0),\n",
       " (-82.38664250832088, 22.831349139878935, 0),\n",
       " (-82.36980105315718, 22.894441650734386, 0),\n",
       " (-82.34876359846366, 22.74493945045241, 0),\n",
       " (-82.35071725488649, 22.836065178918407, 0),\n",
       " (-82.36870557615168, 22.858587101885124, 0),\n",
       " (-82.34287216948098, 22.8405813938756, 0),\n",
       " (-82.3520426392951, 22.853768343029934, 0),\n",
       " (-82.38707751178904, 22.84692137119518, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-329.1834643552928, 70.09144389243511, 0),\n",
       " (-329.1260483979948, 70.152241131887, 0),\n",
       " (-329.17035630819663, 70.10921646337927, 0),\n",
       " (-329.1375115731756, 70.13342382704212, 0),\n",
       " (-329.10992714430984, 70.0987367537029, 0),\n",
       " (-329.1159220818473, 70.09380549280753, 0),\n",
       " (-329.13319618695755, 70.13432022934921, 0),\n",
       " (-329.111869812421, 70.09799077831694, 0),\n",
       " (-329.12043683730843, 70.10943605185781, 0),\n",
       " (-329.1408948335457, 70.14823959661788, 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m observations_o \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mconcatenate([o\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),o1\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m o, o1, _,_ \u001b[38;5;129;01min\u001b[39;00m pred1]\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "observations_o = [np.concatenate([o.numpy().reshape(-1, 1),o1.numpy().reshape(-1, 1)], axis=1) for o, o1, _,_ in pred1]\n",
    "# observations_o = [np.concatenate([o.numpy().reshape(-1, 1),o1.numpy().reshape(-1, 1)], axis=1) for o, o1, _,_ in pred1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat training multiple times and collect metrics\n",
    "import time\n",
    "n_repeats = 10\n",
    "results_o = []\n",
    "results_o1 = []\n",
    "time_o = []\n",
    "time_o1 = []\n",
    "\n",
    "for _ in range(n_repeats):\n",
    "    start_time = time.time()\n",
    "    model_o, mean_likelihood_o, std_likelihood_o, filtered_states_o = train_hmm_and_evaluate(observations_o, n_components=4)\n",
    "    hmm_means = model_o.observations.mus\n",
    "    filtered_states_o = rearrange_states(filtered_states_o, hmm_means)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    error_o = 0\n",
    "    results_o.append((mean_likelihood_o, std_likelihood_o, error_o))\n",
    "    time_o.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "# Calculate mean and standard deviation for metrics\n",
    "metrics_o = np.array(results_o)\n",
    "\n",
    "\n",
    "mean_metrics_o = metrics_o.mean(axis=0)\n",
    "std_metrics_o = metrics_o.std(axis=0)\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation for computation time\n",
    "mean_time_o = np.mean(time_o)\n",
    "std_time_o = np.std(time_o)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"HMM for 'o':\")\n",
    "print(f\"Mean Likelihood: {mean_metrics_o[0]}, Std: {std_metrics_o[0]}\")\n",
    "print(f\"Mean Filtration Error: {mean_metrics_o[2]}, Std: {std_metrics_o[2]}\")\n",
    "print(f\"Mean Computation Time: {mean_time_o}s, Std: {std_time_o}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat training multiple times and collect metrics\n",
    "import time\n",
    "n_repeats = 10\n",
    "results_o = []\n",
    "results_o1 = []\n",
    "time_o = []\n",
    "time_o1 = []\n",
    "\n",
    "for _ in range(n_repeats):\n",
    "    start_time = time.time()\n",
    "    model_o, mean_likelihood_o, std_likelihood_o, filtered_states_o = train_hmm_and_evaluate(observations_o, n_components=8)\n",
    "    hmm_means = model_o.observations.mus\n",
    "    filtered_states_o = rearrange_states(filtered_states_o, hmm_means)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    error_o = 0\n",
    "    results_o.append((mean_likelihood_o, std_likelihood_o, error_o))\n",
    "    time_o.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "# Calculate mean and standard deviation for metrics\n",
    "metrics_o = np.array(results_o)\n",
    "\n",
    "\n",
    "mean_metrics_o = metrics_o.mean(axis=0)\n",
    "std_metrics_o = metrics_o.std(axis=0)\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation for computation time\n",
    "mean_time_o = np.mean(time_o)\n",
    "std_time_o = np.std(time_o)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"HMM for 'o':\")\n",
    "print(f\"Mean Likelihood: {mean_metrics_o[0]}, Std: {std_metrics_o[0]}\")\n",
    "print(f\"Mean Filtration Error: {mean_metrics_o[2]}, Std: {std_metrics_o[2]}\")\n",
    "print(f\"Mean Computation Time: {mean_time_o}s, Std: {std_time_o}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat training multiple times and collect metrics\n",
    "import time\n",
    "n_repeats = 10\n",
    "results_o = []\n",
    "results_o1 = []\n",
    "time_o = []\n",
    "time_o1 = []\n",
    "\n",
    "for _ in range(n_repeats):\n",
    "    start_time = time.time()\n",
    "    model_o, mean_likelihood_o, std_likelihood_o, filtered_states_o = train_hmm_and_evaluate(observations_o, n_components=16)\n",
    "    hmm_means = model_o.observations.mus\n",
    "    filtered_states_o = rearrange_states(filtered_states_o, hmm_means)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    error_o = 0\n",
    "    results_o.append((mean_likelihood_o, std_likelihood_o, error_o))\n",
    "    time_o.append(elapsed_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "# Calculate mean and standard deviation for metrics\n",
    "metrics_o = np.array(results_o)\n",
    "\n",
    "\n",
    "mean_metrics_o = metrics_o.mean(axis=0)\n",
    "std_metrics_o = metrics_o.std(axis=0)\n",
    "\n",
    "\n",
    "# Calculate mean and standard deviation for computation time\n",
    "mean_time_o = np.mean(time_o)\n",
    "std_time_o = np.std(time_o)\n",
    "\n",
    "\n",
    "# Print results\n",
    "print(\"HMM for 'o':\")\n",
    "print(f\"Mean Likelihood: {mean_metrics_o[0]}, Std: {std_metrics_o[0]}\")\n",
    "print(f\"Mean Filtration Error: {mean_metrics_o[2]}, Std: {std_metrics_o[2]}\")\n",
    "print(f\"Mean Computation Time: {mean_time_o}s, Std: {std_time_o}s\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
