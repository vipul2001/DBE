{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.5479,  3.2215,  2.4365,  0.6853,  0.8554,  0.9159,  2.6710,  2.2233,\n",
       "         -0.1686,  2.9843,  1.4648,  4.5075,  4.0692,  3.8508,  4.5972,  5.8720,\n",
       "          4.0631,  3.0704,  4.4791,  4.0818,  1.6249,  1.3546,  0.1000,  1.5773,\n",
       "          0.5991,  2.1473,  1.8445,  2.0972,  0.4935,  3.2618,  1.9072,  1.9349,\n",
       "          1.0198,  1.0192,  0.5831,  3.2829,  4.7930,  3.3433,  3.0644,  3.5635,\n",
       "          3.3306,  4.4336,  2.6478,  4.0929,  2.2132,  3.2766,  1.8492,  3.7453,\n",
       "          1.0101,  4.7955,  4.1699,  5.2598,  5.5890,  3.7431,  5.6802,  6.5477,\n",
       "          3.3756,  5.4386,  4.8547,  6.5949,  5.3636,  4.9999,  4.7772,  5.3478,\n",
       "          6.5626,  4.9264,  5.4575,  4.4071,  4.4837,  6.4981,  5.4464,  3.8082,\n",
       "          4.9631,  6.9141,  4.3463,  4.3502,  5.0893,  8.2295,  7.2309,  6.4354,\n",
       "          7.6669,  7.1627,  7.7724,  6.1383,  6.1725,  7.3185,  7.0089,  5.4340,\n",
       "          8.1964,  9.5749,  7.3816,  6.2128,  7.4494]),\n",
       " tensor([-0.1169,  1.8132,  0.7305,  1.4036,  3.2928,  3.4565,  3.1502,  5.9748,\n",
       "          1.8401,  4.1067,  2.7078,  3.3403,  0.2434,  0.5755,  2.2036,  2.1809,\n",
       "          0.3961,  1.9491,  2.0034,  2.1313,  6.3302,  7.6388,  5.9447,  7.0908,\n",
       "          8.5388,  7.0289,  6.7669,  4.5390,  7.7547,  7.1447,  8.1630,  7.9142,\n",
       "          7.8621,  7.8693,  8.3042,  5.8425,  5.0025,  5.0900,  8.6607,  7.9818,\n",
       "          8.9559,  6.5518,  7.7769,  9.3427,  5.9592,  9.3929,  6.8090,  7.5384,\n",
       "          7.7909,  8.6269,  6.4889,  6.6579,  4.8503,  4.3047,  6.2167,  5.9986,\n",
       "          3.8803,  6.9025,  4.0967,  5.1004,  4.9937,  6.4137,  5.6853,  7.2904,\n",
       "          5.7282,  5.0845,  7.1298,  4.2597,  5.2720,  5.0362,  5.4005,  6.0531,\n",
       "          5.4562,  5.3781,  5.8047,  6.1150,  3.3416,  2.7486,  5.0624,  3.0784,\n",
       "          4.3755,  2.6982,  2.2952,  3.3571,  3.3861,  3.1649,  2.4668,  3.0501,\n",
       "          4.6764,  3.9155,  3.7007,  5.2578,  6.5028]),\n",
       " 93)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2955a351160>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2955a3db770>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11668\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11668\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11668\\2202253750.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_11668\\2202253750.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-393.07696533203125\n",
      "tensor([-12.3887,  -7.4007,  -4.7517,  -1.9357], device='cuda:0')\n",
      "tensor([1.3297, 1.9529, 2.5425, 3.4232], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-394.4491882324219\n",
      "tensor([73.0029, 29.6537, 20.7110,  9.8732], device='cuda:0')\n",
      "tensor([1.3296, 1.9524, 2.5382, 3.4153], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-419.2174072265625\n",
      "tensor([-3.3413e+01, -1.3270e+01, -4.5179e+00,  1.8862e-03], device='cuda:0')\n",
      "tensor([1.4786, 2.3087, 3.2580, 6.1441], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-394.72332763671875\n",
      "tensor([26.8370, 10.7157,  6.9639,  3.3020], device='cuda:0')\n",
      "tensor([1.3292, 1.9529, 2.5362, 3.4043], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-395.1961669921875\n",
      "tensor([6.0320, 2.0157, 0.7210, 1.2592], device='cuda:0')\n",
      "tensor([1.3327, 1.9576, 2.5392, 3.4140], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-394.3716735839844\n",
      "tensor([ 0.3387, -1.2393, -5.9742,  2.0188], device='cuda:0')\n",
      "tensor([1.3335, 1.9595, 2.5359, 3.4082], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-395.27020263671875\n",
      "tensor([ 3.0046,  0.7923,  1.7943, -0.1344], device='cuda:0')\n",
      "tensor([1.3309, 1.9595, 2.5461, 3.4209], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-394.20587158203125\n",
      "tensor([24.6339, 11.0170,  7.7387,  2.4450], device='cuda:0')\n",
      "tensor([1.3346, 1.9698, 2.5541, 3.4276], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-395.94378662109375\n",
      "tensor([-30.2085, -19.7266,  -5.4606,   1.2158], device='cuda:0')\n",
      "tensor([1.3355, 1.9633, 2.5440, 3.4135], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-420.4075927734375\n",
      "tensor([-15.8938,  -7.5940, -10.6964,  -5.3930], device='cuda:0')\n",
      "tensor([0.8802, 1.4870, 2.3227, 3.2794], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5962, 0.3554, 0.3285, 0.4805], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(net.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.3296852, 1.9528537, 2.5424743, 3.4232035], dtype=float32),\n",
       " array([1.3296276, 1.9523696, 2.5381775, 3.4153295], dtype=float32),\n",
       " array([1.4785986, 2.3086748, 3.257966 , 6.1441402], dtype=float32),\n",
       " array([1.3292245, 1.9529123, 2.5361884, 3.4043357], dtype=float32),\n",
       " array([1.3326727, 1.9575534, 2.5391688, 3.4140327], dtype=float32),\n",
       " array([1.3335063, 1.9594676, 2.5359201, 3.4082003], dtype=float32),\n",
       " array([1.3309286, 1.9595041, 2.5460804, 3.4209347], dtype=float32),\n",
       " array([1.3345788, 1.969785 , 2.5540597, 3.4276   ], dtype=float32),\n",
       " array([1.3355166, 1.9632976, 2.5440228, 3.4134805], dtype=float32),\n",
       " array([0.8802064, 1.4870148, 2.322668 , 3.2793865], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc1): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420.15606689453125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-393.07696533203125,\n",
       " -394.4491882324219,\n",
       " -419.2174072265625,\n",
       " -394.72332763671875,\n",
       " -395.1961669921875,\n",
       " -394.3716735839844,\n",
       " -395.27020263671875,\n",
       " -394.20587158203125,\n",
       " -395.94378662109375,\n",
       " -420.4075927734375]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 399.68621826171875, Std Loss: 10.09221397442903\n",
      "Mean Training Time: 2585.1689105510713s, Std Training Time: 220.93674002238254s\n",
      "Final mu values (across trials): [[1.3296852 1.9528537 2.5424743 3.4232035]\n",
      " [1.3296276 1.9523696 2.5381775 3.4153295]\n",
      " [1.4785986 2.3086748 3.257966  6.1441402]\n",
      " [1.3292245 1.9529123 2.5361884 3.4043357]\n",
      " [1.3326727 1.9575534 2.5391688 3.4140327]\n",
      " [1.3335063 1.9594676 2.5359201 3.4082003]\n",
      " [1.3309286 1.9595041 2.5460804 3.4209347]\n",
      " [1.3345788 1.969785  2.5540597 3.4276   ]\n",
      " [1.3355166 1.9632976 2.5440228 3.4134805]\n",
      " [0.8802064 1.4870148 2.322668  3.2793865]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
