{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.5730, 3.2719, 2.5377, 3.0875, 3.7927, 2.2682, 3.4861, 2.2113, 3.9439,\n",
       "         4.2845, 4.0779, 3.6063, 4.3385, 2.5610, 4.3945, 5.6782, 2.5728, 6.6806,\n",
       "         5.6218, 6.1887, 4.3542, 6.1257, 6.4837, 5.8526, 5.2433, 4.4789, 6.8857,\n",
       "         7.0267, 5.0602, 5.7511, 4.7965, 4.7957, 4.2699, 6.4230, 5.8036, 4.7345,\n",
       "         4.0007, 7.0298, 8.5869, 7.1507, 5.6833, 3.4652, 4.5893, 6.0590, 5.0647,\n",
       "         6.6626, 6.1677, 7.0615, 6.4009, 7.5502, 4.9039, 5.5012, 5.8796, 5.4004,\n",
       "         4.8329, 6.0775, 2.9607, 4.8750, 5.6509, 6.6926, 3.8511, 4.6929, 5.0004,\n",
       "         5.0898, 6.0073, 6.4497, 4.8952, 6.8123, 7.7864]),\n",
       " tensor([0.7230, 0.9958, 1.5053, 2.2805, 1.3348, 1.2434, 1.7514, 0.7524, 0.6253,\n",
       "         2.5057, 1.4826, 1.0810, 0.2411, 3.3610, 1.4150, 1.6834, 2.2280, 1.0581,\n",
       "         0.9060, 1.5892, 2.4722, 2.5047, 0.8767, 1.0049, 2.6191, 0.2883, 1.6226,\n",
       "         1.4818, 1.4938, 1.8722, 0.1446, 2.4408, 2.1009, 2.7112, 2.5313, 1.9933,\n",
       "         5.3117, 0.5607, 2.6137, 2.6789, 5.2304, 6.1277, 5.9695, 6.7864, 5.8086,\n",
       "         6.1460, 5.2411, 6.0482, 6.8536, 5.0504, 6.5446, 4.5246, 6.1305, 6.4676,\n",
       "         6.8773, 4.1057, 4.8705, 5.6328, 4.7490, 4.5727, 5.0098, 4.9988, 5.5443,\n",
       "         4.0776, 6.0505, 6.5205, 5.6381, 5.4336, 7.1542]),\n",
       " 69)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26f15c90080>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26f16494b90>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=7):\n",
    "        super(Net, self).__init__()\n",
    "        super(Net, self).__init__()\n",
    "        num_heads = 1\n",
    "        hidden_dim = num_states\n",
    "        self.num_states = num_states\n",
    "        \n",
    "        # Used parameters only\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        \n",
    "        # Only keep used layers\n",
    "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Only keep used attention modules\n",
    "        self.A1 = ATTENTION(num_states)\n",
    "        self.A4 = ATTENTION(num_states)\n",
    "        self.A7 = ATTENTION(num_states)\n",
    "        self.A9 = ATTENTION(num_states)\n",
    "\n",
    "        # Keep all LSTM layers (all are used)\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "\n",
    "\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Dim | Parameter Count\n",
      "-------------------------\n",
      "    3     |      1158      \n",
      "    4     |      1936      \n",
      "    5     |      2910      \n",
      "    6     |      4080      \n",
      "    7     |      5446      \n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"Return total number of parameters in a model.\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def main():\n",
    "    obs_dim = 1\n",
    "    print(\"State Dim | Parameter Count\")\n",
    "    print(\"-------------------------\")\n",
    "    for sd in range(3, 8):  # state_dim from 3 to 7\n",
    "        model = Net(sd)\n",
    "        param_count = count_parameters(model)\n",
    "        print(f\"{sd:^9} | {param_count:^15}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13128\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13128\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13128\\1791965750.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_13128\\1791965750.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-349.2188720703125\n",
      "tensor([-9.8275e+00, -6.0594e+00, -7.2541e+00, -5.3708e+00, -6.3947e+00,\n",
      "         5.7327e-01,  9.0941e-05], device='cuda:0')\n",
      "tensor([1.3295, 1.9331, 2.4658, 3.0297, 3.7351, 6.6962, 6.6966],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-380.8882141113281\n",
      "tensor([-72.5928, -29.3307, -11.0294,  -4.7254,  -1.4580,  -0.3343,  -0.2307],\n",
      "       device='cuda:0')\n",
      "tensor([1.4784, 2.3131, 3.2614, 4.7613, 5.6563, 6.0816, 6.3744],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-384.20819091796875\n",
      "tensor([-4.0568e+01, -1.5204e+01, -2.6213e+00,  5.3936e+00,  1.8583e+00,\n",
      "         3.3505e-02,  5.6365e-06], device='cuda:0')\n",
      "tensor([1.4888, 2.3193, 3.2673, 5.8091, 6.6840, 6.6998, 6.7014],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-363.2932434082031\n",
      "tensor([ 2.3638e+01,  1.4293e+01,  1.4259e+01,  1.0175e+01, -5.2549e-01,\n",
      "         5.9965e-01,  5.6409e-03], device='cuda:0')\n",
      "tensor([1.3450, 1.9735, 2.5488, 3.2723, 3.4219, 6.3528, 6.3781],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-354.5265197753906\n",
      "tensor([31.2018, 14.6923, 14.3258, 13.0756, 10.7101,  0.1942,  0.1687],\n",
      "       device='cuda:0')\n",
      "tensor([1.2889, 1.8466, 2.3673, 2.8958, 3.3435, 3.4791, 3.6427],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-350.5630798339844\n",
      "tensor([-1.3245e+02, -4.4515e+01, -1.7325e+01, -9.8269e+00,  2.0657e-01,\n",
      "         1.9819e-01, -1.0260e-02], device='cuda:0')\n",
      "tensor([1.2982, 1.8578, 2.3738, 2.9078, 3.6510, 5.3711, 6.6994],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-385.0908508300781\n",
      "tensor([ 6.2845,  1.8286,  3.8122, -0.6741, -0.1162, -0.0119,  0.0000],\n",
      "       device='cuda:0')\n",
      "tensor([1.4833, 2.3088, 3.2457, 6.3598, 6.8890, 6.9432, 7.2744],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-371.6182861328125\n",
      "tensor([-5.6066e+01, -2.1507e+01, -5.8652e+00, -2.5289e+00,  1.3820e-01,\n",
      "         4.0627e-03, -4.4471e-08], device='cuda:0')\n",
      "tensor([1.4598, 2.2151, 2.7457, 3.4858, 5.6029, 6.4831, 6.6875],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-352.5039978027344\n",
      "tensor([-39.4598, -10.1315,   0.4803,   2.0158,  -0.4520,  -0.2982,   1.4430],\n",
      "       device='cuda:0')\n",
      "tensor([1.3165, 1.9339, 2.2132, 2.4757, 2.8651, 3.0703, 3.7656],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-358.9426574707031\n",
      "tensor([16.9384,  7.9677,  0.7179, -0.6379,  0.2089,  2.7215,  0.1515],\n",
      "       device='cuda:0')\n",
      "tensor([1.3459, 1.9748, 2.4542, 2.5459, 3.1874, 3.8309, 4.2736],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.3294884, 1.93313  , 2.4658005, 3.0297034, 3.7351406, 6.696224 ,\n",
       "        6.696649 ], dtype=float32),\n",
       " array([1.4783505, 2.3130639, 3.2613683, 4.7613115, 5.6563325, 6.0815597,\n",
       "        6.3744087], dtype=float32),\n",
       " array([1.4888456, 2.3192651, 3.2673469, 5.8091125, 6.684016 , 6.699791 ,\n",
       "        6.701406 ], dtype=float32),\n",
       " array([1.3449727, 1.9734883, 2.5487583, 3.2722886, 3.421867 , 6.352845 ,\n",
       "        6.378132 ], dtype=float32),\n",
       " array([1.2889122, 1.8466414, 2.3673048, 2.8958058, 3.3435428, 3.479076 ,\n",
       "        3.6426694], dtype=float32),\n",
       " array([1.2982084, 1.857847 , 2.3738396, 2.9077756, 3.650954 , 5.3711042,\n",
       "        6.69943  ], dtype=float32),\n",
       " array([1.4832788, 2.3088322, 3.245725 , 6.3598413, 6.889037 , 6.943183 ,\n",
       "        7.274362 ], dtype=float32),\n",
       " array([1.4597794, 2.2150583, 2.745658 , 3.485814 , 5.6029096, 6.4831343,\n",
       "        6.6875186], dtype=float32),\n",
       " array([1.3165035, 1.9339125, 2.2131937, 2.4757357, 2.8650782, 3.0702832,\n",
       "        3.7655897], dtype=float32),\n",
       " array([1.3459002, 1.9748173, 2.4542117, 2.545871 , 3.1874268, 3.8308783,\n",
       "        4.2736   ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=8, out_features=7, bias=False)\n",
       "  (fc1): Linear(in_features=8, out_features=7, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=7, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=7, bias=True)\n",
       "  (fc4): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (fc5): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358.65032958984375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-349.2188720703125,\n",
       " -380.8882141113281,\n",
       " -384.20819091796875,\n",
       " -363.2932434082031,\n",
       " -354.5265197753906,\n",
       " -350.5630798339844,\n",
       " -385.0908508300781,\n",
       " -371.6182861328125,\n",
       " -352.5039978027344,\n",
       " -358.9426574707031]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 365.08539123535155, Std Loss: 13.53394977784873\n",
      "Mean Training Time: 3171.2606078147887s, Std Training Time: 1022.1098498032533s\n",
      "Final mu values (across trials): [[1.3294884 1.93313   2.4658005 3.0297034 3.7351406 6.696224  6.696649 ]\n",
      " [1.4783505 2.3130639 3.2613683 4.7613115 5.6563325 6.0815597 6.3744087]\n",
      " [1.4888456 2.3192651 3.2673469 5.8091125 6.684016  6.699791  6.701406 ]\n",
      " [1.3449727 1.9734883 2.5487583 3.2722886 3.421867  6.352845  6.378132 ]\n",
      " [1.2889122 1.8466414 2.3673048 2.8958058 3.3435428 3.479076  3.6426694]\n",
      " [1.2982084 1.857847  2.3738396 2.9077756 3.650954  5.3711042 6.69943  ]\n",
      " [1.4832788 2.3088322 3.245725  6.3598413 6.889037  6.943183  7.274362 ]\n",
      " [1.4597794 2.2150583 2.745658  3.485814  5.6029096 6.4831343 6.6875186]\n",
      " [1.3165035 1.9339125 2.2131937 2.4757357 2.8650782 3.0702832 3.7655897]\n",
      " [1.3459002 1.9748173 2.4542117 2.545871  3.1874268 3.8308783 4.2736   ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
