{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.4756, 0.6804, 2.7678, 2.0367, 1.2115, 5.6837, 2.4076, 3.1805, 3.6250,\n",
       "         2.4749, 4.8266, 3.5001, 4.1192, 4.7233, 4.6596, 3.6646, 5.3520, 2.4861,\n",
       "         4.6500, 2.9121, 3.2081, 2.5721, 5.9001, 4.8265, 5.5643, 6.6938, 4.4957,\n",
       "         3.9101, 5.0831, 5.0137, 4.3819, 4.5182, 3.8860, 4.1979, 2.4972, 2.4086,\n",
       "         4.2013, 6.2038, 6.3692, 4.1848, 4.4928, 4.2619, 7.3141, 7.8300, 8.1961,\n",
       "         6.6505, 6.0761, 6.0407, 8.0512, 7.6298, 6.4939, 7.0404, 9.2992]),\n",
       " tensor([1.2270, 2.4556, 7.2330, 6.2133, 5.0222, 3.8443, 3.6849, 3.5633, 4.7637,\n",
       "         4.5349, 3.4548, 3.4774, 4.8514, 4.9840, 3.0157, 4.7972, 3.2486, 3.0399,\n",
       "         2.5727, 2.6317, 4.6973, 4.6773, 4.7125, 2.5701, 4.9295, 3.0805, 2.8504,\n",
       "         2.6555, 1.9770, 3.8998, 7.4212, 6.3133, 8.4235, 8.5171, 7.0436, 7.3298,\n",
       "         6.9057, 5.1770, 3.4531, 5.9545, 7.1912, 7.4837, 4.0613, 4.7913, 5.1002,\n",
       "         4.9094, 5.6902, 4.4467, 5.8119, 5.8341, 4.7222, 5.6623, 6.9879]),\n",
       " 53)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13e482aca70>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x13e482d4530>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=5):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21224\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21224\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21224\\237376235.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_21224\\237376235.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-368.96527099609375\n",
      "tensor([-77.8105, -30.7112, -14.3185,  -0.9836,   2.0594], device='cuda:0')\n",
      "tensor([1.3215, 1.9310, 2.4867, 3.0987, 3.7717], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-375.5493469238281\n",
      "tensor([6.3940e-01, 3.8984e+00, 3.3382e+00, 4.5588e-01, 4.8432e-04],\n",
      "       device='cuda:0')\n",
      "tensor([1.3259, 1.9382, 2.5068, 3.2452, 3.2453], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-418.84417724609375\n",
      "tensor([ 3.5077e+01,  1.3892e+01,  6.7667e+00, -1.0610e-01, -2.4101e-03],\n",
      "       device='cuda:0')\n",
      "tensor([1.4864, 2.3140, 3.2527, 5.6377, 6.6994], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-365.76898193359375\n",
      "tensor([27.6497, 14.6235,  9.4664,  7.2800,  4.9397], device='cuda:0')\n",
      "tensor([1.3016, 1.8654, 2.3951, 2.9289, 3.6828], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-364.9815979003906\n",
      "tensor([-157.1935,  -63.1236,  -22.7090,  -13.2832,  -10.1593],\n",
      "       device='cuda:0')\n",
      "tensor([1.2864, 1.8483, 2.3736, 2.9186, 3.6811], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-404.5364074707031\n",
      "tensor([ 3.9429, -0.2690, -0.1944,  0.0000,  0.0000], device='cuda:0')\n",
      "tensor([1.4864, 2.3188, 3.2723, 3.9337, 4.0677], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-400.85400390625\n",
      "tensor([-3.4566e+01, -1.2128e+01, -6.0290e+00,  3.0259e-02,  1.2627e-04],\n",
      "       device='cuda:0')\n",
      "tensor([1.4783, 2.3053, 3.2487, 6.6943, 6.6979], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-371.5802307128906\n",
      "tensor([ -8.2547,  -9.8657, -17.7767, -13.7247,  -4.0370], device='cuda:0')\n",
      "tensor([1.2935, 1.8537, 2.3837, 2.9358, 3.6909], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-368.50274658203125\n",
      "tensor([42.0963, 17.6492,  4.5980,  2.2782,  1.4184], device='cuda:0')\n",
      "tensor([1.2927, 1.8546, 2.3772, 2.9088, 3.6583], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(20630, device='cuda:0')\n",
      "-375.921875\n",
      "tensor([25.4052,  6.3328, -2.4097, -0.3496,  0.8806], device='cuda:0')\n",
      "tensor([1.3302, 1.9642, 2.4576, 2.5484, 3.3832], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.3214964, 1.931032 , 2.4867246, 3.0986772, 3.7716982],\n",
       "       dtype=float32),\n",
       " array([1.325852 , 1.9382386, 2.5067675, 3.2452312, 3.245308 ],\n",
       "       dtype=float32),\n",
       " array([1.4864174, 2.3139815, 3.2526832, 5.637706 , 6.699405 ],\n",
       "       dtype=float32),\n",
       " array([1.3015685, 1.8654491, 2.3950615, 2.928883 , 3.682761 ],\n",
       "       dtype=float32),\n",
       " array([1.2863718, 1.8482732, 2.3735638, 2.9185526, 3.6811347],\n",
       "       dtype=float32),\n",
       " array([1.4864028, 2.3187706, 3.2722533, 3.933702 , 4.0677195],\n",
       "       dtype=float32),\n",
       " array([1.4783236, 2.30527  , 3.248696 , 6.6942797, 6.6978755],\n",
       "       dtype=float32),\n",
       " array([1.293493 , 1.8537216, 2.383719 , 2.9358108, 3.6908727],\n",
       "       dtype=float32),\n",
       " array([1.2927252, 1.8545793, 2.3772132, 2.908752 , 3.6582985],\n",
       "       dtype=float32),\n",
       " array([1.3302063, 1.9641626, 2.4576383, 2.548381 , 3.3832421],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=6, out_features=5, bias=False)\n",
       "  (fc1): Linear(in_features=6, out_features=5, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (fc4): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (fc5): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "375.876220703125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-368.96527099609375,\n",
       " -375.5493469238281,\n",
       " -418.84417724609375,\n",
       " -365.76898193359375,\n",
       " -364.9815979003906,\n",
       " -404.5364074707031,\n",
       " -400.85400390625,\n",
       " -371.5802307128906,\n",
       " -368.50274658203125,\n",
       " -375.921875]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 381.5504638671875, Std Loss: 18.19646609297658\n",
      "Mean Training Time: 2854.167358636856s, Std Training Time: 426.58330833060865s\n",
      "Final mu values (across trials): [[1.3214964 1.931032  2.4867246 3.0986772 3.7716982]\n",
      " [1.325852  1.9382386 2.5067675 3.2452312 3.245308 ]\n",
      " [1.4864174 2.3139815 3.2526832 5.637706  6.699405 ]\n",
      " [1.3015685 1.8654491 2.3950615 2.928883  3.682761 ]\n",
      " [1.2863718 1.8482732 2.3735638 2.9185526 3.6811347]\n",
      " [1.4864028 2.3187706 3.2722533 3.933702  4.0677195]\n",
      " [1.4783236 2.30527   3.248696  6.6942797 6.6978755]\n",
      " [1.293493  1.8537216 2.383719  2.9358108 3.6908727]\n",
      " [1.2927252 1.8545793 2.3772132 2.908752  3.6582985]\n",
      " [1.3302063 1.9641626 2.4576383 2.548381  3.3832421]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
