{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([2.7570, 1.5090, 0.2365, 3.3478, 3.2223, 4.2124, 2.9035, 3.3344, 1.3995,\n",
       "         5.7616, 6.8328, 4.5157, 5.6977, 4.9250, 5.0952, 6.0588, 4.4875, 7.1116,\n",
       "         4.9173, 6.8298, 4.9415, 4.7966, 3.5322, 5.4662, 4.8193, 5.4307, 4.2292,\n",
       "         6.6234, 7.1331, 5.2700, 3.7957, 4.7685, 7.2042, 6.0950, 5.2308, 6.7378,\n",
       "         4.5521, 5.6731, 6.6766, 5.8911, 5.7613, 6.9621, 4.6759, 7.4835, 3.3612,\n",
       "         5.2956, 5.3522, 6.2735, 5.1872, 4.5062, 4.2045, 6.0135, 7.6409, 7.7392,\n",
       "         3.5691, 4.1773, 5.8629, 7.2233, 8.1469, 4.4091, 3.7262, 6.8285, 5.4861,\n",
       "         5.7682, 4.5942, 7.4260]),\n",
       " tensor([ 1.1591,  6.9214,  5.4501,  4.1053,  3.6691,  4.1389,  3.8475,  5.1716,\n",
       "          4.6173,  3.6149,  0.0542, -0.4626,  0.5474,  0.2700,  1.8423,  1.3074,\n",
       "          1.6076,  2.0122,  1.6898, -0.0405,  1.9818,  1.2046,  2.2222,  0.5151,\n",
       "          2.3218,  2.0050,  0.8214,  1.2920,  1.9703,  0.9708,  1.7617,  1.0266,\n",
       "          0.6666,  1.2741,  1.0999,  2.8074,  1.5403,  1.1751,  1.3276,  2.0514,\n",
       "          1.1966,  0.1093,  1.5788,  4.0061,  3.4426,  4.8742,  5.7909,  5.5212,\n",
       "          6.5050,  5.4591,  3.0746,  4.4262,  5.1800,  4.6971,  2.6388,  6.3305,\n",
       "          4.8967,  2.7458,  3.1640,  8.3900,  7.5619,  9.0312,  7.0814,  8.5668,\n",
       "          7.5961,  7.1580]),\n",
       " 66)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1832431cbc0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18324345940>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=7):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19388\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19388\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19388\\1791965750.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_19388\\1791965750.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-499.03076171875\n",
      "tensor([-22.3070, -19.3178, -22.5688,  -6.7665,  -1.3129,   1.1201,   1.0143],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.4635,  6.6004, 10.2195, 10.9979, 11.7199, 12.5782, 14.3760],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-512.1925048828125\n",
      "tensor([-362.7343, -247.6116, -308.4911, -172.8770,  -64.5061,  -23.0144,\n",
      "          13.0909], device='cuda:0')\n",
      "tensor([ 3.4565,  5.7831,  8.6571, 10.3099, 11.1520, 12.0038, 13.7755],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-525.6651000976562\n",
      "tensor([24.4772, 19.3881, 22.2611, 19.3240,  3.5055,  2.0320,  3.2230],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.3707,  5.8670,  8.4959, 10.3281, 11.1692, 12.0198, 13.7915],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-516.096923828125\n",
      "tensor([274.7146, 271.3275, 238.8402, 227.2682,  68.6479,  43.4681,  14.1343],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.7911,  5.5399,  7.9625, 10.3092, 11.1456, 12.0075, 13.7990],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-528.4637451171875\n",
      "tensor([66.4450, 66.8992, 53.4171, 76.1711, 23.5598,  7.7425,  1.1623],\n",
      "       device='cuda:0')\n",
      "tensor([ 2.6220,  5.3031,  7.4083, 10.3233, 11.1697, 12.0278, 13.8114],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-491.32611083984375\n",
      "tensor([137.2366, 106.6094, 128.7304,  27.8308,  22.7006,  24.7463,  11.0334],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.9891,  6.7490, 10.2304, 11.0274, 11.7675, 12.6454, 14.4500],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-504.87921142578125\n",
      "tensor([-94.5694, -69.0689, -85.9264, -18.0190,  -8.7030,   5.3456,   5.8101],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.8917,  6.7376, 10.2580, 11.0668, 11.8131, 12.7265, 14.5633],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-496.7637939453125\n",
      "tensor([-10.8356,  -7.7168,  -8.8540,  -4.8732,  -7.3480,  -8.5657,  -5.1988],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.9235,  6.7191, 10.2269, 11.0184, 11.7456, 12.6274, 14.4368],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-518.4676513671875\n",
      "tensor([348.7849, 290.3719, 202.6269, 189.0822,  64.6026,  15.9491,   5.6107],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.4531,  6.3512,  8.3919, 10.3220, 11.1618, 12.0078, 13.7768],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-514.7545776367188\n",
      "tensor([-1130.8170,  -667.4799,  -909.1655,  -447.6259,  -167.0602,   -67.0990,\n",
      "           -5.7084], device='cuda:0')\n",
      "tensor([ 3.6614,  5.8140,  8.7369, 10.3326, 11.1665, 12.0144, 13.7911],\n",
      "       device='cuda:0', grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3.463474 ,  6.6004014, 10.21954  , 10.997939 , 11.71993  ,\n",
       "        12.578201 , 14.376013 ], dtype=float32),\n",
       " array([ 3.4564607,  5.7830896,  8.6570835, 10.309939 , 11.152041 ,\n",
       "        12.003772 , 13.77554  ], dtype=float32),\n",
       " array([ 3.3707325,  5.8669662,  8.495887 , 10.328106 , 11.169164 ,\n",
       "        12.019754 , 13.791479 ], dtype=float32),\n",
       " array([ 2.7911115,  5.5398684,  7.962476 , 10.309223 , 11.145552 ,\n",
       "        12.007532 , 13.799009 ], dtype=float32),\n",
       " array([ 2.6219757,  5.303149 ,  7.4082737, 10.323283 , 11.169695 ,\n",
       "        12.027809 , 13.81143  ], dtype=float32),\n",
       " array([ 3.9890633,  6.748954 , 10.230371 , 11.027438 , 11.767483 ,\n",
       "        12.645376 , 14.449995 ], dtype=float32),\n",
       " array([ 3.8917015,  6.7375984, 10.258024 , 11.066841 , 11.813139 ,\n",
       "        12.726544 , 14.56333  ], dtype=float32),\n",
       " array([ 3.9234796,  6.7191424, 10.226884 , 11.018446 , 11.74557  ,\n",
       "        12.627409 , 14.436768 ], dtype=float32),\n",
       " array([ 3.4530933,  6.3512297,  8.391898 , 10.322012 , 11.161762 ,\n",
       "        12.007839 , 13.776787 ], dtype=float32),\n",
       " array([ 3.6614158,  5.8140163,  8.736929 , 10.332577 , 11.166536 ,\n",
       "        12.014428 , 13.791117 ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=8, out_features=7, bias=False)\n",
       "  (fc1): Linear(in_features=8, out_features=7, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=7, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=7, bias=True)\n",
       "  (fc4): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (fc5): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=7, out_features=7, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=7, out_features=7, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=7, out_features=7, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(7, 7, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "514.5136108398438"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-499.03076171875,\n",
       " -512.1925048828125,\n",
       " -525.6651000976562,\n",
       " -516.096923828125,\n",
       " -528.4637451171875,\n",
       " -491.32611083984375,\n",
       " -504.87921142578125,\n",
       " -496.7637939453125,\n",
       " -518.4676513671875,\n",
       " -514.7545776367188]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 510.7640380859375, Std Loss: 11.777997877576626\n",
      "Mean Training Time: 3228.9673409700395s, Std Training Time: 1417.9871886434357s\n",
      "Final mu values (across trials): [[ 3.463474   6.6004014 10.21954   10.997939  11.71993   12.578201\n",
      "  14.376013 ]\n",
      " [ 3.4564607  5.7830896  8.6570835 10.309939  11.152041  12.003772\n",
      "  13.77554  ]\n",
      " [ 3.3707325  5.8669662  8.495887  10.328106  11.169164  12.019754\n",
      "  13.791479 ]\n",
      " [ 2.7911115  5.5398684  7.962476  10.309223  11.145552  12.007532\n",
      "  13.799009 ]\n",
      " [ 2.6219757  5.303149   7.4082737 10.323283  11.169695  12.027809\n",
      "  13.81143  ]\n",
      " [ 3.9890633  6.748954  10.230371  11.027438  11.767483  12.645376\n",
      "  14.449995 ]\n",
      " [ 3.8917015  6.7375984 10.258024  11.066841  11.813139  12.726544\n",
      "  14.56333  ]\n",
      " [ 3.9234796  6.7191424 10.226884  11.018446  11.74557   12.627409\n",
      "  14.436768 ]\n",
      " [ 3.4530933  6.3512297  8.391898  10.322012  11.161762  12.007839\n",
      "  13.776787 ]\n",
      " [ 3.6614158  5.8140163  8.736929  10.332577  11.166536  12.014428\n",
      "  13.791117 ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
