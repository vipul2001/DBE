{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.8168, 2.2075, 1.2915, 2.6197, 3.5630, 2.4505, 4.0172, 4.7857, 3.1698,\n",
       "         3.9155, 5.4062, 4.8067, 1.4517, 3.6064, 3.9713, 2.5123, 2.8555, 2.7318,\n",
       "         2.5096, 2.8565, 5.8192, 8.0817, 7.0119, 6.7744, 5.6756, 5.9468, 5.7676,\n",
       "         5.9685, 5.7855, 7.3481, 7.9688, 8.3022, 6.9151, 6.6270, 7.4642, 6.7158,\n",
       "         5.1644, 3.3709, 4.0199, 4.6079, 8.7507, 8.5861, 6.8915, 8.3275]),\n",
       " tensor([ 1.7318, -0.1705,  1.6213,  2.0929,  2.5223,  0.5026,  6.8027,  3.3547,\n",
       "          2.9006,  2.8153,  2.7964,  3.2515,  2.6475,  3.6778,  2.9677,  4.9562,\n",
       "          4.3703,  3.3815,  4.0328,  1.4602,  0.7666,  1.7749,  0.6104,  1.5695,\n",
       "          3.3143,  2.0275, -1.0845,  1.7356,  2.8944,  2.4191, -0.0838,  2.0826,\n",
       "          4.9894,  1.9456,  3.7542,  8.0916,  6.7856,  8.5654,  6.0301,  7.6515,\n",
       "          5.2558,  5.3083,  5.1638,  9.0451]),\n",
       " 44)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a0d72b8d70>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a0d72e4cb0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=5):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10504\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10504\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10504\\237376235.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10504\\237376235.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-588.346435546875\n",
      "tensor([-26.2565, -13.5413, -19.7589,  -9.4568,  -4.9621], device='cuda:0')\n",
      "tensor([ 4.2362,  6.9336, 10.6655, 11.7538, 13.5849], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-586.6359252929688\n",
      "tensor([-49.2677, -55.5554, -60.6891,  -7.6103,  -0.1106], device='cuda:0')\n",
      "tensor([ 3.1470,  6.6981, 10.6692, 11.7439, 13.5557], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-587.053955078125\n",
      "tensor([79.8578, 67.1986, 87.3534, 23.5221, 14.7232], device='cuda:0')\n",
      "tensor([ 3.6462,  6.7616, 10.6654, 11.7560, 13.6064], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-586.6812744140625\n",
      "tensor([-137.6075, -110.7852, -109.2251,  -26.3467,  -11.7988],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.9220,  7.2421, 10.6672, 11.7588, 13.5971], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-586.367919921875\n",
      "tensor([-43.2190, -39.2557, -47.6481, -13.1283,  -0.9473], device='cuda:0')\n",
      "tensor([ 3.5071,  6.7027, 10.6714, 11.7620, 13.5901], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-586.0530395507812\n",
      "tensor([-71.8928, -88.2605, -86.9657, -14.1224,  -5.8259], device='cuda:0')\n",
      "tensor([ 3.0358,  6.7959, 10.6789, 11.7687, 13.6081], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-585.3871459960938\n",
      "tensor([ 68.5460, 101.7838, 105.0839,  18.2811,  10.0806], device='cuda:0')\n",
      "tensor([ 2.6407,  6.6184, 10.6671, 11.7589, 13.6058], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-550.1184692382812\n",
      "tensor([-74.1473, -55.3519, -16.8086, -10.4995,  -9.6602], device='cuda:0')\n",
      "tensor([ 5.8903, 10.3175, 11.1559, 12.0086, 13.7874], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-587.1005859375\n",
      "tensor([-39.8006, -48.9456, -51.6270,   0.3215,  -0.2362], device='cuda:0')\n",
      "tensor([ 2.9758,  6.7235, 10.6815, 11.7710, 13.6116], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-590.8292846679688\n",
      "tensor([201.3708, 272.5888, 258.0785,  43.8922,  20.8523], device='cuda:0')\n",
      "tensor([ 2.9220,  6.8739, 10.6691, 11.7476, 13.5710], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.2362022,  6.9335575, 10.665526 , 11.753833 , 13.584875 ],\n",
       "       dtype=float32),\n",
       " array([ 3.1470144,  6.698064 , 10.669183 , 11.7438965, 13.555669 ],\n",
       "       dtype=float32),\n",
       " array([ 3.6461608,  6.761649 , 10.665366 , 11.756008 , 13.606352 ],\n",
       "       dtype=float32),\n",
       " array([ 3.921987 ,  7.2420816, 10.66718  , 11.758757 , 13.597053 ],\n",
       "       dtype=float32),\n",
       " array([ 3.5070596,  6.70268  , 10.671364 , 11.762017 , 13.590075 ],\n",
       "       dtype=float32),\n",
       " array([ 3.0357912,  6.7959137, 10.678906 , 11.768745 , 13.608136 ],\n",
       "       dtype=float32),\n",
       " array([ 2.6406693,  6.6183834, 10.667135 , 11.7588825, 13.605784 ],\n",
       "       dtype=float32),\n",
       " array([ 5.8903317, 10.317547 , 11.155947 , 12.008601 , 13.787405 ],\n",
       "       dtype=float32),\n",
       " array([ 2.9757836,  6.723545 , 10.681499 , 11.770971 , 13.611553 ],\n",
       "       dtype=float32),\n",
       " array([ 2.9220147,  6.873886 , 10.669143 , 11.747558 , 13.57098  ],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=6, out_features=5, bias=False)\n",
       "  (fc1): Linear(in_features=6, out_features=5, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=5, bias=True)\n",
       "  (fc4): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (fc5): Linear(in_features=5, out_features=5, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=5, out_features=5, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=5, out_features=5, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=5, out_features=5, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(5, 5, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589.422607421875"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-588.346435546875,\n",
       " -586.6359252929688,\n",
       " -587.053955078125,\n",
       " -586.6812744140625,\n",
       " -586.367919921875,\n",
       " -586.0530395507812,\n",
       " -585.3871459960938,\n",
       " -550.1184692382812,\n",
       " -587.1005859375,\n",
       " -590.8292846679688]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 583.4574035644531, Std Loss: 11.204322596455862\n",
      "Mean Training Time: 3065.3024875640867s, Std Training Time: 660.5087501283091s\n",
      "Final mu values (across trials): [[ 4.2362022  6.9335575 10.665526  11.753833  13.584875 ]\n",
      " [ 3.1470144  6.698064  10.669183  11.7438965 13.555669 ]\n",
      " [ 3.6461608  6.761649  10.665366  11.756008  13.606352 ]\n",
      " [ 3.921987   7.2420816 10.66718   11.758757  13.597053 ]\n",
      " [ 3.5070596  6.70268   10.671364  11.762017  13.590075 ]\n",
      " [ 3.0357912  6.7959137 10.678906  11.768745  13.608136 ]\n",
      " [ 2.6406693  6.6183834 10.667135  11.7588825 13.605784 ]\n",
      " [ 5.8903317 10.317547  11.155947  12.008601  13.787405 ]\n",
      " [ 2.9757836  6.723545  10.681499  11.770971  13.611553 ]\n",
      " [ 2.9220147  6.873886  10.669143  11.747558  13.57098  ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
