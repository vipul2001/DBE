{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.9217, 0.2002, 1.8601, 2.9139, 4.3041, 3.6604, 3.1748, 3.4534, 4.2734,\n",
       "         3.4506, 2.7697, 4.9565, 4.5954, 0.8395, 4.0214, 4.5535, 4.1972, 2.5277,\n",
       "         2.0489, 4.6950, 5.3469, 5.5882, 5.2129, 5.7515, 5.0217, 6.7148, 4.4557,\n",
       "         6.5529, 6.8146, 6.0950, 7.9844]),\n",
       " tensor([0.7659, 1.9207, 4.5437, 5.0030, 5.2272, 5.0387, 3.5622, 5.5424, 7.1533,\n",
       "         6.5102, 5.2400, 6.6050, 4.2983, 4.4612, 5.9744, 5.7080, 8.0817, 4.0819,\n",
       "         3.6487, 6.5929, 4.0373, 6.2426, 5.2846, 5.1543, 4.7055, 4.7272, 5.5729,\n",
       "         5.3128, 5.4337, 8.3523, 7.9349]),\n",
       " 31)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fdeb4bacf0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fdf39bb500>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        num_hiddens=4\n",
    "        self.num_states=4\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION()\n",
    "        self.A2=ATTENTION()\n",
    "        self.A3=ATTENTION()\n",
    "        self.A4=ATTENTION()\n",
    "        self.A5=ATTENTION()\n",
    "        self.A6=ATTENTION()\n",
    "        self.A7=ATTENTION()\n",
    "        self.A8=ATTENTION()\n",
    "        self.A9=ATTENTION()\n",
    "        self.A10=ATTENTION()\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10204\\3374062728.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10204\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10204\\4031529318.py:222: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_10204\\4031529318.py:223: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-430.98846435546875\n",
      "tensor([5.3432, 2.6235, 0.2324, 0.0515], device='cuda:0')\n",
      "tensor([1.5008, 2.3586, 3.5514, 6.6989], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-428.9092712402344\n",
      "tensor([-18.6791,  -7.8478,  -1.8917,  -0.8504], device='cuda:0')\n",
      "tensor([1.5022, 2.3581, 3.5379, 6.7068], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-431.19329833984375\n",
      "tensor([-10.0580,  -2.8534,  -0.3010,  -0.0590], device='cuda:0')\n",
      "tensor([1.5094, 2.3643, 3.5354, 6.6989], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-430.2036437988281\n",
      "tensor([-5.2518,  4.4217,  3.4410, -1.2052], device='cuda:0')\n",
      "tensor([1.4979, 2.3534, 3.5345, 6.6946], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-429.54241943359375\n",
      "tensor([83.0413, 34.0621,  9.8752, -1.6758], device='cuda:0')\n",
      "tensor([1.4972, 2.3452, 3.4920, 5.2313], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-495.24005126953125\n",
      "tensor([19.7957,  8.9888,  0.5880,  0.0315], device='cuda:0')\n",
      "tensor([1.9075, 3.4259, 5.7694, 6.6984], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-497.7619934082031\n",
      "tensor([-44.3856, -25.0910,  -9.8379,  -0.8580], device='cuda:0')\n",
      "tensor([1.9131, 3.4347, 5.7358, 6.6908], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-431.64697265625\n",
      "tensor([-8.6499e+00, -2.7748e+00, -1.3363e+00,  9.1850e-04], device='cuda:0')\n",
      "tensor([1.5033, 2.3573, 3.5304, 3.5317], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-496.6666259765625\n",
      "tensor([10.3075,  3.4582,  0.5331,  0.5721], device='cuda:0')\n",
      "tensor([1.9101, 3.4327, 5.2756, 6.6960], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "362\n",
      "362\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-432.9261779785156\n",
      "tensor([-5.8446, -0.8165,  0.2315, -1.5223], device='cuda:0')\n",
      "tensor([1.5047, 2.3615, 3.5338, 6.6924], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "    m_1[:,:-100]=1\n",
    "    m_2[:,:-100]=1\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.5008003, 2.3585773, 3.5514126, 6.6989136], dtype=float32),\n",
       " array([1.5022465, 2.358111 , 3.5378783, 6.706832 ], dtype=float32),\n",
       " array([1.5094032, 2.364257 , 3.5353854, 6.698927 ], dtype=float32),\n",
       " array([1.4979413, 2.3533928, 3.5345452, 6.6945763], dtype=float32),\n",
       " array([1.4972234, 2.3452284, 3.4919968, 5.231285 ], dtype=float32),\n",
       " array([1.9075062, 3.425921 , 5.769442 , 6.6984034], dtype=float32),\n",
       " array([1.9131354, 3.4347324, 5.735813 , 6.690815 ], dtype=float32),\n",
       " array([1.5033176, 2.357287 , 3.530386 , 3.5317314], dtype=float32),\n",
       " array([1.9101027, 3.4326558, 5.2755594, 6.695968 ], dtype=float32),\n",
       " array([1.5047476, 2.3614802, 3.5338445, 6.6924057], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc1): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGgCAYAAABbvTaPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3B0lEQVR4nO3df3hU9Z3//deZmWQSYjglxGRIiYh3EalBtxtcCHULCPJjRa7WfhcrNsXvUqxVQBZYW3Tvq2yvlrDdb7W7N1v8sV5SFYtX74rV1c0SimK5E34UTeWHUlxRAROCmEwSSGYmmc/9xyQHhiAykx8nQ56P6zoXmTPvzHzOZybMaz7nc86xjDFGAAAAKcbjdgMAAACSQYgBAAApiRADAABSEiEGAACkJEIMAABISYQYAACQkggxAAAgJRFiAABASiLEAACAlESIAQAAKSmhELNu3Tpdd911Gjx4sAYPHqySkhL913/9l3O/MUarVq1SQUGBMjMzNXnyZO3fvz/uMUKhkBYvXqzc3FxlZWVpzpw5Onr0aFxNfX29SktLZdu2bNtWaWmpGhoakt9KAABwybESuXbSyy+/LK/Xqy996UuSpF/96lf6l3/5F7311lu69tpr9c///M/66U9/qvXr1+vqq6/WT37yE73xxhs6ePCgsrOzJUnf//739fLLL2v9+vUaOnSoli9frk8//VR79uyR1+uVJM2aNUtHjx7V448/Lkm6++67deWVV+rll1++6A2LRqP6+OOPlZ2dLcuyLvr3AACAe4wxampqUkFBgTyezxlrMd00ZMgQ8x//8R8mGo2aQCBg1qxZ49zX2tpqbNs2jz76qDHGmIaGBpOWlmY2btzo1Bw7dsx4PB5TXl5ujDHmwIEDRpLZsWOHU1NVVWUkmXffffei23XkyBEjiYWFhYWFhSUFlyNHjnzuZ71PSWpvb9dvfvMbnTp1SiUlJTp8+LBqa2s1ffp0p8bv92vSpEmqrKzU9773Pe3Zs0eRSCSupqCgQEVFRaqsrNSMGTNUVVUl27Y1fvx4p2bChAmybVuVlZUaPXr0edsTCoUUCoWc26ZjgOnIkSMaPHhwspsJAAD6UGNjowoLC509OBeScIjZu3evSkpK1Nraqssuu0ybNm3Sl7/8ZVVWVkqS8vPz4+rz8/P14YcfSpJqa2uVnp6uIUOGdKmpra11avLy8ro8b15enlNzPmVlZfqnf/qnLus75+8AAIDUcTFTQRI+Omn06NGqrq7Wjh079P3vf1/z58/XgQMHPvNJjTGf25Bza85X/3mPs3LlSgWDQWc5cuTIxW4SAABIQQmHmPT0dH3pS1/SuHHjVFZWpuuvv17/+q//qkAgIEldRkvq6uqc0ZlAIKBwOKz6+voL1hw/frzL8544caLLKM/Z/H6/M+rC6AsAAJe+bp8nxhijUCikkSNHKhAIqKKiwrkvHA5r27ZtmjhxoiSpuLhYaWlpcTU1NTXat2+fU1NSUqJgMKhdu3Y5NTt37lQwGHRqAAAAEpoT8+CDD2rWrFkqLCxUU1OTNm7cqNdff13l5eWyLEtLly7V6tWrNWrUKI0aNUqrV6/WoEGDNG/ePEmSbdtasGCBli9frqFDhyonJ0crVqzQ2LFjNW3aNEnSmDFjNHPmTC1cuFCPPfaYpNgh1rNnz/7MSb0AAGDgSSjEHD9+XKWlpaqpqZFt27ruuutUXl6um2++WZL0wAMPqKWlRffee6/q6+s1fvx4bd68OW6G8SOPPCKfz6e5c+eqpaVFU6dO1fr1651zxEjShg0btGTJEucopjlz5mjt2rU9sb0AAOASkdDJ7lJJY2OjbNtWMBhkfgwAACkikc9vrp0EAABSEiEGAACkJEIMAABISYQYAACQkggxAAAgJRFiAABASkr6KtYDVej991X/641Ky8/T0O9+1+3mAAAwYDESk6DIsY9V/8wzCr7yqttNAQBgQCPEJKrzStqX5jkCAQBIGYSYRHVkGEIMAADuIsQkyPJ0dBkhBgAAVxFiEuXsToq62w4AAAY4QkzCYiHmEr1uJgAAKYMQkyhnJMbdZgAAMNARYhLFxF4AAPoFQkyCLA6xBgCgXyDEJIoQAwBAv0CISRQhBgCAfoEQk6iOEGOY2QsAgKsIMYni6CQAAPoFQkyCnIm9UU52BwCAmwgxiWJODAAA/QIhJlGEGAAA+gVCTKKY2AsAQL9AiEkYE3sBAOgPCDGJYncSAAD9AiEmUVw7CQCAfoEQkyCunQQAQP9AiEmUp6PLCDEAALiKEJOozqOTCDEAALiKEJMwdicBANAfEGISxcReAAD6BUJMgpjYCwBA/0CISRQhBgCAfoEQkyhCDAAA/QIhJlHOtZMAAICbCDEJYyQGAID+gBCTIMtDiAEAoD8gxCSqc05MNOpuOwAAGOAIMYliYi8AAP0CISZRTOwFAKBfIMQkipEYAAD6BUJMwggxAAD0B4SYBFlcOwkAgH6BEJModicBANAvEGISRYgBAKBfIMQkyhPrMiIMAADuSijElJWV6YYbblB2drby8vL09a9/XQcPHoyrueuuu2RZVtwyYcKEuJpQKKTFixcrNzdXWVlZmjNnjo4ePRpXU19fr9LSUtm2Ldu2VVpaqoaGhuS2skdxsjsAAPqDhELMtm3bdN9992nHjh2qqKhQW1ubpk+frlOnTsXVzZw5UzU1Nc7y6quvxt2/dOlSbdq0SRs3btT27dvV3Nys2bNnq7293amZN2+eqqurVV5ervLyclVXV6u0tLQbm9pDmNgLAEC/4EukuLy8PO72U089pby8PO3Zs0df+9rXnPV+v1+BQOC8jxEMBvXkk0/qmWee0bRp0yRJzz77rAoLC7VlyxbNmDFD77zzjsrLy7Vjxw6NHz9ekvTEE0+opKREBw8e1OjRoxPayJ5kMScGAIB+oVtzYoLBoCQpJycnbv3rr7+uvLw8XX311Vq4cKHq6uqc+/bs2aNIJKLp06c76woKClRUVKTKykpJUlVVlWzbdgKMJE2YMEG2bTs15wqFQmpsbIxbegUhBgCAfiHpEGOM0bJly3TjjTeqqKjIWT9r1ixt2LBBW7du1c9//nPt3r1bN910k0KhkCSptrZW6enpGjJkSNzj5efnq7a21qnJy8vr8px5eXlOzbnKysqc+TO2bauwsDDZTbsw50QxAADATQntTjrbokWL9Pbbb2v79u1x62+//Xbn56KiIo0bN04jRozQK6+8ottuu+0zH88Yc2ZXjRT382fVnG3lypVatmyZc7uxsbF3gsxZz3+h9gAAgN6V1EjM4sWL9dJLL+m1117T8OHDL1g7bNgwjRgxQocOHZIkBQIBhcNh1dfXx9XV1dUpPz/fqTl+/HiXxzpx4oRTcy6/36/BgwfHLb3i7NDCLiUAAFyTUIgxxmjRokV64YUXtHXrVo0cOfJzf+fkyZM6cuSIhg0bJkkqLi5WWlqaKioqnJqamhrt27dPEydOlCSVlJQoGAxq165dTs3OnTsVDAadmn6BEAMAgGsS2p1033336bnnntPvfvc7ZWdnO/NTbNtWZmammpubtWrVKn3zm9/UsGHD9MEHH+jBBx9Ubm6uvvGNbzi1CxYs0PLlyzV06FDl5ORoxYoVGjt2rHO00pgxYzRz5kwtXLhQjz32mCTp7rvv1uzZs109MkmSLM9ZuY8QAwCAaxIKMevWrZMkTZ48OW79U089pbvuukter1d79+7V008/rYaGBg0bNkxTpkzR888/r+zsbKf+kUcekc/n09y5c9XS0qKpU6dq/fr18nq9Ts2GDRu0ZMkS5yimOXPmaO3atcluZ885e3cSJ7wDAMA1ljGX5nBCY2OjbNtWMBjs0fkx7U1N+vMNfyVJGv32n+RJT++xxwYAYKBL5PObaycliom9AAD0C4SYhBFiAADoDwgxCYo7LQwhBgAA1xBiEsXuJAAA+gVCTKIIMQAA9AuEmETFXXbAxXYAADDAEWISdfbJ7kSKAQDALYSYBMVd7pGT3QEA4BpCTKKYEwMAQL9AiEkUIQYAgH6BEJOouIm9hBgAANxCiElU3NnuAACAWwgxiWJ3EgAA/QIhJkEWIQYAgH6BENMdhBgAAFxDiElG5wnvCDEAALiGEJOMjl1KJkqIAQDALYSYZDjzYggxAAC4hRCTjM4Qw+4kAABcQ4hJgnN8EiEGAADXEGKSwUgMAACuI8QkgxADAIDrCDHJ6Dw6iQwDAIBrCDHJ4OgkAABcR4hJgsXuJAAAXEeISUZniIlG3W0HAAADGCEmGYzEAADgOkJMMpyJvYQYAADcQohJhjMS424zAAAYyAgxyeDoJAAAXEeISQKXHQAAwH2EmGQwsRcAANcRYpJBiAEAwHWEmGR4Yt3G0UkAALiHEJMMRmIAAHAdISYZzsFJhBgAANxCiEmCJUZiAABwGyEmGexOAgDAdYSYZHDZAQAAXEeISQaXHQAAwHWEmGSwOwkAANcRYpJx5roDbrYCAIABjRCTBMvq6LZo1N2GAAAwgBFiktFxxl52JwEA4B5CTDI8HUcnRQkxAAC4hRCTBGd3kmF3EgAAbiHEJMPDnBgAANxGiElG5+6kdkIMAABuSSjElJWV6YYbblB2drby8vL09a9/XQcPHoyrMcZo1apVKigoUGZmpiZPnqz9+/fH1YRCIS1evFi5ubnKysrSnDlzdPTo0bia+vp6lZaWyrZt2bat0tJSNTQ0JLeVPczyeGM/sDsJAADXJBRitm3bpvvuu087duxQRUWF2traNH36dJ06dcqp+dnPfqaHH35Ya9eu1e7duxUIBHTzzTerqanJqVm6dKk2bdqkjRs3avv27Wpubtbs2bPV3t7u1MybN0/V1dUqLy9XeXm5qqurVVpa2gOb3AM6dicZdicBAOAe0w11dXVGktm2bZsxxphoNGoCgYBZs2aNU9Pa2mps2zaPPvqoMcaYhoYGk5aWZjZu3OjUHDt2zHg8HlNeXm6MMebAgQNGktmxY4dTU1VVZSSZd99996LaFgwGjSQTDAa7s4nn9T/f+IY5MPoa0/TGH3r8sQEAGMgS+fzu1pyYYDAoScrJyZEkHT58WLW1tZo+fbpT4/f7NWnSJFVWVkqS9uzZo0gkEldTUFCgoqIip6aqqkq2bWv8+PFOzYQJE2TbtlNzrlAopMbGxrilt3B0EgAA7ks6xBhjtGzZMt14440qKiqSJNXW1kqS8vPz42rz8/Od+2pra5Wenq4hQ4ZcsCYvL6/Lc+bl5Tk15yorK3Pmz9i2rcLCwmQ37fN17k46a/cXAADoW0mHmEWLFuntt9/Wr3/96y73WZYVd9sY02Xduc6tOV/9hR5n5cqVCgaDznLkyJGL2YzkeLgAJAAAbksqxCxevFgvvfSSXnvtNQ0fPtxZHwgEJKnLaEldXZ0zOhMIBBQOh1VfX3/BmuPHj3d53hMnTnQZ5enk9/s1ePDguKW3OEcnMbEXAADXJBRijDFatGiRXnjhBW3dulUjR46Mu3/kyJEKBAKqqKhw1oXDYW3btk0TJ06UJBUXFystLS2upqamRvv27XNqSkpKFAwGtWvXLqdm586dCgaDTo2rODoJAADX+RIpvu+++/Tcc8/pd7/7nbKzs50RF9u2lZmZKcuytHTpUq1evVqjRo3SqFGjtHr1ag0aNEjz5s1zahcsWKDly5dr6NChysnJ0YoVKzR27FhNmzZNkjRmzBjNnDlTCxcu1GOPPSZJuvvuuzV79myNHj26J7c/Kc4uLa6dBACAaxIKMevWrZMkTZ48OW79U089pbvuukuS9MADD6ilpUX33nuv6uvrNX78eG3evFnZ2dlO/SOPPCKfz6e5c+eqpaVFU6dO1fr16+X1ep2aDRs2aMmSJc5RTHPmzNHatWuT2cae51x2gIm9AAC4xTLm0pyd2tjYKNu2FQwGe3x+zId3/W+d3rFDBf/n/8iefUuPPjYAAANZIp/fXDspCZZzdBJzYgAAcAshJhkcnQQAgOsIMcnovIo1E3sBAHANISYJzmUHmNgLAIBrCDHJ4DwxAAC4jhCTDOcQa3YnAQDgFkJMEiwPV7EGAMBthJhksDsJAADXEWKS0XmemHZCDAAAbiHEJME5OondSQAAuIYQkwx2JwEA4DpCTDI8XMUaAAC3EWKSYHVedoDdSQAAuIYQk4zO3UlM7AUAwDWEmGQ4u5MIMQAAuIUQkwSOTgIAwH2EmGRwdBIAAK4jxCSDo5MAAHAdISYJnUcnmWi7yy0BAGDgIsQkg6tYAwDgOkJMEiyOTgIAwHWEmGRwdBIAAK4jxCTDOTqJ3UkAALiFEJMEdicBAOA+QkwyODoJAADXEWKSwXliAABwHSEmCZZziDW7kwAAcAshJhkdRycZjk4CAMA1hJhksDsJAADXEWKSYHljE3vFxF4AAFxDiEmGxVWsAQBwGyEmGexOAgDAdYSYJHB0EgAA7iPEJIOjkwAAcB0hJhmdu5PaCTEAALiFEJMEq+OyA1zFGgAA9xBiktF5FWtGYgAAcA0hJgmWl4m9AAC4jRCTjM6rWLdzsjsAANxCiEmCMxJDiAEAwDWEmGR4fZI4Yy8AAG4ixCThzEhMm7sNAQBgACPEJMOZE8NIDAAAbiHEJMHydZwnhjkxAAC4hhCTDOc8MYQYAADcQohJguXt2J0UJcQAAOAWQkwyvJ27k5gTAwCAWxIOMW+88YZuvfVWFRQUyLIsvfjii3H333XXXbIsK26ZMGFCXE0oFNLixYuVm5urrKwszZkzR0ePHo2rqa+vV2lpqWzblm3bKi0tVUNDQ8Ib2BuckRh2JwEA4JqEQ8ypU6d0/fXXa+3atZ9ZM3PmTNXU1DjLq6++Gnf/0qVLtWnTJm3cuFHbt29Xc3OzZs+erfazQsG8efNUXV2t8vJylZeXq7q6WqWlpYk2t1dYXib2AgDgNl+ivzBr1izNmjXrgjV+v1+BQOC89wWDQT355JN65plnNG3aNEnSs88+q8LCQm3ZskUzZszQO++8o/Lycu3YsUPjx4+XJD3xxBMqKSnRwYMHNXr06ESb3bM8zIkBAMBtvTIn5vXXX1deXp6uvvpqLVy4UHV1dc59e/bsUSQS0fTp0511BQUFKioqUmVlpSSpqqpKtm07AUaSJkyYINu2nZpzhUIhNTY2xi29xTnZXRshBgAAt/R4iJk1a5Y2bNigrVu36uc//7l2796tm266SaFQSJJUW1ur9PR0DRkyJO738vPzVVtb69Tk5eV1eey8vDyn5lxlZWXO/BnbtlVYWNjDW3YWLjsAAIDrEt6d9Hluv/125+eioiKNGzdOI0aM0CuvvKLbbrvtM3/PGCPLspzbZ//8WTVnW7lypZYtW+bcbmxs7LUgwwUgAQBwX68fYj1s2DCNGDFChw4dkiQFAgGFw2HV19fH1dXV1Sk/P9+pOX78eJfHOnHihFNzLr/fr8GDB8ctvcbD0UkAALit10PMyZMndeTIEQ0bNkySVFxcrLS0NFVUVDg1NTU12rdvnyZOnChJKikpUTAY1K5du5yanTt3KhgMOjVu4rIDAAC4L+HdSc3NzXrvvfec24cPH1Z1dbVycnKUk5OjVatW6Zvf/KaGDRumDz74QA8++KByc3P1jW98Q5Jk27YWLFig5cuXa+jQocrJydGKFSs0duxY52ilMWPGaObMmVq4cKEee+wxSdLdd9+t2bNnu39kknTW0UnMiQEAwC0Jh5g//vGPmjJlinO7cx7K/PnztW7dOu3du1dPP/20GhoaNGzYME2ZMkXPP/+8srOznd955JFH5PP5NHfuXLW0tGjq1Klav369vJ3nX5G0YcMGLVmyxDmKac6cORc8N01fOnN0Upu7DQEAYACzjDHG7Ub0hsbGRtm2rWAw2OPzY0KHD+v9WX8jT3a2Ru/e9fm/AAAALkoin99cOykJnLEXAAD3EWKSwdFJAAC4jhCThM6jk5jYCwCAewgxyfBwsjsAANxGiEmC5es4qCsa1SU6LxoAgH6PEJMEy3NWtzEaAwCAKwgxyTjrfDbMiwEAwB2EmCRYZ4UYTngHAIA7CDHJYCQGAADXEWKSwJwYAADcR4hJxtkjMYQYAABcQYhJguXxSJYVu0GIAQDAFYSYZHk5ay8AAG4ixCTJOUKJo5MAAHAFISZZjMQAAOAqQkySnJEY5sQAAOAKQkySOg+z5ugkAADcQYhJVlqaJMkwJwYAAFcQYpJkdYaYcMTllgAAMDARYpLkhJg2QgwAAG4gxCTJ8vkkSSZCiAEAwA2EmCQ5IzGEGAAAXEGISVJniOFkdwAAuIMQkyR2JwEA4C5CTJIsDrEGAMBVhJgkWWmMxAAA4CZCTLI4TwwAAK4ixCSJ3UkAALiLEJMky8ch1gAAuIkQkyTOEwMAgLsIMUlyDrHmsgMAALiCEJMkRmIAAHAXISZJnLEXAAB3EWKSxBl7AQBwFyEmSVY654kBAMBNhJgkcZ4YAADcRYhJFruTAABwFSEmSYzEAADgLkJMkjhjLwAA7iLEJInzxAAA4C5CTJLO7E4ixAAA4AZCTJKcEBMOu9wSAAAGJkJMkix/uiTOEwMAgFsIMUmy0jtCTCjkcksAABiYCDFJ8vj9kggxAAC4hRCTJCu9I8REmBMDAIAbCDFJ6pwTEw0RYgAAcAMhJknsTgIAwF0Jh5g33nhDt956qwoKCmRZll588cW4+40xWrVqlQoKCpSZmanJkydr//79cTWhUEiLFy9Wbm6usrKyNGfOHB09ejSupr6+XqWlpbJtW7Ztq7S0VA0NDQlvYG9hYi8AAO5KOMScOnVK119/vdauXXve+3/2s5/p4Ycf1tq1a7V7924FAgHdfPPNampqcmqWLl2qTZs2aePGjdq+fbuam5s1e/Zstbe3OzXz5s1TdXW1ysvLVV5erurqapWWliaxib3D6hiJiXKeGAAA3GG6QZLZtGmTczsajZpAIGDWrFnjrGttbTW2bZtHH33UGGNMQ0ODSUtLMxs3bnRqjh07ZjwejykvLzfGGHPgwAEjyezYscOpqaqqMpLMu+++e1FtCwaDRpIJBoPd2cTPFK49bg6MvsYcuLaoVx4fAICBKJHP7x6dE3P48GHV1tZq+vTpzjq/369JkyapsrJSkrRnzx5FIpG4moKCAhUVFTk1VVVVsm1b48ePd2omTJgg27admnOFQiE1NjbGLb3J0zGxV21tXMkaAAAX9GiIqa2tlSTl5+fHrc/Pz3fuq62tVXp6uoYMGXLBmry8vC6Pn5eX59Scq6yszJk/Y9u2CgsLu709F9K5O0ni0gMAALihV45Osiwr7rYxpsu6c51bc776Cz3OypUrFQwGneXIkSNJtPzidU7slaQok3sBAOhzPRpiAoGAJHUZLamrq3NGZwKBgMLhsOrr6y9Yc/z48S6Pf+LEiS6jPJ38fr8GDx4ct/Qmy+uVfD5JXD8JAAA39GiIGTlypAKBgCoqKpx14XBY27Zt08SJEyVJxcXFSktLi6upqanRvn37nJqSkhIFg0Ht2rXLqdm5c6eCwaBT0x94Og+zDjMSAwBAX/Ml+gvNzc167733nNuHDx9WdXW1cnJydMUVV2jp0qVavXq1Ro0apVGjRmn16tUaNGiQ5s2bJ0mybVsLFizQ8uXLNXToUOXk5GjFihUaO3aspk2bJkkaM2aMZs6cqYULF+qxxx6TJN19992aPXu2Ro8e3RPb3SMsv186fZpzxQAA4IKEQ8wf//hHTZkyxbm9bNkySdL8+fO1fv16PfDAA2ppadG9996r+vp6jR8/Xps3b1Z2drbzO4888oh8Pp/mzp2rlpYWTZ06VevXr5fX63VqNmzYoCVLljhHMc2ZM+czz03jFudcMYQYAAD6nGWMMW43ojc0NjbKtm0Fg8Femx/z3vQZinz0kUb8+jkN+spXeuU5AAAYSBL5/ObaSd3Qea4Yw0UgAQDoc4SYbrDSOy4CycReAAD6HCGmG5gTAwCAewgx3eDJ6BiJaW11uSUAAAw8hJhu8GRlSZKip0653BIAAAYeQkw3eAYRYgAAcAshphsYiQEAwD2EmG4gxAAA4B5CTDd0hpj25maXWwIAwMBDiOmGMyMxp11uCQAAAw8hphvYnQQAgHsIMd1AiAEAwD2EmG4gxAAA4B5CTDd4bVuS1N7Q4G5DAAAYgAgx3eDLGSJJav/0UxljXG4NAAADCyGmG7w5OZIkE4koymHWAAD0KUJMN3gyM2UNGiQpNhoDAAD6DiGmm3wdozFtJwkxAAD0JUJMN3XuUmr75ITLLQEAYGAhxHRT+pUjJEnh//kfl1sCAMDAQojppozR10iSWt896HJLAAAYWAgx3eS/ZrQkqeVPf5KJRl1uDQAAAwchppsGFRfLk52tttpaNW3Z4nZzAAAYMAgx3eTJyNCQO+6QJB1bcr/euWaMIsfrXG4VAACXPkJMD8i9715lTSxxbn8wd67CH3zgXoMAABgACDE9wOP3q/CxxzTkzjslSW3Hj+vjH/yQSxEAANCLCDE9xEpLU+D//kd96fXXZA0apJY//UnBF15wu1kAAFyyCDE9LC0Q0OX33StJOvn4ExyxBABALyHE9IIh3/qWrMxMhT/8UE2bN7vdHAAALkmEmF7gycqS2tokSceW/r3LrQEA4NJEiOklZ0/qNeGwiy0BAODSRIjpJZcvus/5+aO7v+diSwAAuDQRYnrJ0IULnZ9P79jhYks+X7S1VdFQyO1mAACQEEJML7G8Xnlt27ld9/AjLrbms5m2Nh2a+FUdmvhVmfZ2t5sDAMBFI8T0olFVlc7PJx9/XJG6/nc5grZPP1X09GlFT51Se2Oj280BAOCiEWJ6keXxaNQf3nBuf/Lvv3SxNednpaU5PzMBGQCQSggxvcx3+eUa8czTkqSG55/X8bI1avv0U5dbdX6EGMB9kY8/VnvzKbebAaQEQkwfGHTDDcqZ/x1J0qe/+pXemzRZR+69T59u2KCW6mq11de7d52ls563rbbWnTYAkCRFjh3TezdN1Z/HjXO7KUBK8LndgIEif+VKZU2cqBP/9v+odf9+NW/dquatW537rUGDlJafL89ll8kzaFDXJWuQrMzMjttZ8gwaJCs9TR6/X/L6dGpHlTJGjVJaYaFMW5ustPTY76SlybIsybJkIhG1nfxUaYHY81gZGYqebnHa8GHpdzRsTZk8fr9MNKrQu+8qe8ZMeTIzJMuS5fVKHo9kjNqDjUobFpAJh2Uikdjjeb2y0tNjNdGoLL9f6nhuGaNoY2OszudT5OOP5cvPl+X1xgJce3tsYnFbW+xfj0feyy5z2hY5flzeIUPkSU/v09cNfSMaCslKT4+9Vwewpm3bnJ+bt23TZZMmudgaoP+zzCV6qeXGxkbZtq1gMKjBgwe73RyHMUahPx9SU0WFWva+rdCBd9R24oTbzXKP1ytd4KgoT3a2ok1NZ8ovz5XCEZlIRFbWIFm+NMmSFDUybW3yZGTEQpQlWbLOhCjLkjxWx4fkOest6/z1HetlJBkj09oiy5/R0RCPs97yemXa2xVtOa1oY5PSR4xQNBxStPmU2j75RGkFBQq9807cdmXdeKMT7kwkIistTdFTp9Ty1lvKLC6Wx58uY0zsg12W5PMpevqUTlfFDte/bMqU2Cha5+9nZsqEw4q2nJbX/oLTLqePjdHpXbvkzclRxjXXxH7PRGP9J50ZkfNYat23X+H331faF7+otOHDdXrnTmVcf53Svzhc4Q8+UOjwYQ0aN06ey7LkSffLtLfHwqsky+uRaY8qcuSIPIMHd4Rsb+x16eg3y+ORMUYmHJbl8ar1nXfUum+ffPn58ubkyJuVJU9WVizw+v3yZPglWWr+/7Zr0F98RZ7Bg2OvSwcTiXR9XTtea2OMLI9X0dOnZfm88mRdJtPeLtMWiT1/5/Z3hicrtlvVO9iWCYVigdoYWRl+WZYlK90v09YW2/Xq9ait9ngscA/5QuwLgy/trH6PbavTzrZ2WWlpMtHYY8qKtTEaDiva1CxPZoaaX9+m0KFDzu9YGRmx90B6euxLS1rsXyutY11mhjzpfinNF3tujyV5fbI8VuwLimXFziAejcbeBx3bGLfN0lnh0ZIx0dhjysSu/WZZsiyPrLTYd95oKNSxXVbH35oV2662iCyPN7Z9UeM8h/PlJxp17rPS0iSPJbWd9bfvvAbn/ntWG30+mXAktp0+n9Qe69Noayj2PB19fqqySpbfL//oq+XJyOxY7z3zXvd4ZCKR2O1oVFZmhtP2zv9TrHPbIUsy0TO/p9icwrM/QjvfczIdb8fOOYdRE7c9nY8dDYcVevegPIMylVZ4hSyvR7I8Hf+e9Z6WZNrbZHl9zvtbktQee30UbZc83rN+r7PjjEykTdFTzbHb3th2m0hEviE5stLTYq+N1PH/WfTM73o9sixLre+8K9PeJk9GZuz9lpEpT2aGrHS/oi0tShsW0OCZM9WTEvn8JsT0A9HWVrXV1ipSV6fo6dMyp0/HjhjqXE6d/fMp52cTDsf+o41EFP7gA0mSb9iwjv+gI7H/NCKRuA87XeDl/sLttyty5IhMW5ta9+1T9PRpZ4TFSFIk4nz4XCh4AOgBHSEX6M+yvvbXuuLxx3v0MRP5/GZ3Uj/gychQ+pVXKv3KK/vsOaMdAUiKfVuw/P6Eh/KNMbFvMR3f8ExbWyzlt7dLsqS2SPy3lI5vTtFwOBaMOr6FWWm+jm9RPlk+ryyvV22f1iva3BT7lp3uV3tDgzxZg2LP17HLyrS2djyGcR7fhEKxb48dwS024mCcURPpYtd33Nf5lUqSlZ5+1uTnM+tNOCTJkpXmU6SmVr6hOTLtUVlej9rq6uTLz1fjf5Ur45rRavjtC8qZP1+ezl1lZ3W5ZVlqPfhn+f+vq2Lf4CxL0ZaW2MhFxy7C8AeHlT7yKpm2M98GPX6/oi2tMu1tOr1rtzLHFslz2WVnrqAejb1ObfWfSu1ReXOGnPVGMM6ogDyWFDWKnjqlpt//XlklJfJ/6UsKvf8/kpHS8vN0atduZYy+WmnDC2OvTzgs0/ltUJLa2yTLUviDD5VWUCDvF+zYN8HTpzsaa8W+PXb2ZyQi094mX85Q+a++WiYSUfRUcyysNzer/dQpqa1N7Q1BtX3yidJHjox/E0bbO77tdnzrNObMNnW89zrPf2T502VOn5aJtKm9oV5pI0Z0vpHPvKQdt6Otsd2snvR0maiR2tti29n5jbej7Wpv0+nduzWopCS2XR3fZI0xZ0YZOsOIt+N1tDzOblmZ2AiJlZYeux2NyrS36wv/63/Jf9VItdU3dOyyDTu7bk34zM/Ol5lI7G9P0fZYeztHUHw+RVtbz4woxm3v2QHprMuktLXHvvV3tjMalTFR53pwsjyx19HIec+YSCT2Nx3t/JtOcx7WtEUkI2eUQZYVWxc1Z0YOOl+r87wWzr/GxEb9vN7YezsaleXzykTaYr/g8Tp10dOnZdraYn+Lkba4bex8j1g+nzNqa1pDsW32xEYSz33euEZ1bJtldYzmnP3/ZuffUsfPnSN55w2kxqi9vl6nKis1ZN4dsZEzY5xt6/x/qfP/Kad/o2e1qWMUJdbu9tj786y/hc4RSs+gzFi7O97D7adOxUaRIhHn7/HcEUx19oNlyXf55fLagxVtaVW0tUWmpVXR1lZ5MjKU8eUvy02MxAAAgH4jkc9vjk4CAAApiRADAABSEiEGAACkJEIMAABIST0eYlatWhU7n8JZSyAQcO43xmjVqlUqKChQZmamJk+erP3798c9RigU0uLFi5Wbm6usrCzNmTNHR48e7emmAgCAFNYrIzHXXnutampqnGXv3r3OfT/72c/08MMPa+3atdq9e7cCgYBuvvlmNZ11QrOlS5dq06ZN2rhxo7Zv367m5mbNnj1b7ZybBAAAdOiV88T4fL640ZdOxhj94he/0EMPPaTbbrtNkvSrX/1K+fn5eu655/S9731PwWBQTz75pJ555hlNmzZNkvTss8+qsLBQW7Zs0YwZM3qjyQAAIMX0ykjMoUOHVFBQoJEjR+pb3/qW3n//fUnS4cOHVVtbq+nTpzu1fr9fkyZNUmVlpSRpz549ikQicTUFBQUqKipyas4nFAqpsbExbgEAAJeuHg8x48eP19NPP63//u//1hNPPKHa2lpNnDhRJ0+eVG3HVZLz8/Pjfic/P9+5r7a2Vunp6RoyZMhn1pxPWVmZbNt2lsLCwh7eMgAA0J/0eIiZNWuWvvnNb2rs2LGaNm2aXnnlFUmx3Uadzj29/ZnTI3+2z6tZuXKlgsGgsxw5cqQbWwEAAPq7Xj/EOisrS2PHjtWhQ4eceTLnjqjU1dU5ozOBQEDhcFj19fWfWXM+fr9fgwcPjlsAAMClq9dDTCgU0jvvvKNhw4Zp5MiRCgQCqqiocO4Ph8Patm2bJk6cKEkqLi5WWlpaXE1NTY327dvn1AAAAPT40UkrVqzQrbfeqiuuuEJ1dXX6yU9+osbGRs2fP1+WZWnp0qVavXq1Ro0apVGjRmn16tUaNGiQ5s2bJ0mybVsLFizQ8uXLNXToUOXk5GjFihXO7ikAAACpF0LM0aNHdccdd+iTTz7R5ZdfrgkTJmjHjh0a0XHZ+wceeEAtLS269957VV9fr/Hjx2vz5s3Kzs52HuORRx6Rz+fT3Llz1dLSoqlTp2r9+vXyer093VwAAJCiLGOMcbsRvSGRS3kDAID+IZHPb66dBAAAUhIhBgAApCRCDAAASEmEGAAAkJIIMQAAICURYgAAQEoixAAAgJREiAEAACmJEAMAAFISIQYAAKQkQgwAAEhJhBgAAJCSCDEAACAlEWIAAEBKIsQAAICURIgBAAApiRADAABSEiEGAACkJEIMAABISYQYAACQkggxAAAgJRFiAABASiLEAACAlESIAQAAKYkQAwAAUhIhBgAApCRCDAAASEmEGAAAkJIIMQAAICURYgAAQEoixAAAgJREiAEAACmJEAMAAFISIQYAAKQkQgwAAEhJhBgAAJCSCDEAACAlEWIAAEBKIsQAAICURIgBAAApiRADAABSEiEGAACkJEIMAABISYQYAACQkggxAAAgJRFiAABASiLEAACAlESIAQAAKanfh5hf/vKXGjlypDIyMlRcXKw//OEPbjcJAAD0A/06xDz//PNaunSpHnroIb311lv667/+a82aNUsfffSRq+2q+GCzxv5qrP5p7QSdqPnQ1bYAADBQ9esQ8/DDD2vBggX67ne/qzFjxugXv/iFCgsLtW7dOtfaVLX1WS3btlyS9P9mn9KL7/zGtbYAADCQ9dsQEw6HtWfPHk2fPj1u/fTp01VZWdmlPhQKqbGxMW7pDW+1HIq7/Z0bF/XK8wAAgAvrtyHmk08+UXt7u/Lz8+PW5+fnq7a2tkt9WVmZbNt2lsLCwl5p19ybFuv26DW652S7qr7xhvzpGb3yPAAA4MJ8bjfg81iWFXfbGNNlnSStXLlSy5Ytc243Njb2SpDJzczVP/5vdiEBAOC2fhticnNz5fV6u4y61NXVdRmdkSS/3y+/399XzQMAAC7rt7uT0tPTVVxcrIqKirj1FRUVmjhxokutAgAA/UW/HYmRpGXLlqm0tFTjxo1TSUmJHn/8cX300Ue655573G4aAABwWb8OMbfffrtOnjypH//4x6qpqVFRUZFeffVVjRgxwu2mAQAAl1nGGON2I3pDY2OjbNtWMBjU4MGD3W4OAAC4CIl8fvfbOTEAAAAXQogBAAApiRADAABSEiEGAACkJEIMAABISYQYAACQkggxAAAgJRFiAABASurXZ+ztjs5z+DU2NrrcEgAAcLE6P7cv5ly8l2yIaWpqkiQVFha63BIAAJCopqYm2bZ9wZpL9rID0WhUH3/8sbKzs2VZVo8+dmNjowoLC3XkyBEuadCL6Oe+QT/3Dfq579DXfaO3+tkYo6amJhUUFMjjufCsl0t2JMbj8Wj48OG9+hyDBw/mD6QP0M99g37uG/Rz36Gv+0Zv9PPnjcB0YmIvAABISYQYAACQkggxSfD7/frRj34kv9/vdlMuafRz36Cf+wb93Hfo677RH/r5kp3YCwAALm2MxAAAgJREiAEAACmJEAMAAFISIQYAAKQkQgwAAEhJhJgE/fKXv9TIkSOVkZGh4uJi/eEPf3C7Sf1WWVmZbrjhBmVnZysvL09f//rXdfDgwbgaY4xWrVqlgoICZWZmavLkydq/f39cTSgU0uLFi5Wbm6usrCzNmTNHR48ejaupr69XaWmpbNuWbdsqLS1VQ0NDb29iv1RWVibLsrR06VJnHf3cc44dO6Zvf/vbGjp0qAYNGqS/+Iu/0J49e5z76evua2tr0z/+4z9q5MiRyszM1FVXXaUf//jHikajTg39nLg33nhDt956qwoKCmRZll588cW4+/uyTz/66CPdeuutysrKUm5urpYsWaJwOJz4RhlctI0bN5q0tDTzxBNPmAMHDpj777/fZGVlmQ8//NDtpvVLM2bMME899ZTZt2+fqa6uNrfccou54oorTHNzs1OzZs0ak52dbX7729+avXv3mttvv90MGzbMNDY2OjX33HOP+eIXv2gqKirMm2++aaZMmWKuv/5609bW5tTMnDnTFBUVmcrKSlNZWWmKiorM7Nmz+3R7+4Ndu3aZK6+80lx33XXm/vvvd9bTzz3j008/NSNGjDB33XWX2blzpzl8+LDZsmWLee+995wa+rr7fvKTn5ihQ4ea//zP/zSHDx82v/nNb8xll11mfvGLXzg19HPiXn31VfPQQw+Z3/72t0aS2bRpU9z9fdWnbW1tpqioyEyZMsW8+eabpqKiwhQUFJhFixYlvE2EmAT81V/9lbnnnnvi1l1zzTXmhz/8oUstSi11dXVGktm2bZsxxphoNGoCgYBZs2aNU9Pa2mps2zaPPvqoMcaYhoYGk5aWZjZu3OjUHDt2zHg8HlNeXm6MMebAgQNGktmxY4dTU1VVZSSZd999ty82rV9oamoyo0aNMhUVFWbSpElOiKGfe84PfvADc+ONN37m/fR1z7jlllvM3/3d38Wtu+2228y3v/1tYwz93BPODTF92aevvvqq8Xg85tixY07Nr3/9a+P3+00wGExoO9iddJHC4bD27Nmj6dOnx62fPn26KisrXWpVagkGg5KknJwcSdLhw4dVW1sb16d+v1+TJk1y+nTPnj2KRCJxNQUFBSoqKnJqqqqqZNu2xo8f79RMmDBBtm0PqNfmvvvu0y233KJp06bFraefe85LL72kcePG6W//9m+Vl5enr3zlK3riiSec++nrnnHjjTfq97//vf785z9Lkv70pz9p+/bt+pu/+RtJ9HNv6Ms+raqqUlFRkQoKCpyaGTNmKBQKxe2avRiX7FWse9onn3yi9vZ25efnx63Pz89XbW2tS61KHcYYLVu2TDfeeKOKiookyem38/Xphx9+6NSkp6dryJAhXWo6f7+2tlZ5eXldnjMvL2/AvDYbN27Um2++qd27d3e5j37uOe+//77WrVunZcuW6cEHH9SuXbu0ZMkS+f1+fec736Gve8gPfvADBYNBXXPNNfJ6vWpvb9dPf/pT3XHHHZJ4T/eGvuzT2traLs8zZMgQpaenJ9zvhJgEWZYVd9sY02Udulq0aJHefvttbd++vct9yfTpuTXnqx8or82RI0d0//33a/PmzcrIyPjMOvq5+6LRqMaNG6fVq1dLkr7yla9o//79Wrdunb7zne84dfR19zz//PN69tln9dxzz+naa69VdXW1li5dqoKCAs2fP9+po597Xl/1aU/1O7uTLlJubq68Xm+XlFhXV9clUSLe4sWL9dJLL+m1117T8OHDnfWBQECSLtingUBA4XBY9fX1F6w5fvx4l+c9ceLEgHht9uzZo7q6OhUXF8vn88nn82nbtm36t3/7N/l8PqcP6OfuGzZsmL785S/HrRszZow++ugjSbyne8o//MM/6Ic//KG+9a1vaezYsSotLdXf//3fq6ysTBL93Bv6sk8DgUCX56mvr1ckEkm43wkxFyk9PV3FxcWqqKiIW19RUaGJEye61Kr+zRijRYsW6YUXXtDWrVs1cuTIuPtHjhypQCAQ16fhcFjbtm1z+rS4uFhpaWlxNTU1Ndq3b59TU1JSomAwqF27djk1O3fuVDAYHBCvzdSpU7V3715VV1c7y7hx43TnnXequrpaV111Ff3cQ7761a92OU3An//8Z40YMUIS7+mecvr0aXk88R9PXq/XOcSafu55fdmnJSUl2rdvn2pqapyazZs3y+/3q7i4OLGGJzQNeIDrPMT6ySefNAcOHDBLly41WVlZ5oMPPnC7af3S97//fWPbtnn99ddNTU2Ns5w+fdqpWbNmjbFt27zwwgtm79695o477jjvIX3Dhw83W7ZsMW+++aa56aabzntI33XXXWeqqqpMVVWVGTt27CV7mOTFOPvoJGPo556ya9cu4/P5zE9/+lNz6NAhs2HDBjNo0CDz7LPPOjX0dffNnz/ffPGLX3QOsX7hhRdMbm6ueeCBB5wa+jlxTU1N5q233jJvvfWWkWQefvhh89ZbbzmnCemrPu08xHrq1KnmzTffNFu2bDHDhw/nEOu+8O///u9mxIgRJj093fzlX/6lc7gwupJ03uWpp55yaqLRqPnRj35kAoGA8fv95mtf+5rZu3dv3OO0tLSYRYsWmZycHJOZmWlmz55tPvroo7iakydPmjvvvNNkZ2eb7Oxsc+edd5r6+vo+2Mr+6dwQQz/3nJdfftkUFRUZv99vrrnmGvP444/H3U9fd19jY6O5//77zRVXXGEyMjLMVVddZR566CETCoWcGvo5ca+99tp5/0+eP3++MaZv+/TDDz80t9xyi8nMzDQ5OTlm0aJFprW1NeFtsowxJrGxGwAAAPcxJwYAAKQkQgwAAEhJhBgAAJCSCDEAACAlEWIAAEBKIsQAAICURIgBAAApiRADAABSEiEGAACkJEIMAABISYQYAACQkv5/aFKnUZWuF/oAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588.798095703125"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-430.98846435546875,\n",
       " -428.9092712402344,\n",
       " -431.19329833984375,\n",
       " -430.2036437988281,\n",
       " -429.54241943359375,\n",
       " -495.24005126953125,\n",
       " -497.7619934082031,\n",
       " -431.64697265625,\n",
       " -496.6666259765625,\n",
       " -432.9261779785156]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 450.5078918457031, Std Loss: 30.169142147323257\n",
      "Mean Training Time: 2534.115599679947s, Std Training Time: 111.6896981261565s\n",
      "Final mu values (across trials): [[1.5008003 2.3585773 3.5514126 6.6989136]\n",
      " [1.5022465 2.358111  3.5378783 6.706832 ]\n",
      " [1.5094032 2.364257  3.5353854 6.698927 ]\n",
      " [1.4979413 2.3533928 3.5345452 6.6945763]\n",
      " [1.4972234 2.3452284 3.4919968 5.231285 ]\n",
      " [1.9075062 3.425921  5.769442  6.6984034]\n",
      " [1.9131354 3.4347324 5.735813  6.690815 ]\n",
      " [1.5033176 2.357287  3.530386  3.5317314]\n",
      " [1.9101027 3.4326558 5.2755594 6.695968 ]\n",
      " [1.5047476 2.3614802 3.5338445 6.6924057]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
