{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([ 1.5330,  2.3198,  1.9982,  2.3968,  0.3079,  0.5543,  2.3643,  1.8945,\n",
       "          1.1275,  1.3819,  1.7411,  1.8167,  2.2140,  1.6630,  1.8520,  1.1679,\n",
       "          0.9552,  2.5095,  1.5754,  1.6833,  2.0483, -0.3675,  2.2250,  1.3594,\n",
       "          1.1646,  1.7063,  4.0579,  2.8209,  2.5295,  2.5786,  4.1529,  3.8582,\n",
       "          4.3736,  4.2563,  4.5043,  3.4792,  6.2295,  3.2383,  2.9829,  5.1790,\n",
       "          3.3860,  4.5049,  4.2976,  3.8397,  4.4908,  5.2874,  6.1153,  6.0685,\n",
       "          3.7691,  5.6993,  6.9450,  6.0069,  4.7322,  5.0175,  3.9819,  5.9805,\n",
       "          5.5225,  5.6852,  4.0112,  5.9937,  4.1093,  5.8191,  3.6080,  5.7160,\n",
       "          7.3107,  7.9827,  7.8392,  7.7467,  7.1348,  7.9203,  8.5771,  7.7235,\n",
       "          7.8662,  7.3451,  3.7121,  7.0677,  4.9003,  4.7022,  6.5689,  4.7863,\n",
       "          4.9393,  5.6897,  4.7773,  4.6537,  4.1493,  5.3473,  6.9664]),\n",
       " tensor([ 1.1274,  1.7346, -1.7293,  2.9207,  3.0341,  2.0645,  2.7471,  1.1416,\n",
       "          5.4511,  4.1590,  5.5436,  3.6437,  4.9980,  2.4203,  3.4046,  3.8696,\n",
       "          5.6578,  5.9804,  5.8128,  6.4526,  4.9780,  6.5357,  5.8768,  4.7305,\n",
       "          6.7934,  6.7489,  3.3517,  2.4088,  4.2291,  2.8520,  2.6641,  3.4227,\n",
       "          3.2854,  3.5704,  4.4344,  4.5310,  5.1936,  3.1462,  6.1167,  3.2042,\n",
       "          3.6173,  4.5585,  5.6558,  3.5193,  2.6055,  1.5804,  1.7392,  3.4645,\n",
       "          2.5241,  3.9017,  5.2017,  3.0507,  3.5246,  3.7262,  3.8706,  3.9827,\n",
       "          2.6233,  3.2023,  2.9849,  2.8390,  2.5988,  5.7032,  3.6110,  3.4445,\n",
       "          0.7841,  1.1031,  1.9158,  0.5802,  2.5395,  1.3739,  2.2022,  1.9531,\n",
       "          4.0982,  3.1157,  5.8348,  7.6652,  8.9926,  6.1316,  7.5820,  8.2713,\n",
       "          6.7530,  7.0983,  7.2820,  7.2130,  7.0806,  6.6705,  7.9596]),\n",
       " 87)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fc28cb59d0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fc28d397f0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9568\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9568\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9568\\2202253750.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9568\\2202253750.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-613.1045532226562\n",
      "tensor([-0.0405,  0.0257, -0.1357, -0.0642], device='cuda:0')\n",
      "tensor([ 6.5598, 10.6852, 11.7661, 13.5873], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-610.2183837890625\n",
      "tensor([-10.5483,  -6.4004,  -1.4148,  -1.5531], device='cuda:0')\n",
      "tensor([ 6.5304, 10.6783, 11.7604, 13.5997], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-611.2741088867188\n",
      "tensor([70.8748, 45.9611,  5.4760,  2.7065], device='cuda:0')\n",
      "tensor([ 6.4552, 10.6746, 11.7515, 13.5611], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-618.1694946289062\n",
      "tensor([11.2989,  5.7380,  9.3145,  1.8662], device='cuda:0')\n",
      "tensor([ 6.6820, 10.6652, 11.7857, 13.6520], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-610.14599609375\n",
      "tensor([-0.4675,  0.8414,  0.5864,  0.0999], device='cuda:0')\n",
      "tensor([ 6.5836, 10.6755, 11.7489, 13.5685], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-611.0728759765625\n",
      "tensor([-73.2812, -43.4750,  -6.0722,  -2.6813], device='cuda:0')\n",
      "tensor([ 6.5592, 10.6863, 11.7724, 13.6207], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-611.4231567382812\n",
      "tensor([77.0003, 45.2412,  8.0919,  6.4410], device='cuda:0')\n",
      "tensor([ 6.6592, 10.6886, 11.7605, 13.5783], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-723.43701171875\n",
      "tensor([ -7.7279,  -7.0911, -13.0171,   0.5225], device='cuda:0')\n",
      "tensor([ 3.2542,  6.6948, 11.3247, 11.3672], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-615.8078002929688\n",
      "tensor([-102.0327,  -65.0656,  -15.1203,   -6.9367], device='cuda:0')\n",
      "tensor([ 6.5400, 10.6690, 11.7518, 13.5973], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-613.451416015625\n",
      "tensor([-26.2519, -14.9455,  -2.5391,  -2.3358], device='cuda:0')\n",
      "tensor([ 6.6884, 10.6804, 11.7609, 13.5847], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.5229, 0.4612, 0.4226, 1.0419], grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(net.sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 6.559762, 10.685242, 11.766124, 13.587349], dtype=float32),\n",
       " array([ 6.5304008, 10.678251 , 11.760416 , 13.599655 ], dtype=float32),\n",
       " array([ 6.4552255, 10.6746025, 11.751529 , 13.561072 ], dtype=float32),\n",
       " array([ 6.6820483, 10.665183 , 11.785654 , 13.652002 ], dtype=float32),\n",
       " array([ 6.5836062, 10.675497 , 11.748946 , 13.5684805], dtype=float32),\n",
       " array([ 6.559155, 10.686281, 11.772388, 13.620702], dtype=float32),\n",
       " array([ 6.6591697, 10.688586 , 11.760458 , 13.578309 ], dtype=float32),\n",
       " array([ 3.2541876,  6.6948447, 11.3247   , 11.367231 ], dtype=float32),\n",
       " array([ 6.539959 , 10.668995 , 11.7517605, 13.597278 ], dtype=float32),\n",
       " array([ 6.6884146, 10.680384 , 11.76093  , 13.584707 ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc1): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "611.8974609375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-613.1045532226562,\n",
       " -610.2183837890625,\n",
       " -611.2741088867188,\n",
       " -618.1694946289062,\n",
       " -610.14599609375,\n",
       " -611.0728759765625,\n",
       " -611.4231567382812,\n",
       " -723.43701171875,\n",
       " -615.8078002929688,\n",
       " -613.451416015625]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 623.8104797363281, Std Loss: 33.297976860345756\n",
      "Mean Training Time: 2786.9584362745286s, Std Training Time: 324.295772035996s\n",
      "Final mu values (across trials): [[ 6.559762  10.685242  11.766124  13.587349 ]\n",
      " [ 6.5304008 10.678251  11.760416  13.599655 ]\n",
      " [ 6.4552255 10.6746025 11.751529  13.561072 ]\n",
      " [ 6.6820483 10.665183  11.785654  13.652002 ]\n",
      " [ 6.5836062 10.675497  11.748946  13.5684805]\n",
      " [ 6.559155  10.686281  11.772388  13.620702 ]\n",
      " [ 6.6591697 10.688586  11.760458  13.578309 ]\n",
      " [ 3.2541876  6.6948447 11.3247    11.367231 ]\n",
      " [ 6.539959  10.668995  11.7517605 13.597278 ]\n",
      " [ 6.6884146 10.680384  11.76093   13.584707 ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
