{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([1.3285, 0.6439, 1.4886, 3.4801, 0.9283, 2.2547, 2.4394, 1.2519, 2.3957,\n",
       "         3.3853, 0.8915, 1.2931, 0.3824, 1.5857, 1.9789, 0.2465, 1.1106, 4.3295,\n",
       "         3.7957, 4.5999, 3.6837, 4.0460, 2.5077, 4.3001, 4.3427, 5.3969, 3.4456,\n",
       "         5.0748, 6.9543, 8.3728, 6.8273, 8.4904, 8.0105, 7.5912, 7.3014, 5.5224,\n",
       "         6.6309, 7.2503, 8.6681, 7.3395, 7.6207, 7.9342, 8.4155, 8.1580, 7.8531,\n",
       "         8.6000, 9.0047, 6.8361, 7.3787, 4.6113, 3.9098, 6.5176, 5.8048, 6.0818,\n",
       "         6.6814, 5.1718, 7.1510, 7.0826, 5.3718, 4.7382, 5.7618, 6.1869, 7.1602,\n",
       "         5.0176, 5.8109, 4.8408, 5.5899]),\n",
       " tensor([ 1.0204,  2.3802,  2.5675,  2.6438,  1.2323,  0.8434,  2.0448,  1.3626,\n",
       "         -0.0502,  0.3520,  1.5679,  1.0342,  0.7088,  2.6354,  0.8866, -0.3323,\n",
       "          0.9116,  1.6536,  1.4160,  1.8269,  2.0284,  0.7455,  1.5189,  1.3223,\n",
       "          2.1033,  2.2841, -0.7123, -0.0435,  1.9243,  1.3692,  1.5908,  0.3808,\n",
       "          0.2188,  1.0551,  0.5058,  2.4076,  2.2985,  3.7417,  0.1664,  2.4084,\n",
       "          0.7116,  1.1439,  0.5920,  1.5768,  1.3157,  1.1567,  0.8414,  5.7308,\n",
       "          6.1464,  4.5635,  3.7458,  6.4896,  4.8108,  5.1392,  6.1954,  5.8516,\n",
       "          6.2261,  5.0431,  4.9661,  5.8031,  5.2748,  5.0635,  5.4092,  5.3739,\n",
       "          5.2983,  3.7541,  9.0607]),\n",
       " 67)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221ec8096a0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x221ec89ac60>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        self.num_states=num_states\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=6):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=num_states\n",
    "        num_hiddens=num_states\n",
    "        self.num_states=num_states\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION(num_states)\n",
    "        self.A2=ATTENTION(num_states)\n",
    "        self.A3=ATTENTION(num_states)\n",
    "        self.A4=ATTENTION(num_states)\n",
    "        self.A5=ATTENTION(num_states)\n",
    "        self.A6=ATTENTION(num_states)\n",
    "        self.A7=ATTENTION(num_states)\n",
    "        self.A8=ATTENTION(num_states)\n",
    "        self.A9=ATTENTION(num_states)\n",
    "        self.A10=ATTENTION(num_states)\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,self.num_states).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    print(max_length)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9824\\2282235004.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9824\\502618589.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9824\\2278689434.py:319: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_9824\\2278689434.py:320: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-532.1666259765625\n",
      "tensor([155.8840, 151.4843, 185.9088,  27.4789,   5.7568,   2.1022],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.3725,  6.5404, 10.3178, 11.1632, 12.0132, 13.7892], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-530.4519653320312\n",
      "tensor([-21.3750, -13.5329, -14.4661,  -4.4592,  -1.3424,  -0.2018],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.5608,  6.7537, 10.3135, 11.1536, 11.9914, 13.7472], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-533.3564453125\n",
      "tensor([-104.7099,  -63.1154,  -89.7707,  -18.4479,   -7.8818,   -0.4982],\n",
      "       device='cuda:0')\n",
      "tensor([ 4.1855,  6.7223, 10.3085, 11.1593, 12.0164, 13.8030], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-527.8560180664062\n",
      "tensor([-37.6078, -30.3508, -31.5193,  -6.6236,  -2.7472,  -1.3974],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.8189,  6.8693, 10.3302, 11.1718, 12.0204, 13.8070], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-531.9246215820312\n",
      "tensor([-5.1569, -4.2966, -4.9514, -1.0647, -1.1502, -1.5154], device='cuda:0')\n",
      "tensor([ 3.3842,  6.5200, 10.3304, 11.1741, 12.0226, 13.7956], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-573.86279296875\n",
      "tensor([132.7878, 160.4751,  57.1147, 118.2593,  28.8060,  16.7144],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.0166,  6.6492,  7.9474, 10.6662, 11.7556, 13.5906], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-527.9161376953125\n",
      "tensor([251.6887, 175.6156, 225.7941,  25.8144,  -7.6787,  -5.9177],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.9724,  6.7511, 10.3148, 11.1575, 12.0141, 13.7873], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-528.8638305664062\n",
      "tensor([-233.9661, -212.2532, -248.1226,  -48.6622,  -43.1956,  -20.0811],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.4514,  6.5939, 10.3266, 11.1689, 12.0235, 13.8161], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-535.5745849609375\n",
      "tensor([50.5626, 44.5336, 58.6011, 10.4754,  2.9138, -0.2649], device='cuda:0')\n",
      "tensor([ 3.4599,  6.4343, 10.3122, 11.1470, 11.9954, 13.7721], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "525\n",
      "525\n",
      "tensor(24719, device='cuda:0')\n",
      "tensor(24719, device='cuda:0')\n",
      "-573.080322265625\n",
      "tensor([-357.1642, -388.4769, -140.5362, -242.3712,  -84.6659,  -32.1551],\n",
      "       device='cuda:0')\n",
      "tensor([ 3.2931,  6.8887,  8.2517, 10.6708, 11.7587, 13.5897], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 3.3724759,  6.540431 , 10.317796 , 11.163246 , 12.013205 ,\n",
       "        13.789173 ], dtype=float32),\n",
       " array([ 3.5608356,  6.753701 , 10.313521 , 11.15357  , 11.991398 ,\n",
       "        13.747242 ], dtype=float32),\n",
       " array([ 4.1854925,  6.722276 , 10.3085   , 11.159336 , 12.016372 ,\n",
       "        13.802972 ], dtype=float32),\n",
       " array([ 3.8188858,  6.869257 , 10.3302   , 11.1717825, 12.020354 ,\n",
       "        13.806955 ], dtype=float32),\n",
       " array([ 3.3841503,  6.5199614, 10.330368 , 11.174058 , 12.022553 ,\n",
       "        13.79561  ], dtype=float32),\n",
       " array([ 3.0166483,  6.649199 ,  7.9473677, 10.66624  , 11.755604 ,\n",
       "        13.590643 ], dtype=float32),\n",
       " array([ 3.97238 ,  6.751089, 10.314793, 11.157516, 12.0141  , 13.787282],\n",
       "       dtype=float32),\n",
       " array([ 3.4514418,  6.593911 , 10.326555 , 11.168918 , 12.023505 ,\n",
       "        13.816059 ], dtype=float32),\n",
       " array([ 3.4599388,  6.4343257, 10.312231 , 11.147015 , 11.995386 ,\n",
       "        13.772081 ], dtype=float32),\n",
       " array([ 3.2931297,  6.888712 ,  8.25173  , 10.670796 , 11.75875  ,\n",
       "        13.589719 ], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=7, out_features=6, bias=False)\n",
       "  (fc1): Linear(in_features=7, out_features=6, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=6, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=6, bias=True)\n",
       "  (fc4): Linear(in_features=6, out_features=6, bias=True)\n",
       "  (fc5): Linear(in_features=6, out_features=6, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=6, out_features=6, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=6, out_features=6, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=6, out_features=6, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(6, 6, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572.9412231445312"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-532.1666259765625,\n",
       " -530.4519653320312,\n",
       " -533.3564453125,\n",
       " -527.8560180664062,\n",
       " -531.9246215820312,\n",
       " -573.86279296875,\n",
       " -527.9161376953125,\n",
       " -528.8638305664062,\n",
       " -535.5745849609375,\n",
       " -573.080322265625]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 539.5053344726563, Std Loss: 17.13919307440514\n",
      "Mean Training Time: 3186.5293385982513s, Std Training Time: 1003.4121585916733s\n",
      "Final mu values (across trials): [[ 3.3724759  6.540431  10.317796  11.163246  12.013205  13.789173 ]\n",
      " [ 3.5608356  6.753701  10.313521  11.15357   11.991398  13.747242 ]\n",
      " [ 4.1854925  6.722276  10.3085    11.159336  12.016372  13.802972 ]\n",
      " [ 3.8188858  6.869257  10.3302    11.1717825 12.020354  13.806955 ]\n",
      " [ 3.3841503  6.5199614 10.330368  11.174058  12.022553  13.79561  ]\n",
      " [ 3.0166483  6.649199   7.9473677 10.66624   11.755604  13.590643 ]\n",
      " [ 3.97238    6.751089  10.314793  11.157516  12.0141    13.787282 ]\n",
      " [ 3.4514418  6.593911  10.326555  11.168918  12.023505  13.816059 ]\n",
      " [ 3.4599388  6.4343257 10.312231  11.147015  11.995386  13.772081 ]\n",
      " [ 3.2931297  6.888712   8.25173   10.670796  11.75875   13.589719 ]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
