{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)\n",
    "device='cpu'\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "tensor([[0.9900, 0.0100, 0.0000, 0.0000],\n",
      "        [0.0000, 0.9900, 0.0100, 0.0000],\n",
      "        [0.0000, 0.0000, 0.9900, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0000, 1.0000]])\n",
      "tensor([1., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0144,  3.4474,  3.2228,  3.6029,  4.9559,  2.3345,  3.5464,  5.0534,\n",
       "          2.1019,  2.3658,  3.2838,  2.2045,  3.5425,  3.1757,  2.4765,  5.7581,\n",
       "          7.5586,  4.5430,  6.1195,  6.7743,  5.5079,  3.4853,  4.6481,  4.2214,\n",
       "          5.9238,  6.5365,  5.1170,  8.2133,  9.7169,  8.3032,  7.9267,  7.8179,\n",
       "          7.6744,  5.6622,  8.4938,  6.1247,  8.0283,  6.4422,  6.6952,  6.0119,\n",
       "          6.5876,  7.5769]),\n",
       " tensor([ 0.1423,  1.6262,  2.2450,  0.8896, -0.3697,  2.4023,  3.6374,  3.6856,\n",
       "          3.1830,  2.5792,  2.5813,  3.0977,  4.6693,  1.5972,  4.5033,  4.1809,\n",
       "          4.4924,  4.1312,  3.8620,  1.9967,  4.7857,  3.8118,  4.4371,  4.3530,\n",
       "          2.9370,  3.5523,  1.6049,  0.9694,  1.7480,  1.7285,  1.5623,  1.2108,\n",
       "          1.7496,  2.8758,  0.7787,  0.9444,  3.2291,  5.6063,  2.1450,  3.6791,\n",
       "          4.4800,  8.8796]),\n",
       " 42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the joint state space for two units (4 states each)\n",
    "joint_states = [(i, j) for i in range(4) for j in range(4)]\n",
    "num_joint_states = len(joint_states)\n",
    "# Sort the joint states based on the sum of values in each tuple\n",
    "joint_states = sorted(joint_states, key=lambda x: sum(x))\n",
    "# Example: Define a 4x4 joint transition matrix P((s1, s2) -> (s1', s2'))\n",
    "P_joint = np.array([\n",
    "    # Each row corresponds to a joint state, each column to the next joint state\n",
    "    [0.4, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0, 0.0],  # (0,0) -> various states\n",
    "    [0.3, 0.3, 0.1, 0.1, 0.07, 0.05, 0.03, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (0,1)\n",
    "    [0.2, 0.2, 0.3, 0.1, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.03, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,2)\n",
    "    [0.1, 0.1, 0.2, 0.4, 0.05, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0],  # (0,3)\n",
    "    [0.3, 0.2, 0.1, 0.1, 0.2, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.0, 0.0, 0.0, 0.0],  # (1,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.3, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.0, 0.0],  # (1,1)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.4, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.02, 0.01, 0.0, 0.0],  # (1,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.2, 0.1, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.01, 0.0],  # (1,3)\n",
    "    [0.3, 0.1, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01],  # (2,0)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.1, 0.4, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01],  # (2,1)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.05, 0.05, 0.3, 0.05, 0.02, 0.02, 0.01, 0.01],  # (2,2)\n",
    "    [0.1, 0.1, 0.2, 0.3, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.1, 0.3, 0.05, 0.05, 0.02, 0.02],  # (2,3)\n",
    "    [0.2, 0.2, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.01, 0.01, 0.3, 0.1, 0.05, 0.05],  # (3,0)\n",
    "    [0.1, 0.1, 0.2, 0.2, 0.05, 0.1, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.3, 0.05, 0.1],  # (3,1)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.01, 0.01, 0.01, 0.01, 0.05, 0.1, 0.4, 0.2],  # (3,2)\n",
    "    [0.05, 0.05, 0.1, 0.1, 0.05, 0.05, 0.02, 0.02, 0.02, 0.02, 0.05, 0.05, 0.1, 0.1, 0.2, 0.3],  # (3,3)\n",
    "])\n",
    "\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i):\n",
    "        P_joint[i, j] = 0\n",
    "# Set lower triangular elements to 0\n",
    "for i in range(16):\n",
    "    for j in range(i+5,16):\n",
    "        P_joint[i, j] = 0\n",
    "\n",
    "P_joint=P_joint+np.eye(16,16)*0.8\n",
    "# Normalize each row so the sum of probabilities equals 1\n",
    "P_joint = P_joint / P_joint.sum(axis=1, keepdims=True)\n",
    "\n",
    "# Check if the rows sum to 1\n",
    "row_sums = P_joint.sum(axis=1)\n",
    "print(row_sums)  # Should all be close to 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "P_joint\n",
    "\n",
    "\n",
    "\n",
    "T_matrix=torch.tensor([[0.99, 0.01, 0.0,0.0],\n",
    "        [0.00, 0.99, 0.01,0],\n",
    "        [0.0, 0.00, 0.99,.01],\n",
    "                      [0.0,0.0,0.0,1]], device=device) \n",
    "\n",
    "T_matrix1=torch.zeros(4,4,4)\n",
    "T_matrix1[0,:,:]=T_matrix+torch.eye(4,4)*-0.0\n",
    "T_matrix1[1,:,:]=T_matrix+torch.eye(4,4)*-0.3\n",
    "T_matrix1[2,:,:]=T_matrix+torch.eye(4,4)*-0.5\n",
    "T_matrix1[3,:,:]=T_matrix+torch.eye(4,4)*-0.7\n",
    "\n",
    "T_matrix1=T_matrix1/torch.sum(T_matrix1, 2).reshape(4,4,1)\n",
    "\n",
    "T_matrix1\n",
    "\n",
    "\n",
    "s_0=torch.rand(3).to(device)\n",
    "\n",
    "s_0=torch.tensor([1,0,0])\n",
    "s_0=s_0/torch.sum(s_0)\n",
    "\n",
    "O_matrix_mean=torch.tensor([1.5,3.5,5.5,7.5]).to(device)\n",
    "O_matrix_std=torch.tensor([1.0,1.0,1.0,1.0]).to(device)\n",
    "\n",
    "\n",
    "print(T_matrix)\n",
    "\n",
    "print(s_0)\n",
    "\n",
    "\n",
    "\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "\n",
    "def simulate_HMM(T_matrix_1,O_matrix,x,t):\n",
    "    x=(0,0)\n",
    "    current_state_idx = joint_states.index(x)\n",
    "    states=[]\n",
    "    t=0\n",
    "    o=[]\n",
    "    o1=[]\n",
    "#     o_1=torch.matmul(x.to(device),O_matrix.to(device))\n",
    "# #         print(o_1)\n",
    "    o.append(Normal(O_matrix_mean[x[0]],1).sample().to(device))\n",
    "    o1.append(Normal(O_matrix_mean[x[1]],1).sample().to(device))\n",
    "    t=1\n",
    "    h=torch.tensor([1,0,0,0.0])\n",
    "    states.append(x)\n",
    "    for i in range(1,5000):\n",
    "#         print(i)\n",
    "        # print(states)\n",
    "        t+=1\n",
    "        p = P_joint[current_state_idx]\n",
    "        # print(current_state_idx)\n",
    "        # Choose the next joint state based on the current transition probabilities\n",
    "        next_state_idx = np.random.choice(range(num_joint_states), p=p)\n",
    "        next_state = joint_states[next_state_idx]\n",
    "        x=next_state\n",
    "        # x[0]=x1.item()\n",
    "        # x[1]=x2.item()\n",
    "        states.append(x)\n",
    "        # print(x[0])\n",
    "        # print(alpha)\n",
    "        o_1,o_2 = MultivariateNormal(torch.stack([O_matrix_mean[x[0]], O_matrix_mean[x[1]]]), \n",
    "                                     torch.tensor([[1, 0.0],\n",
    "                              [0.0, 1]])).sample().t().to(device)\n",
    "        o.append(o_1)\n",
    "        o1.append(o_2)\n",
    "        current_state_idx=next_state_idx\n",
    "\n",
    "        \n",
    "        t1=t\n",
    "        \n",
    "        if x[0]==3 and x[1]==3:\n",
    "            break\n",
    "    \n",
    "    \n",
    "    \n",
    "    o=torch.tensor(o).to(device)\n",
    "    o1=torch.tensor(o1).to(device)\n",
    "    return o,o1,t\n",
    "\n",
    "o=simulate_HMM(T_matrix,O_matrix_mean,torch.tensor([1.0,0,0.0]),1000)\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c155e40c80>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "# plt.plot(o[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c15e239d30>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.plot(o[0].cpu())\n",
    "plt.plot(o[1].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred1=[]\n",
    "# for i in range(0, 100):\n",
    "#             pred1.append(simulate_HMM(T_matrix, O_matrix_mean, s_0, 1))\n",
    "\n",
    "# torch.save(pred1, 'DATA_NON_LINEAR.pth')\n",
    "\n",
    "# print(\"pred1 list saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MAIN MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ATTENTION(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K, V, t1):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.tanh(self.fc_1(attn_output))+(t1))   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):  #@save\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, dropout, max_len=300):\n",
    "        super().__init__()\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "        \n",
    "        # self.P[:, :, 0::2] = torch.cos(X)\n",
    "        # self.P[:, :, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.P[:, :X.shape[1], :].to(X.device)\n",
    "        return self.dropout(X)\n",
    "\n",
    "\n",
    "\n",
    "class ATTENTION1(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(ATTENTION1, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        self.num_states=4\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "\n",
    "        self.input_projection_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection1_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.input_projection2_1 = nn.Linear(hidden_dim, hidden_dim,bias=False)\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim=hidden_dim, num_heads=num_heads, batch_first=True)\n",
    "        self.fc_1 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "\n",
    "    def forward(self,  Q, K,V):\n",
    "\n",
    "        seq_len = Q.size(1)\n",
    "        attn_mask = torch.triu(torch.ones(seq_len, seq_len, device=Q.device), diagonal=1)\n",
    "        # print(attn_mask)\n",
    "        attn_mask = attn_mask.masked_fill(attn_mask == 1, float('-inf')).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "        \n",
    "        projected_inputs = self.input_projection_1(Q)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs1 = self.input_projection1_1(K)  # [batch, seq_len, hidden_dim]\n",
    "        projected_inputs2 = self.input_projection2_1(V)  # [batch, seq_len, hidden_dim]\n",
    "        attn_output, _ = self.attention_1(projected_inputs, projected_inputs1, projected_inputs2, attn_mask=attn_mask)  # [batch, seq_len, hidden_dim]\n",
    "        pred = (F.leaky_relu(self.fc_1(attn_output))+ V )   # [batch, seq_len, output_dim]\n",
    "\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_states=4):\n",
    "        super(Net, self).__init__()\n",
    "        num_heads=1\n",
    "        hidden_dim=4\n",
    "        num_hiddens=4\n",
    "        self.num_states=4\n",
    "        self.s_0 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.s_01 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.mu1 = nn.Parameter(torch.rand(num_states), requires_grad=True)\n",
    "        self.sigma =nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "        self.sigma1 = nn.Parameter(torch.zeros(num_states), requires_grad=True)\n",
    "\n",
    "        self.base =3*(torch.tensor([0.0,0.25,0.5,1]))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "        # Create a long enough P\n",
    "        max_len=300\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        X = 3*torch.arange(max_len, dtype=torch.float32).reshape(\n",
    "            -1, 1)/max_len #/ torch.pow(10000, torch.arange(\n",
    "            #0, num_hiddens, 2, dtype=torch.float32) / num_hiddens)\n",
    "        for i in range(4):\n",
    "            self.P[:, :, i:i+1] = torch.cos(self.base[i]-X)\n",
    "\n",
    "        \n",
    "        self.input_dim = num_states\n",
    "        input_dim=num_states\n",
    "        \n",
    "        self.fc = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc1 = nn.Linear(input_dim+1 ,hidden_dim,bias=False)\n",
    "        self.fc2 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc3 = nn.Linear(1 ,hidden_dim)\n",
    "        self.fc4 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        self.fc5 = nn.Linear(hidden_dim ,hidden_dim)\n",
    "        \n",
    "        self.A1=ATTENTION()\n",
    "        self.A2=ATTENTION()\n",
    "        self.A3=ATTENTION()\n",
    "        self.A4=ATTENTION()\n",
    "        self.A5=ATTENTION()\n",
    "        self.A6=ATTENTION()\n",
    "        self.A7=ATTENTION()\n",
    "        self.A8=ATTENTION()\n",
    "        self.A9=ATTENTION()\n",
    "        self.A10=ATTENTION()\n",
    "\n",
    "        # LSTM Encoder\n",
    "        self.lstm1 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm3 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        self.lstm4 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True, num_layers=2, bidirectional=False)\n",
    "        \n",
    "\n",
    "        self.pos_encoding = PositionalEncoding(hidden_dim, 0)\n",
    "    def forward(self,  t, pred,pred1, prob=0.3):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def training1(self,  t, pred,pred1,m1,m2, prob=0.0):\n",
    "\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        #pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        #pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            #print(o_1.shape)\n",
    "            #print(new_pred[:,:,i].shape)\n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone().unsqueeze(2).repeat(1,1,4).clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states)).cuda()\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states)).cuda()\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(pred.shape)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "    \n",
    "        KL=torch.sum(F.softmax(pred_1, dim=2)*torch.log(pred[0,:-1]/F.softmax(pred_1, dim=2)))\n",
    "        KL+=torch.sum(F.softmax(pred1_1, dim=2)*torch.log(pred1[0,:-1]/F.softmax(pred1_1, dim=2)))\n",
    "        \n",
    "        pred_1=torch.sum(pred_1, dim=2)\n",
    "        pred_1=torch.log(pred_1)\n",
    "        pred_1=torch.sum(pred_1*m1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.sum(pred1_1, dim=2)\n",
    "        pred1_1=torch.log(pred1_1)\n",
    "        pred1_1=torch.sum(pred1_1*m2, dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        return pred_1, pred1_1,KL\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def state_filter(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "       \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "            pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred_1, pred1_1\n",
    "        \n",
    "    def state_filter1(self,  t, pred,pred1, prob=0.0):\n",
    "        s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "        s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(4,1,1)\n",
    "\n",
    "        # print(s_0.shape)\n",
    "        exp_mu = torch.exp(self.mu)           # Step 1: Take exponential\n",
    "        cumsum_mu = torch.cumsum(exp_mu, dim=0)\n",
    "\n",
    "\n",
    "        exp_mu1 = torch.exp(self.mu1)           # Step 1: Take exponential\n",
    "        cumsum_mu1 = torch.cumsum(exp_mu1, dim=0)\n",
    "\n",
    "        \n",
    "        pred_base=pred.clone()\n",
    "        pred1_base=pred1.clone()\n",
    "\n",
    "        pred=pred.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "        pred1=pred1.unsqueeze(0).repeat(1,1).unsqueeze(2).repeat(4,1,4).clone()\n",
    "\n",
    "        new_pred = pred.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "            \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred=new_pred.clone()\n",
    "        \n",
    "        new_pred = pred1.clone()\n",
    "        for i in range(self.num_states):\n",
    "            dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "            o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "            new_pred[:,:,i] = o_1#*1/self.num_states\n",
    "        pred1=new_pred.clone()\n",
    "\n",
    "        # Generate a binary mask to decide which timestamps to replace\n",
    "        mask = (torch.rand(4, pred.shape[1], 1, device=pred.device) < prob).float()\n",
    "        replacement = torch.full_like(pred, -1.0)\n",
    "        pred = pred * (1 - mask) + replacement * mask\n",
    "        pred1 = pred1 * (1 - mask) + replacement * mask\n",
    "        \n",
    "        # print(pred.shape)\n",
    "        pred=pred/torch.sum(pred,dim=2, keepdim=True)\n",
    "        pred1=pred1/torch.sum(pred1,dim=2, keepdim=True)\n",
    "        # print(pred[0,0])\n",
    "\n",
    "        position = torch.arange(0, t, device=device).unsqueeze(0).repeat(4,1).unsqueeze(2).float()\n",
    "        # print(position.shape)\n",
    "        t1=F.tanh(self.fc2(position)).float()\n",
    "        t2=F.tanh(self.fc3(position)).float()\n",
    "        # t2=self.fc3(position).float()\n",
    "        # pred=torch.concat([pred, position], dim=2)\n",
    "        # pred1=torch.concat([pred1, position], dim=2)\n",
    "        # print(pred.shape)\n",
    "        \n",
    "\n",
    "\n",
    "        # pred = F.leaky_relu(self.fc(pred))\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1))\n",
    "        pred_base1=pred.clone()\n",
    "        pred1_base1=pred1.clone()\n",
    "\n",
    "        pred_1, _ = self.lstm1(pred)\n",
    "        pred1_1, _ = self.lstm2(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "        \n",
    "        # pred = F.leaky_relu(self.fc(pred)) + P\n",
    "        # pred1 = F.leaky_relu(self.fc1(pred1)) + P\n",
    "        # pred = pred + t1\n",
    "        # pred1 = pred1 + t1\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        pred=self.A1(pred,pred,pred,pred_base1)\n",
    "        # pred=self.A2(pred,pred,pred,t1)\n",
    "        # pred=self.A3(pred,pred,pred,t1)\n",
    "\n",
    "        \n",
    "        pred1=self.A4(pred1,pred1,pred1,pred1_base1)\n",
    "        # pred1=self.A5(pred1,pred1,pred1,t1)\n",
    "        # pred1=self.A6(pred1,pred1,pred1,t1)\n",
    "\n",
    "        pred_1, _ = self.lstm3(pred)\n",
    "        pred1_1, _ = self.lstm4(pred1)\n",
    "        pred=F.tanh(pred)+pred_1\n",
    "        pred1=F.tanh(pred1)+pred1_1\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        pred_1=self.A7(pred,pred1,pred,pred)\n",
    "        # pred_1=self.A8(pred_1,pred1,pred_1)\n",
    "\n",
    "        \n",
    "        pred1_1=self.A9(pred1,pred ,pred1,pred1)\n",
    "        # pred1_1=self.A10(pred1_1,pred ,pred1_1)\n",
    "\n",
    "        pred=pred_1.clone()\n",
    "        pred1=pred1_1.clone()\n",
    "\n",
    "\n",
    "        pred = F.softmax(self.fc4(pred)+pred, dim=2)\n",
    "        pred1 = F.softmax(self.fc5(pred1)+pred1, dim=2)\n",
    "\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        pred_1=torch.zeros((pred.shape[0], pred.shape[1],self.num_states))\n",
    "        pred1_1=torch.zeros((pred1.shape[0], pred1.shape[1],self.num_states))\n",
    "        pred=torch.concat([s_0,pred], dim=1)\n",
    "        pred1=torch.concat([s_01,pred1], dim=1)\n",
    "        # print(pred.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu[i], torch.exp(self.sigma[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred_base))\n",
    "             \n",
    "        #     pred_1[:,:,i] = o_1*pred[:,:-1,i] \n",
    "\n",
    "\n",
    "        # for i in range(self.num_states):\n",
    "        #     dist = Normal(cumsum_mu1[i], torch.exp(self.sigma1[i]))\n",
    "        #     o_1 = torch.exp(dist.log_prob(pred1_base))\n",
    "             \n",
    "        #     pred1_1[:,:,i] = o_1*pred1[:,:-1,i] \n",
    "\n",
    "\n",
    "        # pred_1=torch.softmax(pred_1, dim=2)\n",
    "        # pred_1=torch.log(pred_1)\n",
    "        # pred_1=torch.sum(pred_1, dim=1)\n",
    "\n",
    "\n",
    "        # pred1_1=torch.softmax(pred1_1, dim=2)\n",
    "        # pred1_1=torch.log(pred1_1)\n",
    "        # pred1_1=torch.sum(pred1_1, dim=1)\n",
    "\n",
    "\n",
    "        return pred, pred1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with a padding value.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.full((len(sequences), max_length), padding_value, dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "    \n",
    "    return padded_sequences, mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        # Fill the remaining positions with the last value of the sequence\n",
    "        if length > 0:  # Check to ensure sequence is not empty\n",
    "            padded_sequences[i, length:] = seq[-1]\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, padding_value=0):\n",
    "    \"\"\"\n",
    "    Pads a list of variable-length sequences with the last value of each sequence.\n",
    "    Returns the padded sequences and a mask indicating the original positions.\n",
    "    \"\"\"\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = torch.zeros((len(sequences), max_length), dtype=torch.float32)\n",
    "    mask = torch.zeros((len(sequences), max_length), dtype=torch.bool)\n",
    "    # Collect the last values from all sequences\n",
    "    last_values = [seq[-1] for seq in sequences if len(seq) > 0]\n",
    "    last_values_tensor = torch.tensor(last_values, dtype=torch.float32)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        length = len(seq)\n",
    "        padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
    "        mask[i, :length] = True\n",
    "        \n",
    "        if length > 0:  # Ensure the sequence is not empty\n",
    "            random_indices = torch.randint(0, len(last_values_tensor), (max_length - length,))\n",
    "            random_values = last_values_tensor[random_indices]\n",
    "            padded_sequences[i, length:] = random_values\n",
    "    \n",
    "    return padded_sequences, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f=distribution_initalize(o[0])\n",
    "\n",
    "def Expectation_diff(o_1):\n",
    "    return torch.sum(o_1[0])\n",
    "\n",
    "def var_diff(o_1):\n",
    "    return torch.sum(o_1[1])\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=torch.abs(e2-e1)\n",
    "    return k\n",
    "\n",
    "\n",
    "def abs_diff(e):\n",
    "    k=0\n",
    "    for i,e1 in enumerate(e):\n",
    "        for j,e2 in enumerate(e):\n",
    "            if j==i+1:\n",
    "                k+=(e2-e1)**2\n",
    "    return k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_32520\\3374062728.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_32520\\2254581785.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  padded_sequences[i, :length] = torch.tensor(seq, dtype=torch.float32)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_32520\\4031529318.py:222: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_0=F.softmax(self.s_0).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n",
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_32520\\4031529318.py:223: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  s_01=F.softmax(self.s_01).unsqueeze(0).repeat(1,1).unsqueeze(1).repeat(100,1,1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-432.445068359375\n",
      "tensor([9.1100e+00, 2.7630e+00, 1.1144e+00, 6.9764e-03], device='cuda:0')\n",
      "tensor([1.5025, 2.3585, 3.5325, 6.6976], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 2/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-495.46343994140625\n",
      "tensor([ 3.2785e+01,  1.0642e+01,  1.1147e-01, -6.8042e-03], device='cuda:0')\n",
      "tensor([1.9066, 3.4111, 6.6921, 8.2676], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 3/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-496.07879638671875\n",
      "tensor([ 0.7206, -0.7315, -0.7275, -0.0679], device='cuda:0')\n",
      "tensor([1.9125, 3.4307, 5.6641, 6.6992], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 4/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-407.4455261230469\n",
      "tensor([-2.8697, -0.0081,  0.1422,  0.3532], device='cuda:0')\n",
      "tensor([1.3367, 1.9764, 2.5790, 3.6490], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 5/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-429.8021545410156\n",
      "tensor([-3.2178e+01, -1.4423e+01, -6.4705e+00,  4.1247e-03], device='cuda:0')\n",
      "tensor([1.5054, 2.3626, 3.5394, 6.6985], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 6/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-410.3472595214844\n",
      "tensor([-14.5900,  -5.3130,  -3.6197,  -2.1528], device='cuda:0')\n",
      "tensor([1.3415, 1.9858, 2.5895, 3.6587], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 7/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-409.31011962890625\n",
      "tensor([58.3074, 28.4313, 30.4823, 13.5642], device='cuda:0')\n",
      "tensor([1.3368, 1.9790, 2.5816, 3.6477], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 8/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-409.142333984375\n",
      "tensor([-4.0964, -0.9417, -0.2619,  0.0887], device='cuda:0')\n",
      "tensor([1.3442, 1.9829, 2.5823, 3.6490], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 9/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-429.37371826171875\n",
      "tensor([-62.3964, -26.9143, -12.7133,   0.1097], device='cuda:0')\n",
      "tensor([1.4941, 2.3519, 3.5359, 6.7141], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n",
      "Trial 10/10\n",
      "tensor(20630, device='cuda:0')\n",
      "tensor(26657, device='cuda:0')\n",
      "-432.67529296875\n",
      "tensor([-21.6392,  -9.3875,  -1.3707,  -0.2276], device='cuda:0')\n",
      "tensor([1.5024, 2.3607, 3.5385, 6.6978], device='cuda:0',\n",
      "       grad_fn=<CumsumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "pred1 = torch.load('TRAIN_DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _ in pred1]\n",
    "   \n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o=observations_o.cuda()\n",
    "    observations_o1=observations_o1.cuda()\n",
    "    m1=m1.cuda()\n",
    "    m2=m2.cuda()\n",
    "    m_1=m1.clone()\n",
    "    m_2=m2.clone()\n",
    "    print(m1.sum())\n",
    "    m_1[:,:-100]=1\n",
    "    m_2[:,:-100]=1\n",
    "    print(m_1.sum())\n",
    "    \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "    \n",
    "\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1,l2,_=net.training1(0, observations_o, observations_o1,m_1,m_2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        # Add L1 regularization\n",
    "        # l1_reg = torch.tensor(0., device=device)\n",
    "        # for param in net.parameters():\n",
    "        #     l1_reg += torch.norm(param, 1)\n",
    "\n",
    "        # # Update loss\n",
    "        # loss += 0.01 * l1_reg\n",
    "        #print(loss)\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    l1,l2,_=net.training1(0, observations_o, observations_o1,m1,m2, 0.0)\n",
    "    loss = (torch.mean(l1) + torch.mean(l2))\n",
    "    # Save trial results\n",
    "    print((loss.item()))\n",
    "    print(net.mu.grad)\n",
    "    print(torch.cumsum(torch.exp(net.mu),dim=0))\n",
    "    all_losses.append(loss.item())\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(torch.cumsum(torch.exp(net.mu),dim=0).detach().cpu().numpy())\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o=observations_o.cpu()\n",
    "    observations_o1=observations_o1.cpu()\n",
    "   \n",
    "\n",
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)\n",
    "\n",
    "# # Plot loss curve\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# for trial_losses in all_losses:\n",
    "#     plt.plot(trial_losses, alpha=0.7)\n",
    "# plt.title('Loss Curve Across Trials')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.5024853, 2.3585064, 3.532491 , 6.6976438], dtype=float32),\n",
       " array([1.9065608, 3.4111376, 6.6921225, 8.267569 ], dtype=float32),\n",
       " array([1.9125398, 3.4307384, 5.6641383, 6.699179 ], dtype=float32),\n",
       " array([1.3366681, 1.976375 , 2.578997 , 3.6489625], dtype=float32),\n",
       " array([1.5054245, 2.3626263, 3.5393944, 6.698512 ], dtype=float32),\n",
       " array([1.3414607, 1.9858316, 2.5894508, 3.6587434], dtype=float32),\n",
       " array([1.3367913, 1.9789622, 2.5816119, 3.6477387], dtype=float32),\n",
       " array([1.3441894, 1.9828898, 2.5823033, 3.6490064], dtype=float32),\n",
       " array([1.4940727, 2.3519166, 3.5358906, 6.714122 ], dtype=float32),\n",
       " array([1.5024222, 2.3607109, 3.5385122, 6.6978407], dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred1 = torch.load('DATA_NON_LINEAR.pth')\n",
    "\n",
    "# Parameters for experiment\n",
    "batch_size = 100\n",
    "num_epochs = 10000\n",
    "n_trials = 10\n",
    "learning_rate = 0.01\n",
    "device='cuda'\n",
    "# Initialize result containers\n",
    "all_losses = []\n",
    "all_l1 = []\n",
    "all_l2 = []\n",
    "trial_times = []\n",
    "final_mu_values = []\n",
    "# Parameters for early stopping\n",
    "patience = 200000  # Number of epochs to wait for improvement\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    print(f\"Trial {trial + 1}/{n_trials}\")\n",
    "    trial_start_time = time.time()\n",
    "\n",
    "    net = Net().to(device)  # Initialize model\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    scheduler = lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.0001, total_iters=num_epochs)\n",
    "\n",
    "    net.train()\n",
    "    trial_losses = []\n",
    "    trial_l1 = []\n",
    "    trial_l2 = []\n",
    "    # Extract observations and underlying states\n",
    "    observations_o = [o for o, _, _, _ in pred1]\n",
    "    observations_o1 = [o for _, o, _, _ in pred1]\n",
    "    true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "    true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "    observations_o, m1 = pad_sequences(observations_o, padding_value=-1)\n",
    "    observations_o1, m2 = pad_sequences(observations_o1, padding_value=-1)\n",
    "    observations_o = observations_o.cuda()\n",
    "    observations_o1 = observations_o1.cuda()\n",
    "    m1 = m1.cuda()\n",
    "    m2 = m2.cuda()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        net.t = 0  # Reset any time-dependent states\n",
    "\n",
    "        # Create random batch indices\n",
    "        loss = 0.0\n",
    "        optimizer.zero_grad()\n",
    "        l1, l2, _ = net.training1(0, observations_o, observations_o1, m1, m2, 0.0)\n",
    "        loss -= (torch.mean(l1) + torch.mean(l2))\n",
    "\n",
    "        trial_losses.append(loss.item())\n",
    "\n",
    "        # Compute gradients and update parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Check early stopping\n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            best_model_state = net.state_dict()  # Save the best model state\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "\n",
    "        if counter >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    # Save trial results\n",
    "    print(f\"Final Loss for Trial {trial + 1}: {loss.item()}\")\n",
    "    all_losses.append(best_loss)\n",
    "    trial_times.append(time.time() - trial_start_time)\n",
    "    final_mu_values.append(net.mu.detach().cpu().numpy())\n",
    "\n",
    "    # Load the best model state for evaluation\n",
    "    net.load_state_dict(best_model_state)\n",
    "\n",
    "    # Save filtered states\n",
    "    net.cpu()\n",
    "    observations_o = observations_o.cpu()\n",
    "    observations_o1 = observations_o1.cpu()\n",
    "    m1 = m1.cpu()\n",
    "    m2 = m2.cpu()\n",
    "    for i in range(0, batch_size):\n",
    "        o, o1, t, states = pred1[i]\n",
    "        o, o1 = o.to('cpu'), o1.to('cpu')\n",
    "        filtered_l1, filtered_l2 = net.state_filter(t, o, o1, 0.0)\n",
    "        trial_l1.append(torch.argmax(filtered_l1, dim=2)[0])\n",
    "        trial_l2.append(torch.argmax(filtered_l2, dim=2)[0])\n",
    "\n",
    "    all_l1.append(trial_l1)\n",
    "    all_l2.append(trial_l2)\n",
    "\n",
    "# Compute statistics across trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc1): Linear(in_features=5, out_features=4, bias=False)\n",
       "  (fc2): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc3): Linear(in_features=1, out_features=4, bias=True)\n",
       "  (fc4): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (fc5): Linear(in_features=4, out_features=4, bias=True)\n",
       "  (A1): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A2): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A3): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A4): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A5): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A6): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A7): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A8): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A9): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (A10): ATTENTION(\n",
       "    (input_projection_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection1_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (input_projection2_1): Linear(in_features=4, out_features=4, bias=False)\n",
       "    (attention_1): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=4, out_features=4, bias=True)\n",
       "    )\n",
       "    (fc_1): Linear(in_features=4, out_features=4, bias=True)\n",
       "  )\n",
       "  (lstm1): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm2): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm3): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (lstm4): LSTM(4, 4, num_layers=2, batch_first=True)\n",
       "  (pos_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(trial_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592.1566162109375"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(trial_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-432.445068359375,\n",
       " -495.46343994140625,\n",
       " -496.07879638671875,\n",
       " -407.4455261230469,\n",
       " -429.8021545410156,\n",
       " -410.3472595214844,\n",
       " -409.31011962890625,\n",
       " -409.142333984375,\n",
       " -429.37371826171875,\n",
       " -432.67529296875]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics across trials\n",
    "mean_losses = np.mean(all_losses, axis=0)\n",
    "std_losses = np.std(all_losses, axis=0)\n",
    "mean_trial_time = np.mean(trial_times)\n",
    "std_trial_time = np.std(trial_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Results:\n",
      "Mean Loss (Final Epoch): 435.2083709716797, Std Loss: 31.862537287631007\n",
      "Mean Training Time: 2534.0721572875977s, Std Training Time: 109.99369052707188s\n",
      "Final mu values (across trials): [[1.5024853 2.3585064 3.532491  6.6976438]\n",
      " [1.9065608 3.4111376 6.6921225 8.267569 ]\n",
      " [1.9125398 3.4307384 5.6641383 6.699179 ]\n",
      " [1.3366681 1.976375  2.578997  3.6489625]\n",
      " [1.5054245 2.3626263 3.5393944 6.698512 ]\n",
      " [1.3414607 1.9858316 2.5894508 3.6587434]\n",
      " [1.3367913 1.9789622 2.5816119 3.6477387]\n",
      " [1.3441894 1.9828898 2.5823033 3.6490064]\n",
      " [1.4940727 2.3519166 3.5358906 6.714122 ]\n",
      " [1.5024222 2.3607109 3.5385122 6.6978407]]\n"
     ]
    }
   ],
   "source": [
    "# Output results\n",
    "print(\"Training Results:\")\n",
    "print(f\"Mean Loss (Final Epoch): {-mean_losses}, Std Loss: {std_losses}\")\n",
    "print(f\"Mean Training Time: {mean_trial_time}s, Std Training Time: {std_trial_time}s\")\n",
    "print(f\"Final mu values (across trials): {np.array(final_mu_values)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred1[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "pred1[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State filtering MSE\n",
    "true_states = [t.numpy()[:, 0:1] for _, _, _, t in pred1]\n",
    "true_states1 = [t.numpy()[:, 1:2] for _, _, _, t in pred1]\n",
    "\n",
    "trial_mse_l1 = []\n",
    "trial_mse_l2 = []\n",
    "\n",
    "for trial in range(n_trials):\n",
    "    mse_l1 = 0.0\n",
    "    mse_l2 = 0.0\n",
    "    total_states = 0\n",
    "\n",
    "    l1 = all_l1[trial]\n",
    "    l2 = all_l2[trial]\n",
    "\n",
    "    for filtered_l1, filtered_l2, ts, ts1 in zip(l1, l2, true_states, true_states1):\n",
    "        mse_l1 += (np.diag(filtered_l1.detach().numpy() == ts) ).sum()\n",
    "        # print(filtered_l1.shape)\n",
    "        # print(ts.shape)\n",
    "        mse_l2 += (np.diag(filtered_l2.detach().numpy() ==  ts1)).sum()\n",
    "        total_states += ts.shape[0]\n",
    "    # print(mse_l1)\n",
    "    # print(total_states)\n",
    "    \n",
    "\n",
    "    mse_l1 /= total_states\n",
    "    mse_l2 /= total_states\n",
    "\n",
    "    trial_mse_l1.append(mse_l1)\n",
    "    trial_mse_l2.append(mse_l2)\n",
    "\n",
    "# Calculate average error and standard deviation\n",
    "average_mse_l1 = np.mean(trial_mse_l1)\n",
    "std_mse_l1 = np.std(trial_mse_l1)\n",
    "\n",
    "average_mse_l2 = np.mean(trial_mse_l2)\n",
    "std_mse_l2 = np.std(trial_mse_l2)\n",
    "\n",
    "# Print results\n",
    "print(\"State Filtering MSE Results:\")\n",
    "print(f\"Average MSE for l1: {average_mse_l1:.4f}, Std Dev: {std_mse_l1:.4f}\")\n",
    "print(f\"Average MSE for l2: {average_mse_l2:.4f}, Std Dev: {std_mse_l2:.4f}\")\n",
    "\n",
    "# Print MSE for each trial\n",
    "for trial, (mse_l1, mse_l2) in enumerate(zip(trial_mse_l1, trial_mse_l2), 1):\n",
    "    print(f\"Trial {trial}: MSE l1 = {mse_l1:.4f}, MSE l2 = {mse_l2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_mse_l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trial_mse_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of parameters\n",
    "total_params = sum(p.numel() for p in net.parameters())\n",
    "\n",
    "# Print the result\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
